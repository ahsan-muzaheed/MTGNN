{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1SGtSEIb8V7",
        "outputId": "c21b6d6e-1f09-4269-db46-3ba32ce9938e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f405af531d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import LSTM\n",
        "#from keras.layers import Dense\n",
        "#import scipy.stats as st\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n"
      ],
      "metadata": {
        "id": "B0Z7J3cGB0WS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        obj = pickle.load(fp)\n",
        "    return obj\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normal_std(x):\n",
        "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
        "\n",
        "class DataLoaderS(object):\n",
        "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
        "    def __init__(self, file_name, train, valid, device, horizon, window, normalize=2):\n",
        "        print('__init__')\n",
        "        self.P = window\n",
        "        self.h = horizon\n",
        "        \n",
        "        #fin = open(file_name)\n",
        "        #self.rawdat = np.loadtxt(fin, delimiter=',')\n",
        "\n",
        "        self.rawdat=load(file_name)\n",
        "        \n",
        "        self.dat = np.zeros(self.rawdat.shape)\n",
        "\n",
        "\n",
        "        self.n, self.m = self.dat.shape\n",
        "\n",
        "\n",
        "        self.normalize = 2\n",
        "        self.scale = np.ones(self.m)\n",
        "        self._normalized(normalize)\n",
        "        self._split(int(train * self.n), int((train + valid) * self.n), self.n)\n",
        "\n",
        "        self.scale = torch.from_numpy(self.scale).float()\n",
        "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m)\n",
        "\n",
        "        self.scale = self.scale.to(device)\n",
        "        self.scale = Variable(self.scale)\n",
        "\n",
        "        self.rse = normal_std(tmp)\n",
        "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)))\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def _normalized(self, normalize):\n",
        "        print('_normalized')\n",
        "        # normalized by the maximum value of entire matrix.\n",
        "\n",
        "        if (normalize == 0):\n",
        "            self.dat = self.rawdat\n",
        "\n",
        "        if (normalize == 1):\n",
        "            self.dat = self.rawdat / np.max(self.rawdat)\n",
        "\n",
        "        # normlized by the maximum value of each row(sensor).\n",
        "        if (normalize == 2):\n",
        "            for i in range(self.m):\n",
        "                self.scale[i] = np.max(np.abs(self.rawdat[:, i]))\n",
        "                self.dat[:, i] = self.rawdat[:, i] / np.max(np.abs(self.rawdat[:, i]))\n",
        "\n",
        "    def _split(self, train, valid, test):\n",
        "        print('_split')\n",
        "        train_set = range(self.P + self.h - 1, train)\n",
        "        valid_set = range(train, valid)\n",
        "        test_set = range(valid, self.n)\n",
        "        self.train = self._batchify(train_set, self.h)\n",
        "        self.valid = self._batchify(valid_set, self.h)\n",
        "        self.test = self._batchify(test_set, self.h)\n",
        "\n",
        "    def _batchify(self, idx_set, horizon):\n",
        "        print('_batchify')\n",
        "        n = len(idx_set)\n",
        "        X = torch.zeros((n, self.P, self.m))\n",
        "        Y = torch.zeros((n, self.m))\n",
        "        for i in range(n):\n",
        "            end = idx_set[i] - self.h + 1\n",
        "            start = end - self.P\n",
        "            X[i, :, :] = torch.from_numpy(self.dat[start:end, :])\n",
        "            Y[i, :] = torch.from_numpy(self.dat[idx_set[i], :])\n",
        "        return [X, Y]\n",
        "\n",
        "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
        "        print('get_batches')\n",
        "        length = len(inputs)\n",
        "        if shuffle:\n",
        "            index = torch.randperm(length)\n",
        "        else:\n",
        "            index = torch.LongTensor(range(length))\n",
        "        start_idx = 0\n",
        "        while (start_idx < length):\n",
        "            end_idx = min(length, start_idx + batch_size)\n",
        "            excerpt = index[start_idx:end_idx]\n",
        "            X = inputs[excerpt]\n",
        "            Y = targets[excerpt]\n",
        "            X = X.to(self.device)\n",
        "            Y = Y.to(self.device)\n",
        "            yield Variable(X), Variable(Y)\n",
        "            start_idx += batch_size\n",
        "\n",
        "class DataLoaderM(object):\n",
        "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        print( 'DataLoaderM __init__')\n",
        "        self.current_ind = 0\n",
        "        if pad_with_last_sample:\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        print('shuffle')\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        print('get_iterator')\n",
        "        self.current_ind = 0\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "def sym_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
        "\n",
        "def asym_adj(adj):\n",
        "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "def calculate_normalized_laplacian(adj):\n",
        "    \"\"\"\n",
        "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
        "    # D = diag(A 1)\n",
        "    :param adj:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    d = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "    return normalized_laplacian\n",
        "\n",
        "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
        "    if undirected:\n",
        "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
        "    L = calculate_normalized_laplacian(adj_mx)\n",
        "    if lambda_max is None:\n",
        "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
        "        lambda_max = lambda_max[0]\n",
        "    L = sp.csr_matrix(L)\n",
        "    M, _ = L.shape\n",
        "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
        "    L = (2 / lambda_max * L) - I\n",
        "    return L.astype(np.float32).todense()\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "def load_adj(pkl_filename):\n",
        "    sensor_ids, sensor_id_to_ind, adj = load_pickle(pkl_filename)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "    # Data format\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def masked_mse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds-labels)**2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_rmse(preds, labels, null_val=np.nan):\n",
        "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
        "\n",
        "\n",
        "def masked_mae(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_mape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)/labels\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "def metric(pred, real):\n",
        "    mae = masked_mae(pred,real,0.0).item()\n",
        "    mape = masked_mape(pred,real,0.0).item()\n",
        "    rmse = masked_rmse(pred,real,0.0).item()\n",
        "    return mae,mape,rmse\n",
        "\n",
        "\n",
        "def load_node_feature(path):\n",
        "    fi = open(path)\n",
        "    x = []\n",
        "    for li in fi:\n",
        "        li = li.strip()\n",
        "        li = li.split(\",\")\n",
        "        e = [float(t) for t in li[1:]]\n",
        "        x.append(e)\n",
        "    x = np.array(x)\n",
        "    mean = np.mean(x,axis=0)\n",
        "    std = np.std(x,axis=0)\n",
        "    z = torch.tensor((x-mean)/std,dtype=torch.float)\n",
        "    return z\n",
        "\n",
        "\n",
        "def normal_std(x):\n",
        "    return x.std() * np.sqrt((len(x) - 1.) / (len(x)))\n",
        "\n",
        "\n",
        "\n",
        "            "
      ],
      "metadata": {
        "id": "YqojS9jkCEvp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def normal_std(x):\n",
        "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
        "\n",
        "class DataLoaderS(object):\n",
        "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
        "    def __init__(self, file_name, train, valid, device, horizon, window, normalize=2):\n",
        "        self.P = window\n",
        "        self.h = horizon\n",
        "        fin = open(file_name)\n",
        "        self.rawdat = np.loadtxt(fin, delimiter=',')\n",
        "        self.dat = np.zeros(self.rawdat.shape)\n",
        "        self.n, self.m = self.dat.shape\n",
        "        self.normalize = 2\n",
        "        self.scale = np.ones(self.m)\n",
        "        self._normalized(normalize)\n",
        "        self._split(int(train * self.n), int((train + valid) * self.n), self.n)\n",
        "\n",
        "        self.scale = torch.from_numpy(self.scale).float()\n",
        "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m)\n",
        "\n",
        "        self.scale = self.scale.to(device)\n",
        "        self.scale = Variable(self.scale)\n",
        "\n",
        "        self.rse = normal_std(tmp)\n",
        "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)))\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def _normalized(self, normalize):\n",
        "        # normalized by the maximum value of entire matrix.\n",
        "\n",
        "        if (normalize == 0):\n",
        "            self.dat = self.rawdat\n",
        "\n",
        "        if (normalize == 1):\n",
        "            self.dat = self.rawdat / np.max(self.rawdat)\n",
        "\n",
        "        # normlized by the maximum value of each row(sensor).\n",
        "        if (normalize == 2):\n",
        "            for i in range(self.m):\n",
        "                self.scale[i] = np.max(np.abs(self.rawdat[:, i]))\n",
        "                self.dat[:, i] = self.rawdat[:, i] / np.max(np.abs(self.rawdat[:, i]))\n",
        "\n",
        "    def _split(self, train, valid, test):\n",
        "\n",
        "        train_set = range(self.P + self.h - 1, train)\n",
        "        valid_set = range(train, valid)\n",
        "        test_set = range(valid, self.n)\n",
        "        self.train = self._batchify(train_set, self.h)\n",
        "        self.valid = self._batchify(valid_set, self.h)\n",
        "        self.test = self._batchify(test_set, self.h)\n",
        "\n",
        "    def _batchify(self, idx_set, horizon):\n",
        "        n = len(idx_set)\n",
        "        X = torch.zeros((n, self.P, self.m))\n",
        "        Y = torch.zeros((n, self.m))\n",
        "        for i in range(n):\n",
        "            end = idx_set[i] - self.h + 1\n",
        "            start = end - self.P\n",
        "            X[i, :, :] = torch.from_numpy(self.dat[start:end, :])\n",
        "            Y[i, :] = torch.from_numpy(self.dat[idx_set[i], :])\n",
        "        return [X, Y]\n",
        "\n",
        "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
        "        length = len(inputs)\n",
        "        if shuffle:\n",
        "            index = torch.randperm(length)\n",
        "        else:\n",
        "            index = torch.LongTensor(range(length))\n",
        "        start_idx = 0\n",
        "        while (start_idx < length):\n",
        "            end_idx = min(length, start_idx + batch_size)\n",
        "            excerpt = index[start_idx:end_idx]\n",
        "            X = inputs[excerpt]\n",
        "            Y = targets[excerpt]\n",
        "            X = X.to(self.device)\n",
        "            Y = Y.to(self.device)\n",
        "            yield Variable(X), Variable(Y)\n",
        "            start_idx += batch_size\n",
        "\n",
        "class DataLoaderM(object):\n",
        "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "        if pad_with_last_sample:\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "\n",
        "def sym_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
        "\n",
        "def asym_adj(adj):\n",
        "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "def calculate_normalized_laplacian(adj):\n",
        "    \"\"\"\n",
        "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
        "    # D = diag(A 1)\n",
        "    :param adj:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    d = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "    return normalized_laplacian\n",
        "\n",
        "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
        "    if undirected:\n",
        "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
        "    L = calculate_normalized_laplacian(adj_mx)\n",
        "    if lambda_max is None:\n",
        "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
        "        lambda_max = lambda_max[0]\n",
        "    L = sp.csr_matrix(L)\n",
        "    M, _ = L.shape\n",
        "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
        "    L = (2 / lambda_max * L) - I\n",
        "    return L.astype(np.float32).todense()\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "def load_adj(pkl_filename):\n",
        "    sensor_ids, sensor_id_to_ind, adj = load_pickle(pkl_filename)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "    # Data format\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def masked_mse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds-labels)**2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_rmse(preds, labels, null_val=np.nan):\n",
        "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
        "\n",
        "\n",
        "def masked_mae(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_mape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)/labels\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "def metric(pred, real):\n",
        "    mae = masked_mae(pred,real,0.0).item()\n",
        "    mape = masked_mape(pred,real,0.0).item()\n",
        "    rmse = masked_rmse(pred,real,0.0).item()\n",
        "    return mae,mape,rmse\n",
        "\n",
        "\n",
        "def load_node_feature(path):\n",
        "    fi = open(path)\n",
        "    x = []\n",
        "    for li in fi:\n",
        "        li = li.strip()\n",
        "        li = li.split(\",\")\n",
        "        e = [float(t) for t in li[1:]]\n",
        "        x.append(e)\n",
        "    x = np.array(x)\n",
        "    mean = np.mean(x,axis=0)\n",
        "    std = np.std(x,axis=0)\n",
        "    z = torch.tensor((x-mean)/std,dtype=torch.float)\n",
        "    return z\n",
        "\n",
        "\n",
        "def normal_std(x):\n",
        "    return x.std() * np.sqrt((len(x) - 1.) / (len(x)))\n",
        "\n",
        "\n",
        "\n",
        "            "
      ],
      "metadata": {
        "id": "be1lMHhiHxBh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#layer.py\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class nconv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(nconv,self).__init__()\n",
        "\n",
        "    def forward(self,x, A):\n",
        "        x = torch.einsum('ncwl,vw->ncvl',(x,A))\n",
        "        return x.contiguous()\n",
        "\n",
        "class dy_nconv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(dy_nconv,self).__init__()\n",
        "\n",
        "    def forward(self,x, A):\n",
        "        x = torch.einsum('ncvl,nvwl->ncwl',(x,A))\n",
        "        return x.contiguous()\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self,c_in,c_out,bias=True):\n",
        "        super(linear,self).__init__()\n",
        "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class prop(nn.Module):\n",
        "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
        "        super(prop, self).__init__()\n",
        "        self.nconv = nconv()\n",
        "        self.mlp = linear(c_in,c_out)\n",
        "        self.gdep = gdep\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self,x,adj):\n",
        "        adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
        "        d = adj.sum(1)\n",
        "        h = x\n",
        "        dv = d\n",
        "        a = adj / dv.view(-1, 1)\n",
        "        for i in range(self.gdep):\n",
        "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,a)\n",
        "        ho = self.mlp(h)\n",
        "        return ho\n",
        "\n",
        "\n",
        "class mixprop(nn.Module):\n",
        "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
        "        super(mixprop, self).__init__()\n",
        "        self.nconv = nconv()\n",
        "        self.mlp = linear((gdep+1)*c_in,c_out)\n",
        "        self.gdep = gdep\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    def forward(self,x,adj):\n",
        "        adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
        "        d = adj.sum(1)\n",
        "        h = x\n",
        "        out = [h]\n",
        "        a = adj / d.view(-1, 1)\n",
        "        for i in range(self.gdep):\n",
        "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,a)\n",
        "            out.append(h)\n",
        "        ho = torch.cat(out,dim=1)\n",
        "        ho = self.mlp(ho)\n",
        "        return ho\n",
        "\n",
        "class dy_mixprop(nn.Module):\n",
        "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
        "        super(dy_mixprop, self).__init__()\n",
        "        self.nconv = dy_nconv()\n",
        "        self.mlp1 = linear((gdep+1)*c_in,c_out)\n",
        "        self.mlp2 = linear((gdep+1)*c_in,c_out)\n",
        "\n",
        "        self.gdep = gdep\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "        self.lin1 = linear(c_in,c_in)\n",
        "        self.lin2 = linear(c_in,c_in)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        #adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
        "        #d = adj.sum(1)\n",
        "        x1 = torch.tanh(self.lin1(x))\n",
        "        x2 = torch.tanh(self.lin2(x))\n",
        "        adj = self.nconv(x1.transpose(2,1),x2)\n",
        "        adj0 = torch.softmax(adj, dim=2)\n",
        "        adj1 = torch.softmax(adj.transpose(2,1), dim=2)\n",
        "\n",
        "        h = x\n",
        "        out = [h]\n",
        "        for i in range(self.gdep):\n",
        "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,adj0)\n",
        "            out.append(h)\n",
        "        ho = torch.cat(out,dim=1)\n",
        "        ho1 = self.mlp1(ho)\n",
        "\n",
        "\n",
        "        h = x\n",
        "        out = [h]\n",
        "        for i in range(self.gdep):\n",
        "            h = self.alpha * x + (1 - self.alpha) * self.nconv(h, adj1)\n",
        "            out.append(h)\n",
        "        ho = torch.cat(out, dim=1)\n",
        "        ho2 = self.mlp2(ho)\n",
        "\n",
        "        return ho1+ho2\n",
        "\n",
        "\n",
        "\n",
        "class dilated_1D(nn.Module):\n",
        "    def __init__(self, cin, cout, dilation_factor=2):\n",
        "        super(dilated_1D, self).__init__()\n",
        "        self.tconv = nn.ModuleList()\n",
        "        self.kernel_set = [2,3,6,7]\n",
        "        self.tconv = nn.Conv2d(cin,cout,(1,7),dilation=(1,dilation_factor))\n",
        "\n",
        "    def forward(self,input):\n",
        "        x = self.tconv(input)\n",
        "        return x\n",
        "\n",
        "class dilated_inception(nn.Module):\n",
        "    def __init__(self, cin, cout, dilation_factor=2):\n",
        "        super(dilated_inception, self).__init__()\n",
        "        self.tconv = nn.ModuleList()\n",
        "        self.kernel_set = [2,3,6,7]\n",
        "        cout = int(cout/len(self.kernel_set))\n",
        "        for kern in self.kernel_set:\n",
        "            self.tconv.append(nn.Conv2d(cin,cout,(1,kern),dilation=(1,dilation_factor)))\n",
        "\n",
        "    def forward(self,input):\n",
        "        x = []\n",
        "        for i in range(len(self.kernel_set)):\n",
        "            x.append(self.tconv[i](input))\n",
        "        for i in range(len(self.kernel_set)):\n",
        "            x[i] = x[i][...,-x[-1].size(3):]\n",
        "        x = torch.cat(x,dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class graph_constructor(nn.Module):\n",
        "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
        "        super(graph_constructor, self).__init__()\n",
        "        self.nnodes = nnodes\n",
        "        if static_feat is not None:\n",
        "            xd = static_feat.shape[1]\n",
        "            self.lin1 = nn.Linear(xd, dim)\n",
        "            self.lin2 = nn.Linear(xd, dim)\n",
        "        else:\n",
        "            self.emb1 = nn.Embedding(nnodes, dim)\n",
        "            self.emb2 = nn.Embedding(nnodes, dim)\n",
        "            self.lin1 = nn.Linear(dim,dim)\n",
        "            self.lin2 = nn.Linear(dim,dim)\n",
        "\n",
        "        self.device = device\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.static_feat = static_feat\n",
        "\n",
        "    def forward(self, idx):\n",
        "        if self.static_feat is None:\n",
        "            nodevec1 = self.emb1(idx)\n",
        "            nodevec2 = self.emb2(idx)\n",
        "        else:\n",
        "            nodevec1 = self.static_feat[idx,:]\n",
        "            nodevec2 = nodevec1\n",
        "\n",
        "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
        "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
        "\n",
        "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))-torch.mm(nodevec2, nodevec1.transpose(1,0))\n",
        "        adj = F.relu(torch.tanh(self.alpha*a))\n",
        "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
        "        mask.fill_(float('0'))\n",
        "        s1,t1 = (adj + torch.rand_like(adj)*0.01).topk(self.k,1)\n",
        "        mask.scatter_(1,t1,s1.fill_(1))\n",
        "        adj = adj*mask\n",
        "        return adj\n",
        "\n",
        "    def fullA(self, idx):\n",
        "        if self.static_feat is None:\n",
        "            nodevec1 = self.emb1(idx)\n",
        "            nodevec2 = self.emb2(idx)\n",
        "        else:\n",
        "            nodevec1 = self.static_feat[idx,:]\n",
        "            nodevec2 = nodevec1\n",
        "\n",
        "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
        "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
        "\n",
        "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))-torch.mm(nodevec2, nodevec1.transpose(1,0))\n",
        "        adj = F.relu(torch.tanh(self.alpha*a))\n",
        "        return adj\n",
        "\n",
        "class graph_global(nn.Module):\n",
        "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
        "        super(graph_global, self).__init__()\n",
        "        self.nnodes = nnodes\n",
        "        self.A = nn.Parameter(torch.randn(nnodes, nnodes).to(device), requires_grad=True).to(device)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        return F.relu(self.A)\n",
        "\n",
        "\n",
        "class graph_undirected(nn.Module):\n",
        "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
        "        super(graph_undirected, self).__init__()\n",
        "        self.nnodes = nnodes\n",
        "        if static_feat is not None:\n",
        "            xd = static_feat.shape[1]\n",
        "            self.lin1 = nn.Linear(xd, dim)\n",
        "        else:\n",
        "            self.emb1 = nn.Embedding(nnodes, dim)\n",
        "            self.lin1 = nn.Linear(dim,dim)\n",
        "\n",
        "        self.device = device\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.static_feat = static_feat\n",
        "\n",
        "    def forward(self, idx):\n",
        "        if self.static_feat is None:\n",
        "            nodevec1 = self.emb1(idx)\n",
        "            nodevec2 = self.emb1(idx)\n",
        "        else:\n",
        "            nodevec1 = self.static_feat[idx,:]\n",
        "            nodevec2 = nodevec1\n",
        "\n",
        "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
        "        nodevec2 = torch.tanh(self.alpha*self.lin1(nodevec2))\n",
        "\n",
        "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
        "        adj = F.relu(torch.tanh(self.alpha*a))\n",
        "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
        "        mask.fill_(float('0'))\n",
        "        s1,t1 = adj.topk(self.k,1)\n",
        "        mask.scatter_(1,t1,s1.fill_(1))\n",
        "        adj = adj*mask\n",
        "        return adj\n",
        "\n",
        "\n",
        "\n",
        "class graph_directed(nn.Module):\n",
        "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
        "        super(graph_directed, self).__init__()\n",
        "        self.nnodes = nnodes\n",
        "        if static_feat is not None:\n",
        "            xd = static_feat.shape[1]\n",
        "            self.lin1 = nn.Linear(xd, dim)\n",
        "            self.lin2 = nn.Linear(xd, dim)\n",
        "        else:\n",
        "            self.emb1 = nn.Embedding(nnodes, dim)\n",
        "            self.emb2 = nn.Embedding(nnodes, dim)\n",
        "            self.lin1 = nn.Linear(dim,dim)\n",
        "            self.lin2 = nn.Linear(dim,dim)\n",
        "\n",
        "        self.device = device\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.static_feat = static_feat\n",
        "\n",
        "    def forward(self, idx):\n",
        "        if self.static_feat is None:\n",
        "            nodevec1 = self.emb1(idx)\n",
        "            nodevec2 = self.emb2(idx)\n",
        "        else:\n",
        "            nodevec1 = self.static_feat[idx,:]\n",
        "            nodevec2 = nodevec1\n",
        "\n",
        "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
        "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
        "\n",
        "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
        "        adj = F.relu(torch.tanh(self.alpha*a))\n",
        "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
        "        mask.fill_(float('0'))\n",
        "        s1,t1 = adj.topk(self.k,1)\n",
        "        mask.scatter_(1,t1,s1.fill_(1))\n",
        "        adj = adj*mask\n",
        "        return adj\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    __constants__ = ['normalized_shape', 'weight', 'bias', 'eps', 'elementwise_affine']\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        self.normalized_shape = tuple(normalized_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "            self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.elementwise_affine:\n",
        "            init.ones_(self.weight)\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, idx):\n",
        "        if self.elementwise_affine:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight[:,idx,:], self.bias[:,idx,:], self.eps)\n",
        "        else:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight, self.bias, self.eps)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)\n"
      ],
      "metadata": {
        "id": "pWhZKUObCf2x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from layer import *\n",
        "#net.py\n",
        "\n",
        "class gtnet(nn.Module):\n",
        "    def __init__(self, gcn_true, buildA_true, gcn_depth, num_nodes, device, predefined_A=None, static_feat=None, dropout=0.3, subgraph_size=20, node_dim=40, dilation_exponential=1, conv_channels=32, residual_channels=32, skip_channels=64, end_channels=128, seq_length=12, in_dim=2, out_dim=12, layers=3, propalpha=0.05, tanhalpha=3, layer_norm_affline=True):\n",
        "        super(gtnet, self).__init__()\n",
        "        self.gcn_true = gcn_true\n",
        "        self.buildA_true = buildA_true\n",
        "        self.num_nodes = num_nodes\n",
        "        self.dropout = dropout\n",
        "        self.predefined_A = predefined_A\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        self.gconv1 = nn.ModuleList()\n",
        "        self.gconv2 = nn.ModuleList()\n",
        "        self.norm = nn.ModuleList()\n",
        "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=(1, 1))\n",
        "        self.gc = graph_constructor(num_nodes, subgraph_size, node_dim, device, alpha=tanhalpha, static_feat=static_feat)\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        kernel_size = 7\n",
        "        if dilation_exponential>1:\n",
        "            self.receptive_field = int(1+(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
        "        else:\n",
        "            self.receptive_field = layers*(kernel_size-1) + 1\n",
        "\n",
        "        for i in range(1):\n",
        "            if dilation_exponential>1:\n",
        "                rf_size_i = int(1 + i*(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
        "            else:\n",
        "                rf_size_i = i*layers*(kernel_size-1)+1\n",
        "            new_dilation = 1\n",
        "            for j in range(1,layers+1):\n",
        "                if dilation_exponential > 1:\n",
        "                    rf_size_j = int(rf_size_i + (kernel_size-1)*(dilation_exponential**j-1)/(dilation_exponential-1))\n",
        "                else:\n",
        "                    rf_size_j = rf_size_i+j*(kernel_size-1)\n",
        "\n",
        "                self.filter_convs.append(dilated_inception(residual_channels, conv_channels, dilation_factor=new_dilation))\n",
        "                self.gate_convs.append(dilated_inception(residual_channels, conv_channels, dilation_factor=new_dilation))\n",
        "                self.residual_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                                    out_channels=residual_channels,\n",
        "                                                 kernel_size=(1, 1)))\n",
        "                if self.seq_length>self.receptive_field:\n",
        "                    self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                                    out_channels=skip_channels,\n",
        "                                                    kernel_size=(1, self.seq_length-rf_size_j+1)))\n",
        "                else:\n",
        "                    self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                                    out_channels=skip_channels,\n",
        "                                                    kernel_size=(1, self.receptive_field-rf_size_j+1)))\n",
        "\n",
        "                if self.gcn_true:\n",
        "                    self.gconv1.append(mixprop(conv_channels, residual_channels, gcn_depth, dropout, propalpha))\n",
        "                    self.gconv2.append(mixprop(conv_channels, residual_channels, gcn_depth, dropout, propalpha))\n",
        "\n",
        "                if self.seq_length>self.receptive_field:\n",
        "                    self.norm.append(LayerNorm((residual_channels, num_nodes, self.seq_length - rf_size_j + 1),elementwise_affine=layer_norm_affline))\n",
        "                else:\n",
        "                    self.norm.append(LayerNorm((residual_channels, num_nodes, self.receptive_field - rf_size_j + 1),elementwise_affine=layer_norm_affline))\n",
        "\n",
        "                new_dilation *= dilation_exponential\n",
        "\n",
        "        self.layers = layers\n",
        "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
        "                                             out_channels=end_channels,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
        "                                             out_channels=out_dim,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        if self.seq_length > self.receptive_field:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.seq_length), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, self.seq_length-self.receptive_field+1), bias=True)\n",
        "\n",
        "        else:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.receptive_field), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, 1), bias=True)\n",
        "\n",
        "\n",
        "        self.idx = torch.arange(self.num_nodes).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, input, idx=None):\n",
        "        seq_len = input.size(3)\n",
        "        assert seq_len==self.seq_length, 'input sequence length not equal to preset sequence length'\n",
        "\n",
        "        if self.seq_length<self.receptive_field:\n",
        "            input = nn.functional.pad(input,(self.receptive_field-self.seq_length,0,0,0))\n",
        "\n",
        "\n",
        "\n",
        "        if self.gcn_true:\n",
        "            if self.buildA_true:\n",
        "                if idx is None:\n",
        "                    adp = self.gc(self.idx)\n",
        "                else:\n",
        "                    adp = self.gc(idx)\n",
        "            else:\n",
        "                adp = self.predefined_A\n",
        "\n",
        "        x = self.start_conv(input)\n",
        "        skip = self.skip0(F.dropout(input, self.dropout, training=self.training))\n",
        "        for i in range(self.layers):\n",
        "            residual = x\n",
        "            filter = self.filter_convs[i](x)\n",
        "            filter = torch.tanh(filter)\n",
        "            gate = self.gate_convs[i](x)\n",
        "            gate = torch.sigmoid(gate)\n",
        "            x = filter * gate\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            s = x\n",
        "            s = self.skip_convs[i](s)\n",
        "            skip = s + skip\n",
        "            if self.gcn_true:\n",
        "                x = self.gconv1[i](x, adp)+self.gconv2[i](x, adp.transpose(1,0))\n",
        "            else:\n",
        "                x = self.residual_convs[i](x)\n",
        "\n",
        "            x = x + residual[:, :, :, -x.size(3):]\n",
        "            if idx is None:\n",
        "                x = self.norm[i](x,self.idx)\n",
        "            else:\n",
        "                x = self.norm[i](x,idx)\n",
        "\n",
        "        skip = self.skipE(x) + skip\n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "x16a5qW2CaJo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import math\n",
        "#from net import *\n",
        "#import util"
      ],
      "metadata": {
        "id": "PNK111lPCVsk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainer\n",
        "import torch.optim as optim\n",
        "import math\n",
        "#from net import *\n",
        "#import util\n",
        "class Trainer():\n",
        "    def __init__(self, model, lrate, wdecay, clip, step_size, seq_out_len, scaler, device, cl=True):\n",
        "        self.scaler = scaler\n",
        "        self.model = model\n",
        "        self.model.to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "        self.loss = util.masked_mae\n",
        "        self.clip = clip\n",
        "        self.step = step_size\n",
        "        self.iter = 1\n",
        "        self.task_level = 1\n",
        "        self.seq_out_len = seq_out_len\n",
        "        self.cl = cl\n",
        "\n",
        "    def train(self, input, real_val, idx=None):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(input, idx=idx)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        if self.iter%self.step==0 and self.task_level<=self.seq_out_len:\n",
        "            self.task_level +=1\n",
        "        if self.cl:\n",
        "            loss = self.loss(predict[:, :, :, :self.task_level], real[:, :, :, :self.task_level], 0.0)\n",
        "        else:\n",
        "            loss = self.loss(predict, real, 0.0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "\n",
        "        self.optimizer.step()\n",
        "        # mae = util.masked_mae(predict,real,0.0).item()\n",
        "        mape = util.masked_mape(predict,real,0.0).item()\n",
        "        rmse = util.masked_rmse(predict,real,0.0).item()\n",
        "        self.iter += 1\n",
        "        return loss.item(),mape,rmse\n",
        "\n",
        "    def eval(self, input, real_val):\n",
        "        self.model.eval()\n",
        "        output = self.model(input)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        loss = self.loss(predict, real, 0.0)\n",
        "        mape = util.masked_mape(predict,real,0.0).item()\n",
        "        rmse = util.masked_rmse(predict,real,0.0).item()\n",
        "        return loss.item(),mape,rmse\n",
        "\n",
        "\n",
        "\n",
        "class Optim(object):\n",
        "\n",
        "    def _makeOptimizer(self):\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
        "        elif self.method == 'adadelta':\n",
        "            self.optimizer = optim.Adadelta(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def __init__(self, params, method, lr, clip, lr_decay=1, start_decay_at=None):\n",
        "        self.params = params  # careful: params may be a generator\n",
        "        self.last_ppl = None\n",
        "        self.lr = lr\n",
        "        self.clip = clip\n",
        "        self.method = method\n",
        "        self.lr_decay = lr_decay\n",
        "        self.start_decay_at = start_decay_at\n",
        "        self.start_decay = False\n",
        "\n",
        "        self._makeOptimizer()\n",
        "\n",
        "    def step(self):\n",
        "        # Compute gradients norm.\n",
        "        grad_norm = 0\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.params, self.clip)\n",
        "\n",
        "        # for param in self.params:\n",
        "        #     grad_norm += math.pow(param.grad.data.norm(), 2)\n",
        "        #\n",
        "        # grad_norm = math.sqrt(grad_norm)\n",
        "        # if grad_norm > 0:\n",
        "        #     shrinkage = self.max_grad_norm / grad_norm\n",
        "        # else:\n",
        "        #     shrinkage = 1.\n",
        "        #\n",
        "        # for param in self.params:\n",
        "        #     if shrinkage < 1:\n",
        "        #         param.grad.data.mul_(shrinkage)\n",
        "        self.optimizer.step()\n",
        "        return  grad_norm\n",
        "\n",
        "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
        "    def updateLearningRate(self, ppl, epoch):\n",
        "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
        "            self.start_decay = True\n",
        "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
        "            self.start_decay = True\n",
        "\n",
        "        if self.start_decay:\n",
        "            self.lr = self.lr * self.lr_decay\n",
        "            print(\"Decaying learning rate to %g\" % self.lr)\n",
        "        #only decay for one epoch\n",
        "        self.start_decay = False\n",
        "\n",
        "        self.last_ppl = ppl\n",
        "\n",
        "        self._makeOptimizer()\n"
      ],
      "metadata": {
        "id": "i4zIp8tiDHJz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from util import *\n",
        "#from trainer import Optim\n",
        "#from net import gtnet"
      ],
      "metadata": {
        "id": "xFATKLd2B9UW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_single_step.py\n",
        "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_loss_l1 = 0\n",
        "    n_samples = 0\n",
        "    predict = None\n",
        "    test = None\n",
        "\n",
        "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
        "        X = torch.unsqueeze(X,dim=1)\n",
        "        X = X.transpose(2,3)\n",
        "        with torch.no_grad():\n",
        "            output = model(X)\n",
        "        output = torch.squeeze(output)\n",
        "        if len(output.shape)==1:\n",
        "            output = output.unsqueeze(dim=0)\n",
        "        if predict is None:\n",
        "            predict = output\n",
        "            test = Y\n",
        "        else:\n",
        "            predict = torch.cat((predict, output))\n",
        "            test = torch.cat((test, Y))\n",
        "\n",
        "        scale = data.scale.expand(output.size(0), data.m)\n",
        "        total_loss += evaluateL2(output * scale, Y * scale).item()\n",
        "        total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
        "        n_samples += (output.size(0) * data.m)\n",
        "\n",
        "    rse = math.sqrt(total_loss / n_samples) / data.rse\n",
        "    rae = (total_loss_l1 / n_samples) / data.rae\n",
        "\n",
        "    predict = predict.data.cpu().numpy()\n",
        "    Ytest = test.data.cpu().numpy()\n",
        "    sigma_p = (predict).std(axis=0)\n",
        "    sigma_g = (Ytest).std(axis=0)\n",
        "    mean_p = predict.mean(axis=0)\n",
        "    mean_g = Ytest.mean(axis=0)\n",
        "    index = (sigma_g != 0)\n",
        "    correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis=0) / (sigma_p * sigma_g)\n",
        "    correlation = (correlation[index]).mean()\n",
        "    return rse, rae, correlation\n"
      ],
      "metadata": {
        "id": "Y_-Pwc7HDS0c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_single_step.py\n",
        "def train(data, X, Y, model, criterion, optim, batch_size):\n",
        "    print(\"train()\")\n",
        "    numcycle=2\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_samples = 0\n",
        "    iter = 0\n",
        "    cycle=0\n",
        "    for X, Y in data.get_batches(X, Y, batch_size, True):\n",
        "        \n",
        "        #if  cycle> numcycle:\n",
        "            #print(\"break:--->\",cycle)\n",
        "            #break\n",
        "        \n",
        "        model.zero_grad()\n",
        "        X = torch.unsqueeze(X,dim=1)\n",
        "        X = X.transpose(2,3)\n",
        "        fff=iter % step_size\n",
        "        #print(\"iter:\",iter)\n",
        "        #print(\"fff:\",fff)\n",
        "        if  fff== 0:\n",
        "            perm = np.random.permutation(range(num_nodes))\n",
        "        num_sub = int(num_nodes / num_split)\n",
        "\n",
        "        for j in range(num_split):\n",
        "            if j != num_split - 1:\n",
        "                id = perm[j * num_sub:(j + 1) * num_sub]\n",
        "            else:\n",
        "                id = perm[j * num_sub:]\n",
        "                \n",
        "                \n",
        "            # id = torch.tensor(id)\n",
        "            id = torch.LongTensor(id).to(device)\n",
        "            # print(\"id:\",id)\n",
        "            #id = torch.tensor(id).type(torch(bool))\n",
        "            #print(\"id:\",id)\n",
        "            #id = torch.tensor(id).type(torch(bool)).to(device)\n",
        "            #print(\"id:\",id)\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "           \n",
        "            tx = X[:, :, id, :]\n",
        "            ty = Y[:, id]\n",
        "            output = model(tx,id)\n",
        "            output = torch.squeeze(output)\n",
        "            scale = data.scale.expand(output.size(0), data.m)\n",
        "            scale = scale[:,id]\n",
        "            loss = criterion(output * scale, ty * scale)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            n_samples += (output.size(0) * data.m)\n",
        "            grad_norm = optim.step()\n",
        "\n",
        "        fssfsf=iter%100\n",
        "        #print(\"iter%100:\",fssfsf)\n",
        "        if fssfsf ==0:\n",
        "            print('iter:{:3d} | loss: {:.3f}'.format(iter,loss.item()/(output.size(0) * data.m)))\n",
        "        iter += 1\n",
        "        cycle=cycle+1\n",
        "        #print(\"cycle:--->\",cycle)\n",
        "    return total_loss / n_samples\n"
      ],
      "metadata": {
        "id": "IEZRD125DU2C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive/')"
      ],
      "metadata": {
        "id": "xR_H4EUVHN8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f022a6-dc1b-4c77-f1ec-7ca494675364"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parser = argparse.ArgumentParser(description='PyTorch Time series forecasting'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data='solar_AL.txt'#,help='location of the data file'\n",
        "log_interval=2000\n",
        "metavar='N'#,help='report interval'\n",
        "save='model/model.pt'#,help='path to save the final model'\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t\n",
        "optim='adam'\n",
        "L1Loss=True\n",
        "normalize=2\n",
        "#device='cuda:1'\n",
        "device='cuda:0'\n",
        "gcn_true=True\n",
        "buildA_true=True\n",
        "gcn_depth=2\n",
        "num_nodes=137\n",
        "dropout=0.3#,help='dropout rate'\n",
        "subgraph_size=20#,help='k'\n",
        "node_dim=40#,help='dim of nodes'\n",
        "dilation_exponential=2#,help='dilation exponential'\n",
        "conv_channels=16#,help='convolution channels'\n",
        "residual_channels=16#,help='residual channels'\n",
        "skip_channels=32#,help='skip channels'\n",
        "end_channels=64#,help='end channels'\n",
        "in_dim=1#,help='inputs dimension'\n",
        "seq_in_len=24*7#,help='input sequence length'\n",
        "seq_out_len=1#,help='output sequence length'\n",
        "horizon=3\n",
        "layers=5#,help='number of layers'\n",
        "\n",
        "batch_size=32#,help='batch size'\n",
        "lr=0.0001#,help='learning rate'\n",
        "weight_decay=0.00001#,help='weight decay rate'\n",
        "\n",
        "clip=5#,help='clip'\n",
        "\n",
        "propalpha=0.05#,help='prop alpha'\n",
        "tanhalpha=3#,help='tanh alpha'\n",
        "\n",
        "epochs=1\n",
        "num_split=1#,help='number of splits for graphs'\n",
        "step_size=100#,help='step_size'"
      ],
      "metadata": {
        "id": "COHPvU4JF2RQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_single_step.py\n",
        "def main():\n",
        "\n",
        "    Data = DataLoaderS(data, 0.6, 0.2, device, horizon, seq_in_len, normalize)\n",
        "\n",
        "    model = gtnet(gcn_true, buildA_true, gcn_depth, num_nodes,\n",
        "                  device, dropout=dropout, subgraph_size=subgraph_size,\n",
        "                  node_dim=node_dim, dilation_exponential=dilation_exponential,\n",
        "                  conv_channels=conv_channels, residual_channels=residual_channels,\n",
        "                  skip_channels=skip_channels, end_channels= end_channels,\n",
        "                  seq_length=seq_in_len, in_dim=in_dim, out_dim=seq_out_len,\n",
        "                  layers=layers, propalpha=propalpha, tanhalpha=tanhalpha, layer_norm_affline=False)\n",
        "    model = model.to(device)\n",
        "\n",
        "    #print(args)\n",
        "    print('The recpetive field size is', model.receptive_field)\n",
        "    nParams = sum([p.nelement() for p in model.parameters()])\n",
        "    print('Number of model parameters is', nParams, flush=True)\n",
        "\n",
        "    if L1Loss:\n",
        "        criterion = nn.L1Loss(size_average=False).to(device)\n",
        "    else:\n",
        "        criterion = nn.MSELoss(size_average=False).to(device)\n",
        "    evaluateL2 = nn.MSELoss(size_average=False).to(device)\n",
        "    evaluateL1 = nn.L1Loss(size_average=False).to(device)\n",
        "\n",
        "\n",
        "    best_val = 10000000\n",
        "    optim = Optim(\n",
        "        model.parameters(), optim, lr, clip, lr_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # At any point you can hit Ctrl + C to break out of training early.\n",
        "    try:\n",
        "        print('begin training')\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time = time.time()\n",
        "            train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, optim, batch_size)\n",
        "            val_loss, val_rae, val_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
        "                                               batch_size)\n",
        "            print(\n",
        "                '| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(\n",
        "                    epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr), flush=True)\n",
        "            # Save the model if the validation loss is the best we've seen so far.\n",
        "\n",
        "            if val_loss < best_val:\n",
        "                with open(save, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val = val_loss\n",
        "            if epoch % 5 == 0:\n",
        "                test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
        "                                                     batch_size)\n",
        "                print(\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr), flush=True)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    # Load the best saved model.\n",
        "    with open(save, 'rb') as f:\n",
        "        model = torch.load(f)\n",
        "\n",
        "    vtest_acc, vtest_rae, vtest_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
        "                                         batch_size)\n",
        "    test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
        "                                         batch_size)\n",
        "    print(\"final test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
        "    return vtest_acc, vtest_rae, vtest_corr, test_acc, test_rae, test_corr\n"
      ],
      "metadata": {
        "id": "vlCZqiEPDiEx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_single_step.py\n",
        "def main():\n",
        "    Data = DataLoaderS(data, 0.6, 0.2, device, horizon, seq_in_len, normalize)\n",
        "    model = gtnet(gcn_true, buildA_true, gcn_depth, num_nodes,\n",
        "                      device, dropout=dropout, subgraph_size=subgraph_size,\n",
        "                      node_dim=node_dim, dilation_exponential=dilation_exponential,\n",
        "                      conv_channels=conv_channels, residual_channels=residual_channels,\n",
        "                      skip_channels=skip_channels, end_channels= end_channels,\n",
        "                      seq_length=seq_in_len, in_dim=in_dim, out_dim=seq_out_len,\n",
        "                      layers=layers, propalpha=propalpha, tanhalpha=tanhalpha, layer_norm_affline=False)\n",
        "    model = model.to(device)"
      ],
      "metadata": {
        "id": "viUWZDKQHpst"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "_3wGjyeQHgJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save= \"/model-solar-3.pt\"\n",
        "num_nodes= 137 \n",
        "batch_size=1 \n",
        "epochs=1 \n",
        "horizon=3 \n",
        "step_size=1 \n",
        "device=\"cuda:0\"\n",
        "device=\"cpu\"\n",
        "\n",
        "#train_single_step.py\n",
        "vacc = []\n",
        "vrae = []\n",
        "vcorr = []\n",
        "acc = []\n",
        "rae = []\n",
        "corr = []\n",
        "for i in range(2):\n",
        "    main()\n",
        " "
      ],
      "metadata": {
        "id": "AqDSTmnqDlOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " vacc.append(val_acc)\n",
        "  vrae.append(val_rae)\n",
        "  vcorr.append(val_corr)\n",
        "  acc.append(test_acc)\n",
        "  rae.append(test_rae)\n",
        "  corr.append(test_corr)\n",
        "print('\\n\\n')\n",
        "print('10 runs average')\n",
        "print('\\n\\n')\n",
        "print(\"valid\\trse\\trae\\tcorr\")\n",
        "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(vacc), np.mean(vrae), np.mean(vcorr)))\n",
        "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(vacc), np.std(vrae), np.std(vcorr)))\n",
        "print('\\n\\n')\n",
        "print(\"test\\trse\\trae\\tcorr\")\n",
        "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(acc), np.mean(rae), np.mean(corr)))\n",
        "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(acc), np.std(rae), np.std(corr)))"
      ],
      "metadata": {
        "id": "Vi4FewZlHXmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyXnYPwfhzT"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww-dj07OG0TO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive/')\n",
        "#4/1AX4XfWj4gZil2kGwsKoDw0jn6UuUv1lyfMwtQPErYTPJe9_uq8OKay-tj0I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI6z9lG8fpl5"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "labelFile=\"Sampled_labels.pck\"\n",
        "inputsFile=\"Sampled_inputs.pck\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHrUCJKSgDL9"
      },
      "outputs": [],
      "source": [
        "#inputsFile=\"C:\\\\Users\\\\XXXX\\\\Desktop\\\\Sampled_inputs.pck\"\n",
        "#labelFile=\"C:\\\\Users\\\\XXXX\\\\Desktop\\\\Sampled_labels.pck\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpK6f07qJGwm"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        obj = pickle.load(fp)\n",
        "    return obj\n",
        "\n",
        "Sampled_inputs=load(inputsFile)\n",
        "Sampled_labels=load(labelFile)\n",
        "#print(Sampled_inputs.shape)\n",
        "#print(Sampled_inputs[0])\n",
        "#print(Sampled_inputs)\n",
        "temp=Sampled_inputs[0]\n",
        "#print(temp)\n",
        "df = pd.DataFrame(temp)\n",
        "trainData=Sampled_inputs\n",
        "trainLebel=Sampled_labels\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "print(\"trainLebel.shape: \",trainLebel.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZT-utEhpsVf"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KtxaVyLEJMK"
      },
      "outputs": [],
      "source": [
        "#four-class problem {X, M, B/C, Q}\n",
        "print(\"np.unique(trainLebel): \",np.unique(trainLebel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn7rRDm3a1De"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA40fAsxMjSR"
      },
      "outputs": [],
      "source": [
        "\n",
        "temptrainData=np.empty([1540,60, 33])\n",
        "n=len(trainData)\n",
        "for l in range(0, n):\n",
        "  temp=trainData[l]\n",
        "  #print(temp)\n",
        "  #temp=np.transpose(temp)\n",
        "  temp=temp.T\n",
        "  #print(temp.shape)\n",
        "  #print(temp)\n",
        "  temptrainData[l,:,:]=temp\n",
        "  n=n+1\n",
        "  #np.append(temptrainData, temp)\n",
        "  #print(temptrainData)\n",
        "\n",
        "#print(temptrainData.shape)\n",
        "#print(trainData.shape) \n",
        "trainData=temptrainData\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "#print(trainData[0])\n",
        "\n",
        "temp=trainData[0]\n",
        "#print(temp)\n",
        "df = pd.DataFrame(temp)\n",
        "#df=pd.DataFrame.from_dict(trainData)\n",
        "trainData222=trainData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYxXXp5tj2YZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdGdcLLjRYw9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "print(trainData.shape)\n",
        "\n",
        "print(type(trainData))\n",
        "npArrays=[]\n",
        "for l in range(0, len(trainData)):\n",
        "  trainData_std = sc.fit_transform(trainData[l])\n",
        "  #trainData_std = trainData_std.astype(np.float64)\n",
        "  #print(type(trainData_std[0][0]))\n",
        "  npArrays.append(trainData_std)\n",
        "\n",
        "\n",
        "print(type(npArrays))\n",
        "arr = np.asarray(npArrays)\n",
        "print(type(arr))\n",
        "trainData=arr\n",
        "df = pd.DataFrame(trainData[0])\n",
        "\n",
        "#print(npArrays.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXhxMJ7PZU-c"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZmYGWKTbhsK"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 1320\n",
        "HIDDEN_DIM = 64\n",
        "NUM_TS = 60\n",
        "NUM_CLASSES = 4\n",
        "#NUM_CLASSES = 1\n",
        "num_layers = 1 #number of stacked lstm layers\n",
        "hidden_size=HIDDEN_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WymsvIrI2M8x"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 33\n",
        "HIDDEN_DIM = 33\n",
        "NUM_TS = 60\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "\n",
        "class LSTM_MVTS_LRN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, num_steps): #input_dim = 33, hidden_dim = 33\n",
        "    super(LSTM_MVTS_LRN, self).__init__()\n",
        "    self.input_dim = 33#input_dim\n",
        "    self.hidden_dim = 33#hidden_dim\n",
        "    self.output_dim = 33#input_dim\n",
        "    self.lstm1 = nn.LSTM(33, 33) #encoder\n",
        "    self.lstm2 = nn.LSTM(33, 33) #decoder\n",
        "    self.num_steps = num_steps\n",
        "    #self.hidden2class = nn.Linear(hidden_dim, num_classes)\n",
        "  def forward(self, mvts):\n",
        "    #print(\"model():\",\"mvts.shape: \",mvts.shape)#[40, 33]\n",
        "    #input single mvts (60, 33); output class probability vector (1,4)\n",
        "    \n",
        "    input1=mvts.view(len(mvts), 1, -1)\n",
        "    #print(\"model():\",\"input1.shape: \",input1.shape) #[40, 1, 33]\n",
        "   \n",
        "    lstm_out, _ = self.lstm1(input1) #mvts.shape: (40, 33); len(mvts)=40; new shape: (40, 1, 33); lstm_out1 --> (40, 128)\n",
        "    #print(\"model():\",\"lstm_out.shape: \",lstm_out.shape) #[40, 1, 33]\n",
        "\n",
        "    lstm_out2 = lstm_out[:, -1, :] \n",
        "    #print(\"model():\",\"lstm_out2.shape: \",lstm_out2.shape) #[40, 33]\n",
        "\n",
        "    lstm_out_of_lastRow = lstm_out[len(lstm_out2)-1] #vector of 40th timestep : [1,128]\n",
        "    #print(\"model():\",\"lstm_out_of_lastRow.shape: \",lstm_out_of_lastRow.shape)#(([1, 33]\n",
        "\n",
        " \n",
        "    num=20\n",
        "    predicted_timestamp_vectors = torch.zeros((num,33)) #\n",
        "    #print(\"predicted_timestamp_vectors \",predicted_timestamp_vectors.shape)#(([20, 33]\n",
        "    for i in range(num):\n",
        "      #last_lstm2_out =  self.lstm2(lstm_out_of_lastRow) #vecotr of 41st timestamp: [1,33]\n",
        "      \n",
        "      input2=lstm_out_of_lastRow.view(len(lstm_out_of_lastRow), 1, -1)\n",
        "      #print(\"model():\",\"input2.shape: \",input2.shape) #[1, 1, 33]\n",
        "\n",
        "      last_lstm2_out, _ = self.lstm2(input2)\n",
        "      #print(\"last_lstm2_out-\",i, last_lstm2_out.shape)#([1, 1, 33]\n",
        "      last_lstm2_out2 = last_lstm2_out[:, -1, :] \n",
        "      #print(\"model():\",i,\"last_lstm2_out2.shape: \",last_lstm2_out2.shape) #[1, 33]\n",
        "      predicted_timestamp_vectors[i, :] = last_lstm2_out2\n",
        "\n",
        "\n",
        "\n",
        "    #lstm_out2, _ = self.lstm2(lstm_out1.view(len(lstm_out1), 1, -1)) #lstm_out1.shape: (40, 128); len(mvts)=40; new shape: (40, 1, 128); lstm_out2 --> (40, 128) -> can use later \n",
        "    #lstm_out2_clipped = lstm_out[0:20,:] -> can use later\n",
        "    #return lstm_out2_clipped \n",
        "    #return lstm_out\n",
        "    #print(\"model() predicted_timestamp_vectors.shape: \",predicted_timestamp_vectors.shape) #[1, 33]\n",
        "    return predicted_timestamp_vectors\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GV7Is6I8S-O"
      },
      "outputs": [],
      "source": [
        "def showBarChart(test_no,losses,label,xlabel,ylabel):\n",
        "              import matplotlib.pyplot as plt\n",
        "              #print(\"showCurve() test_no:\",test_no)\n",
        "              #print(losses)\n",
        "              #plt.plot(test_no, losses, label = 'Test error')\n",
        "              plt.bar(test_no, losses, label = label)\n",
        "              plt.xlabel(xlabel)\n",
        "              plt.ylabel(ylabel)\n",
        "              plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ1m0dmrZNjH"
      },
      "outputs": [],
      "source": [
        "def showBarChart2(test_no2,losses2,label2):\n",
        "              import matplotlib.pyplot as plt2\n",
        "              #print(\"showCurve() test_no:\",test_no)\n",
        "              print(losses2)\n",
        "              #plt.plot(test_no, losses, label = 'Test error')\n",
        "              plt2.bar(test_no2, losses2, label = label2)\n",
        "              plt2.xlabel(\"epoch\")\n",
        "              plt2.ylabel(\"losses\")\n",
        "              plt2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcVbhOwqZ7yb"
      },
      "outputs": [],
      "source": [
        "def showCurve(test_no,losses,label):\n",
        "              import matplotlib.pyplot as plt\n",
        "              #print(\"showCurve() test_no:\",test_no)\n",
        "              #print(losses)\n",
        "              plt.plot(test_no, losses, label = label)\n",
        "              plt.xlabel(\"epoch\")\n",
        "              plt.ylabel(\"losses\")\n",
        "              plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxvvcgO9fhzW"
      },
      "outputs": [],
      "source": [
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZSRSDCXfhzZ"
      },
      "outputs": [],
      "source": [
        "def doClassSpecificCalulcation(Accuracy,trainLebel,classification_report_dict):\n",
        "  print('\\np.mean(Accuracy) :',np.mean(Accuracy))\n",
        "  print('\\np.std(Accuracy) :',np.std(Accuracy))\n",
        "  print('\\n33333333 p.mean np.std(Accuracy) :     ',np.round(np.mean(Accuracy),2),\"+-\",np.round(np.std(Accuracy),2) )\n",
        "  for j in range( len(np.unique(trainLebel)) ):\n",
        "    print('\\n\\n\\n\\nclass :',j) \n",
        "    precision=[]\n",
        "    recall=[]\n",
        "    f1_score=[]\n",
        "    for i in range(len(classification_report_dict)):\n",
        "      report=classification_report_dict[i]\n",
        "      #print('classification_report : \\n',report) \n",
        "      temp=report[str(j)]['precision'] \n",
        "      precision.append(temp)\n",
        "\n",
        "      temp=report[str(j)]['recall'] \n",
        "      recall.append(temp)\n",
        "\n",
        "      temp=report[str(j)]['f1-score'] \n",
        "      f1_score.append(temp)\n",
        "\n",
        "    print('\\np.mean(precision) \\t p.mean(recall) \\t p.mean(f1_score) :') \n",
        "\n",
        "\n",
        "    print(np.mean(precision)) \n",
        "    print(np.mean(recall)) \n",
        "    print(np.mean(f1_score))\n",
        "\n",
        "    print('\\np.mean p.std(precision) \\tp.mean  p.std(recall) \\tp.mean  p.std(f1_score) :')\n",
        "\n",
        "    print(np.round(np.mean(precision),2),\"+-\",np.round(np.std(precision),2) )\n",
        "    print(np.round(np.mean(recall),2),\"+-\",np.round(np.std(recall),2) )\n",
        "    print(np.round(np.mean(f1_score),2),\"+-\",np.round(np.std(f1_score),2) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht8P3MN66O0E"
      },
      "outputs": [],
      "source": [
        "def doTrainingForThisMVTS(mvts,model,loss_function):\n",
        "           \n",
        "            #print(\"mvts no:\",i ,\"input mvts.shape: \",mvts.shape)\n",
        "            #df = pd.DataFrame(mvts)\n",
        "            #print(df)\n",
        "            mvts = torch.from_numpy(mvts).float()\n",
        "            mvts_input = mvts[0:40,:]\n",
        "            df_mvts_input = pd.DataFrame(mvts_input)\n",
        "            #print(df_mvts_input)\n",
        "\n",
        "            #mvts = mvts.to(device)#print(mvts.is_cuda)\n",
        "            #mvts = mvts.view(mvts.size(0), -1)\n",
        "            #print(\"mvts no:\",i ,\"df_mvts_input.shape: \",df_mvts_input.shape)\n",
        "            #print(\"mvts no:\",i ,\"mvts_input.shape: \",mvts_input.shape)\n",
        "            Predicted_20_rows = model(mvts_input) #Predicted_20_rows -> [20, 33]\n",
        "            #print(\"mvts no:\",i ,\"Predicted_20_rows.shape: \",Predicted_20_rows.shape)\n",
        "\n",
        "\n",
        "            target = mvts[40:60,:]\n",
        "            #df_target = pd.DataFrame(target)\n",
        "\n",
        "            #print(\"mvts no:\",i ,\"target.shape: \",target.shape)\n",
        "            target = torch.from_numpy(np.array(target)).float()\n",
        "            #print(\"mvts no:\",i ,\"target.shape: \",target.shape)\n",
        "            loss_total=0\n",
        "            for k in range(20):\n",
        "               tttt = loss_function(Predicted_20_rows[k], target[k]) #distance calculate\n",
        "               #print(\"row no:\",k ,\" loss: \",tttt)\n",
        "               loss_total=loss_total+tttt\n",
        "            return loss_total "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdWsmh8BVF7I"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "#losses = []\n",
        "\n",
        "losses_test = []\n",
        "test_no_test = []\n",
        "df_mvts_input=[]\n",
        "df_target=[]\n",
        "\n",
        "totalLoss_epoch_test2=[]\n",
        "totalLoss_epoch2=[]\n",
        "epochs=[]\n",
        "def doLstmBasedCalculations():\n",
        "    HIDDEN_DIM=20*33\n",
        "    num_masterIteration=1\n",
        "\n",
        "    print(\"trainData.shape: \",trainData.shape)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(trainData, trainLebel, test_size=0.3, random_state=0,shuffle = False)\n",
        "    print(\"X_train.shape X_test.shape y_train.shape y_test.shape \",X_train.shape, X_test.shape ,y_train.shape, y_test.shape)\n",
        " \n",
        "    classification_report_dict=[]\n",
        "    Accuracy=[]\n",
        "    for masterIteration in range(num_masterIteration):\n",
        "        #print(\"\\nmasterIteration HIDDEN_DIM : \",masterIteration, HIDDEN_DIM)\n",
        "        #print(bcolors.WARNING + \"\\nmasterIteration :\" + bcolors.WARNING,masterIteration)\n",
        "        #random_state=random.randint(42, 100)\n",
        "        #print(\"random_state: \",random_state)\n",
        "\n",
        "\n",
        "        model = LSTM_MVTS_LRN(INPUT_DIM, \n",
        "                              #hds,\n",
        "                              660, \n",
        "                              NUM_CLASSES)\n",
        "        #loss_function = nn.NLLLoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "        loss_function = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
        "\n",
        "        num_mvts_to_process =trainData.shape[0]# X_train.shape[0] #numTrain#200 #\n",
        "        num_mvts_to_process=20\n",
        "        numEpochs =500\n",
        "        numEpochs =5\n",
        "        model.zero_grad()\n",
        "        #train\n",
        "       \n",
        "        for epoch in range(numEpochs):\n",
        "          print(\"\\n masterIteration: \",masterIteration, \"     epoch: \",epoch)\n",
        "          totalLoss_epoch=0\n",
        "          #loss_values = []\n",
        "          #running_loss = 0.0\n",
        "\n",
        "          \n",
        "         \n",
        "          endMvts=X_train.shape[0]\n",
        "          losses=[]\n",
        "          train_no = []\n",
        "          k=0\n",
        "          for i in range(0,endMvts):\n",
        "          #for i in range(0,2):\n",
        "            model.zero_grad()\n",
        "            model.to(device)\n",
        "            mvts = trainData[i,:,:]\n",
        "            loss_total=doTrainingForThisMVTS(mvts,model,loss_function)\n",
        "            loss_total2=loss_total.detach().numpy()\n",
        "            #print(\"train mvts no:\",i,\"loss_total2\",loss_total2)\n",
        "            losses.append(loss_total2)\n",
        "            k=k+1\n",
        "            train_no.append(k)\n",
        "            totalLoss_epoch=totalLoss_epoch+loss_total\n",
        "\n",
        "            #print(\"------- >\")\n",
        "            #print(\"------- >\")\n",
        "            #loss.backward()\n",
        "\n",
        "            \n",
        "\n",
        "            loss_total.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "          #showCurve(test_no,losses,\"tarining\")   \n",
        "          #showBarChart(train_no,losses,\"tarining\",\"epoch train- \"+str(epoch),\"losses train- \"+str(epoch))\n",
        "          #print(\"\\n masterIteration: \",masterIteration, \"     epoch: \",epoch,\" tarining totalLoss_epoch:\",totalLoss_epoch)\n",
        "          print(\" tarining totalLoss_epoch:\",totalLoss_epoch)\n",
        "          totalLoss_epoch2.append(totalLoss_epoch.detach().numpy())        \n",
        "          #print(\"\\n test\")\n",
        "\n",
        "          with torch.no_grad():\n",
        "            losses_test=[]\n",
        "            test_no2=[]\n",
        "            loss_total_test=0\n",
        "            totalLoss_epoch_test=0\n",
        "            j=0\n",
        "            endMvts=X_test.shape[0]\n",
        "            for i in range(0,endMvts):\n",
        "            #for i in range(0,2):\n",
        "              #model.zero_grad()\n",
        "              #model.to(device)\n",
        "\n",
        "              mvts = trainData[i,:,:]\n",
        "              #print(\"mvts no:\",i ,\"input mvts.shape: \",mvts.shape)\n",
        "              #df = pd.DataFrame(mvts)\n",
        "              #print(df)\n",
        "              mvts = torch.from_numpy(mvts).float()\n",
        "              mvts_input = mvts[0:40,:]\n",
        "              df_mvts_input = pd.DataFrame(mvts_input)\n",
        "              #print(df_mvts_input)\n",
        "\n",
        "              #mvts = mvts.to(device)#print(mvts.is_cuda)\n",
        "              #mvts = mvts.view(mvts.size(0), -1)\n",
        "              #print(\"mvts no:\",i ,\"df_mvts_input.shape: \",df_mvts_input.shape)\n",
        "              #print(\"mvts no:\",i ,\"mvts_input.shape: \",mvts_input.shape)\n",
        "              Predicted_20_rows = model(mvts_input) #Predicted_20_rows -> [20, 33]\n",
        "              #print(\"test mvts no:\",i ,\"Predicted_20_rows.shape: \",Predicted_20_rows.shape)\n",
        "\n",
        "\n",
        "              target = mvts[40:60,:]\n",
        "              #df_target = pd.DataFrame(target)\n",
        "\n",
        "              #print(\"mvts no:\",i ,\"target.shape: \",target.shape)\n",
        "              target = torch.from_numpy(np.array(target)).float()\n",
        "              #print(\"mvts no:\",i ,\"target.shape: \",target.shape)\n",
        "              loss_totalX=0\n",
        "              for k in range(20):\n",
        "                tttt = loss_function(Predicted_20_rows[k], target[k]) #distance calculate\n",
        "                #print(\"row no:\",k ,\" loss: \",tttt)\n",
        "                loss_totalX=loss_totalX+tttt\n",
        "\n",
        "              loss_total2=loss_totalX.detach().numpy()\n",
        "              #print(\"test mvts no:\",i,\"loss_total\",loss_total2)\n",
        "              \n",
        "              #print(\"mvts test no:\",i,\"loss_total\",loss_total)\n",
        "              losses_test.append(loss_total2)\n",
        "              j=j+1\n",
        "              test_no2.append(j)\n",
        "              totalLoss_epoch_test=totalLoss_epoch_test+loss_total\n",
        "\n",
        "\n",
        "            #print(losses_test)\n",
        "            #showBarChart(test_no2,losses_test,\"testing\",\"epoch test- \"+str(epoch),\"losses test- \"+str(epoch))\n",
        "            #print(\"\\n masterIteration: \",masterIteration, \"     epoch: \",epoch,\" tarining totalLoss_epoch_test:\",totalLoss_epoch_test)\n",
        "            print(\"test totalLoss_epoch_test:\",totalLoss_epoch_test)\n",
        "            totalLoss_epoch_test2.append(totalLoss_epoch_test.detach().numpy())\n",
        "            epochs.append(epoch)\n",
        "            #print(\"totalLoss_epoch_test2:\",totalLoss_epoch_test2)\n",
        "            #print(\"totalLoss_epoch2:\",totalLoss_epoch2)\n",
        "            #print(\"epochs:\",epochs)\n",
        "          showBarChart(epochs,totalLoss_epoch2,\"tarining\",\"epoch train \",\"losses train\" )           \n",
        "          showBarChart(epochs,totalLoss_epoch_test2,\"testing\",\"epoch test\" ,\"losses test \")\n",
        "              #print(\"------- >\")\n",
        "              #print(\"------- >\")\n",
        "              #loss.backward()\n",
        "\n",
        "              #print(\"mvts no:\",i ,\" loss: \",loss.detach().numpy())\n",
        "\n",
        "              #loss_total.backward(retain_graph=True)\n",
        "              #optimizer.step()\n",
        "\n",
        "   \n",
        "               \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOUE-5Qkfhza"
      },
      "outputs": [],
      "source": [
        "#startCalulations()\n",
        "doLstmBasedCalculations()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MTGNN+model5",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}