{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1SGtSEIb8V7",
    "outputId": "c21b6d6e-1f09-4269-db46-3ba32ce9938e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import LSTM\n",
    "#from keras.layers import Dense\n",
    "#import scipy.stats as st\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import torch\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "B0Z7J3cGB0WS"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "pWhZKUObCf2x"
   },
   "outputs": [],
   "source": [
    "#layer.py\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numbers\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv,self).__init__()\n",
    "\n",
    "    def forward(self,x, A):\n",
    "        x = torch.einsum('ncwl,vw->ncvl',(x,A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class dy_nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dy_nconv,self).__init__()\n",
    "\n",
    "    def forward(self,x, A):\n",
    "        x = torch.einsum('ncvl,nvwl->ncwl',(x,A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class linear(nn.Module):\n",
    "    def __init__(self,c_in,c_out,bias=True):\n",
    "        super(linear,self).__init__()\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class prop(nn.Module):\n",
    "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
    "        super(prop, self).__init__()\n",
    "        self.nconv = nconv()\n",
    "        self.mlp = linear(c_in,c_out)\n",
    "        self.gdep = gdep\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self,x,adj):\n",
    "        adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
    "        d = adj.sum(1)\n",
    "        h = x\n",
    "        dv = d\n",
    "        a = adj / dv.view(-1, 1)\n",
    "        for i in range(self.gdep):\n",
    "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,a)\n",
    "        ho = self.mlp(h)\n",
    "        return ho\n",
    "\n",
    "\n",
    "class mixprop(nn.Module):\n",
    "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
    "        super(mixprop, self).__init__()\n",
    "        self.nconv = nconv()\n",
    "        self.mlp = linear((gdep+1)*c_in,c_out)\n",
    "        self.gdep = gdep\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def forward(self,x,adj):\n",
    "        adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
    "        d = adj.sum(1)\n",
    "        h = x\n",
    "        out = [h]\n",
    "        a = adj / d.view(-1, 1)\n",
    "        for i in range(self.gdep):\n",
    "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,a)\n",
    "            out.append(h)\n",
    "        ho = torch.cat(out,dim=1)\n",
    "        ho = self.mlp(ho)\n",
    "        return ho\n",
    "\n",
    "class dy_mixprop(nn.Module):\n",
    "    def __init__(self,c_in,c_out,gdep,dropout,alpha):\n",
    "        super(dy_mixprop, self).__init__()\n",
    "        self.nconv = dy_nconv()\n",
    "        self.mlp1 = linear((gdep+1)*c_in,c_out)\n",
    "        self.mlp2 = linear((gdep+1)*c_in,c_out)\n",
    "\n",
    "        self.gdep = gdep\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.lin1 = linear(c_in,c_in)\n",
    "        self.lin2 = linear(c_in,c_in)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        #adj = adj + torch.eye(adj.size(0)).to(x.device)\n",
    "        #d = adj.sum(1)\n",
    "        x1 = torch.tanh(self.lin1(x))\n",
    "        x2 = torch.tanh(self.lin2(x))\n",
    "        adj = self.nconv(x1.transpose(2,1),x2)\n",
    "        adj0 = torch.softmax(adj, dim=2)\n",
    "        adj1 = torch.softmax(adj.transpose(2,1), dim=2)\n",
    "\n",
    "        h = x\n",
    "        out = [h]\n",
    "        for i in range(self.gdep):\n",
    "            h = self.alpha*x + (1-self.alpha)*self.nconv(h,adj0)\n",
    "            out.append(h)\n",
    "        ho = torch.cat(out,dim=1)\n",
    "        ho1 = self.mlp1(ho)\n",
    "\n",
    "\n",
    "        h = x\n",
    "        out = [h]\n",
    "        for i in range(self.gdep):\n",
    "            h = self.alpha * x + (1 - self.alpha) * self.nconv(h, adj1)\n",
    "            out.append(h)\n",
    "        ho = torch.cat(out, dim=1)\n",
    "        ho2 = self.mlp2(ho)\n",
    "\n",
    "        return ho1+ho2\n",
    "\n",
    "\n",
    "\n",
    "class dilated_1D(nn.Module):\n",
    "    def __init__(self, cin, cout, dilation_factor=2):\n",
    "        super(dilated_1D, self).__init__()\n",
    "        self.tconv = nn.ModuleList()\n",
    "        self.kernel_set = [2,3,6,7]\n",
    "        self.tconv = nn.Conv2d(cin,cout,(1,7),dilation=(1,dilation_factor))\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = self.tconv(input)\n",
    "        return x\n",
    "\n",
    "class dilated_inception(nn.Module):\n",
    "    def __init__(self, cin, cout, dilation_factor=2):\n",
    "        super(dilated_inception, self).__init__()\n",
    "        self.tconv = nn.ModuleList()\n",
    "        self.kernel_set = [2,3,6,7]\n",
    "        cout = int(cout/len(self.kernel_set))\n",
    "        for kern in self.kernel_set:\n",
    "            self.tconv.append(nn.Conv2d(cin,cout,(1,kern),dilation=(1,dilation_factor)))\n",
    "\n",
    "    def forward(self,input):\n",
    "        x = []\n",
    "        for i in range(len(self.kernel_set)):\n",
    "            x.append(self.tconv[i](input))\n",
    "        for i in range(len(self.kernel_set)):\n",
    "            x[i] = x[i][...,-x[-1].size(3):]\n",
    "        x = torch.cat(x,dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class graph_constructor(nn.Module):\n",
    "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
    "        super(graph_constructor, self).__init__()\n",
    "        self.nnodes = nnodes\n",
    "        if static_feat is not None:\n",
    "            xd = static_feat.shape[1]\n",
    "            self.lin1 = nn.Linear(xd, dim)\n",
    "            self.lin2 = nn.Linear(xd, dim)\n",
    "        else:\n",
    "            self.emb1 = nn.Embedding(nnodes, dim)\n",
    "            self.emb2 = nn.Embedding(nnodes, dim)\n",
    "            self.lin1 = nn.Linear(dim,dim)\n",
    "            self.lin2 = nn.Linear(dim,dim)\n",
    "\n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.static_feat = static_feat\n",
    "\n",
    "    def forward(self, idx):\n",
    "        if self.static_feat is None:\n",
    "            nodevec1 = self.emb1(idx)\n",
    "            nodevec2 = self.emb2(idx)\n",
    "        else:\n",
    "            nodevec1 = self.static_feat[idx,:]\n",
    "            nodevec2 = nodevec1\n",
    "\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
    "\n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))-torch.mm(nodevec2, nodevec1.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
    "        mask.fill_(float('0'))\n",
    "        s1,t1 = (adj + torch.rand_like(adj)*0.01).topk(self.k,1)\n",
    "        mask.scatter_(1,t1,s1.fill_(1))\n",
    "        adj = adj*mask\n",
    "        return adj\n",
    "\n",
    "    def fullA(self, idx):\n",
    "        if self.static_feat is None:\n",
    "            nodevec1 = self.emb1(idx)\n",
    "            nodevec2 = self.emb2(idx)\n",
    "        else:\n",
    "            nodevec1 = self.static_feat[idx,:]\n",
    "            nodevec2 = nodevec1\n",
    "\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
    "\n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))-torch.mm(nodevec2, nodevec1.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        return adj\n",
    "\n",
    "class graph_global(nn.Module):\n",
    "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
    "        super(graph_global, self).__init__()\n",
    "        self.nnodes = nnodes\n",
    "        self.A = nn.Parameter(torch.randn(nnodes, nnodes).to(device), requires_grad=True).to(device)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return F.relu(self.A)\n",
    "\n",
    "\n",
    "class graph_undirected(nn.Module):\n",
    "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
    "        super(graph_undirected, self).__init__()\n",
    "        self.nnodes = nnodes\n",
    "        if static_feat is not None:\n",
    "            xd = static_feat.shape[1]\n",
    "            self.lin1 = nn.Linear(xd, dim)\n",
    "        else:\n",
    "            self.emb1 = nn.Embedding(nnodes, dim)\n",
    "            self.lin1 = nn.Linear(dim,dim)\n",
    "\n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.static_feat = static_feat\n",
    "\n",
    "    def forward(self, idx):\n",
    "        if self.static_feat is None:\n",
    "            nodevec1 = self.emb1(idx)\n",
    "            nodevec2 = self.emb1(idx)\n",
    "        else:\n",
    "            nodevec1 = self.static_feat[idx,:]\n",
    "            nodevec2 = nodevec1\n",
    "\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin1(nodevec2))\n",
    "\n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
    "        mask.fill_(float('0'))\n",
    "        s1,t1 = adj.topk(self.k,1)\n",
    "        mask.scatter_(1,t1,s1.fill_(1))\n",
    "        adj = adj*mask\n",
    "        return adj\n",
    "\n",
    "\n",
    "\n",
    "class graph_directed(nn.Module):\n",
    "    def __init__(self, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
    "        super(graph_directed, self).__init__()\n",
    "        self.nnodes = nnodes\n",
    "        if static_feat is not None:\n",
    "            xd = static_feat.shape[1]\n",
    "            self.lin1 = nn.Linear(xd, dim)\n",
    "            self.lin2 = nn.Linear(xd, dim)\n",
    "        else:\n",
    "            self.emb1 = nn.Embedding(nnodes, dim)\n",
    "            self.emb2 = nn.Embedding(nnodes, dim)\n",
    "            self.lin1 = nn.Linear(dim,dim)\n",
    "            self.lin2 = nn.Linear(dim,dim)\n",
    "\n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.static_feat = static_feat\n",
    "\n",
    "    def forward(self, idx):\n",
    "        if self.static_feat is None:\n",
    "            nodevec1 = self.emb1(idx)\n",
    "            nodevec2 = self.emb2(idx)\n",
    "        else:\n",
    "            nodevec1 = self.static_feat[idx,:]\n",
    "            nodevec2 = nodevec1\n",
    "\n",
    "        nodevec1 = torch.tanh(self.alpha*self.lin1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self.alpha*self.lin2(nodevec2))\n",
    "\n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1,0))\n",
    "        adj = F.relu(torch.tanh(self.alpha*a))\n",
    "        mask = torch.zeros(idx.size(0), idx.size(0)).to(self.device)\n",
    "        mask.fill_(float('0'))\n",
    "        s1,t1 = adj.topk(self.k,1)\n",
    "        mask.scatter_(1,t1,s1.fill_(1))\n",
    "        adj = adj*mask\n",
    "        return adj\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    __constants__ = ['normalized_shape', 'weight', 'bias', 'eps', 'elementwise_affine']\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.elementwise_affine:\n",
    "            init.ones_(self.weight)\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input, idx):\n",
    "        if self.elementwise_affine:\n",
    "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight[:,idx,:], self.bias[:,idx,:], self.eps)\n",
    "        else:\n",
    "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight, self.bias, self.eps)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{normalized_shape}, eps={eps}, ' \\\n",
    "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "PNK111lPCVsk"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "#from net import *\n",
    "#import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "i4zIp8tiDHJz"
   },
   "outputs": [],
   "source": [
    "#Trainer\n",
    "import torch.optim as optim\n",
    "import math\n",
    "#from net import *\n",
    "#import util\n",
    "class Trainer():\n",
    "    def __init__(self, model, lrate, wdecay, clip, step_size, seq_out_len, scaler, device, cl=True):\n",
    "        self.scaler = scaler\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.loss = util.masked_mae\n",
    "        self.clip = clip\n",
    "        self.step = step_size\n",
    "        self.iter = 1\n",
    "        self.task_level = 1\n",
    "        self.seq_out_len = seq_out_len\n",
    "        self.cl = cl\n",
    "\n",
    "    def train(self, input, real_val, idx=None):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(input, idx=idx)\n",
    "        output = output.transpose(1,3)\n",
    "        real = torch.unsqueeze(real_val,dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "        if self.iter%self.step==0 and self.task_level<=self.seq_out_len:\n",
    "            self.task_level +=1\n",
    "        if self.cl:\n",
    "            loss = self.loss(predict[:, :, :, :self.task_level], real[:, :, :, :self.task_level], 0.0)\n",
    "        else:\n",
    "            loss = self.loss(predict, real, 0.0)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        # mae = util.masked_mae(predict,real,0.0).item()\n",
    "        mape = util.masked_mape(predict,real,0.0).item()\n",
    "        rmse = util.masked_rmse(predict,real,0.0).item()\n",
    "        self.iter += 1\n",
    "        return loss.item(),mape,rmse\n",
    "\n",
    "    def eval(self, input, real_val):\n",
    "        self.model.eval()\n",
    "        output = self.model(input)\n",
    "        output = output.transpose(1,3)\n",
    "        real = torch.unsqueeze(real_val,dim=1)\n",
    "        predict = self.scaler.inverse_transform(output)\n",
    "        loss = self.loss(predict, real, 0.0)\n",
    "        mape = util.masked_mape(predict,real,0.0).item()\n",
    "        rmse = util.masked_rmse(predict,real,0.0).item()\n",
    "        return loss.item(),mape,rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "Y_-Pwc7HDS0c"
   },
   "outputs": [],
   "source": [
    "#train_single_step.py\n",
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_l1 = 0\n",
    "    n_samples = 0\n",
    "    predict = None\n",
    "    test = None\n",
    "\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, False):\n",
    "        X = torch.unsqueeze(X,dim=1)\n",
    "        X = X.transpose(2,3)\n",
    "        with torch.no_grad():\n",
    "            output = model(X)\n",
    "        output = torch.squeeze(output)\n",
    "        if len(output.shape)==1:\n",
    "            output = output.unsqueeze(dim=0)\n",
    "        if predict is None:\n",
    "            predict = output\n",
    "            test = Y\n",
    "        else:\n",
    "            predict = torch.cat((predict, output))\n",
    "            test = torch.cat((test, Y))\n",
    "\n",
    "        scale = data.scale.expand(output.size(0), data.m)\n",
    "        total_loss += evaluateL2(output * scale, Y * scale).item()\n",
    "        total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
    "        n_samples += (output.size(0) * data.m)\n",
    "\n",
    "    rse = math.sqrt(total_loss / n_samples) / data.rse\n",
    "    rae = (total_loss_l1 / n_samples) / data.rae\n",
    "\n",
    "    predict = predict.data.cpu().numpy()\n",
    "    Ytest = test.data.cpu().numpy()\n",
    "    sigma_p = (predict).std(axis=0)\n",
    "    sigma_g = (Ytest).std(axis=0)\n",
    "    mean_p = predict.mean(axis=0)\n",
    "    mean_g = Ytest.mean(axis=0)\n",
    "    index = (sigma_g != 0)\n",
    "    aaa=((predict - mean_p) * (Ytest - mean_g)).mean(axis=0)\n",
    "    bbb=(sigma_p * sigma_g)\n",
    "    \n",
    "    print(\"sigma_p: \",sigma_p)\n",
    "    print(\"sigma_g: \",sigma_p)\n",
    "    \n",
    "    correlation =  aaa/ bbb\n",
    "    print(\"correlation: \",correlation)\n",
    "    correlation = (correlation[index]).mean()\n",
    "    return rse, rae, correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "IEZRD125DU2C"
   },
   "outputs": [],
   "source": [
    "#train_single_step.py\n",
    "def train(data, X, Y, model, criterion, optim, batch_size):\n",
    "    print(\"train()\")\n",
    "    numcycle=2\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_samples = 0\n",
    "    iter = 0\n",
    "    cycle=0\n",
    "    for X, Y in data.get_batches(X, Y, batch_size, True):\n",
    "        \n",
    "        #if  cycle> numcycle:\n",
    "            #print(\"break:--->\",cycle)\n",
    "            #break\n",
    "        \n",
    "        model.zero_grad()\n",
    "        X = torch.unsqueeze(X,dim=1)\n",
    "        X = X.transpose(2,3)\n",
    "        fff=iter % step_size\n",
    "        #print(\"iter:\",iter)\n",
    "        #print(\"fff:\",fff)\n",
    "        if  fff== 0:\n",
    "            perm = np.random.permutation(range(num_nodes))\n",
    "        num_sub = int(num_nodes / num_split)\n",
    "\n",
    "        for j in range(num_split):\n",
    "            if j != num_split - 1:\n",
    "                id = perm[j * num_sub:(j + 1) * num_sub]\n",
    "            else:\n",
    "                id = perm[j * num_sub:]\n",
    "                \n",
    "                \n",
    "            # id = torch.tensor(id)\n",
    "            id = torch.LongTensor(id).to(device)\n",
    "            # print(\"id:\",id)\n",
    "            #id = torch.tensor(id).type(torch(bool))\n",
    "            #print(\"id:\",id)\n",
    "            #id = torch.tensor(id).type(torch(bool)).to(device)\n",
    "            #print(\"id:\",id)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            tx = X[:, :, id, :]\n",
    "            ty = Y[:, id]\n",
    "            output = model(tx,id)\n",
    "            output = torch.squeeze(output)\n",
    "            scale = data.scale.expand(output.size(0), data.m)\n",
    "            scale = scale[:,id]\n",
    "            loss = criterion(output * scale, ty * scale)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            n_samples += (output.size(0) * data.m)\n",
    "            grad_norm = optim.step()\n",
    "\n",
    "        fssfsf=iter%100\n",
    "        #print(\"iter%100:\",fssfsf)\n",
    "        if fssfsf ==0:\n",
    "            print('iter:{:3d} | loss: {:.3f}'.format(iter,loss.item()/(output.size(0) * data.m)))\n",
    "        iter += 1\n",
    "        cycle=cycle+1\n",
    "        #print(\"cycle:--->\",cycle)\n",
    "    return total_loss / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "x16a5qW2CaJo"
   },
   "outputs": [],
   "source": [
    "#from layer import *\n",
    "#net.py\n",
    "\n",
    "class gtnet(nn.Module):\n",
    "    def __init__(self, gcn_true, buildA_true, gcn_depth, num_nodes, device, predefined_A=None, static_feat=None, dropout=0.3, subgraph_size=20, node_dim=40, dilation_exponential=1, conv_channels=32, residual_channels=32, skip_channels=64, end_channels=128, seq_length=12, in_dim=2, out_dim=12, layers=3, propalpha=0.05, tanhalpha=3, layer_norm_affline=True):\n",
    "        super(gtnet, self).__init__()\n",
    "        self.gcn_true = gcn_true\n",
    "        self.buildA_true = buildA_true\n",
    "        self.num_nodes = num_nodes\n",
    "        self.dropout = dropout\n",
    "        self.predefined_A = predefined_A\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.gconv1 = nn.ModuleList()\n",
    "        self.gconv2 = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
    "                                    out_channels=residual_channels,\n",
    "                                    kernel_size=(1, 1))\n",
    "        self.gc = graph_constructor(num_nodes, subgraph_size, node_dim, device, alpha=tanhalpha, static_feat=static_feat)\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        kernel_size = 7\n",
    "        if dilation_exponential>1:\n",
    "            self.receptive_field = int(1+(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
    "        else:\n",
    "            self.receptive_field = layers*(kernel_size-1) + 1\n",
    "\n",
    "        for i in range(1):\n",
    "            if dilation_exponential>1:\n",
    "                rf_size_i = int(1 + i*(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
    "            else:\n",
    "                rf_size_i = i*layers*(kernel_size-1)+1\n",
    "            new_dilation = 1\n",
    "            for j in range(1,layers+1):\n",
    "                if dilation_exponential > 1:\n",
    "                    rf_size_j = int(rf_size_i + (kernel_size-1)*(dilation_exponential**j-1)/(dilation_exponential-1))\n",
    "                else:\n",
    "                    rf_size_j = rf_size_i+j*(kernel_size-1)\n",
    "\n",
    "                self.filter_convs.append(dilated_inception(residual_channels, conv_channels, dilation_factor=new_dilation))\n",
    "                self.gate_convs.append(dilated_inception(residual_channels, conv_channels, dilation_factor=new_dilation))\n",
    "                self.residual_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
    "                                                    out_channels=residual_channels,\n",
    "                                                 kernel_size=(1, 1)))\n",
    "                if self.seq_length>self.receptive_field:\n",
    "                    self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
    "                                                    out_channels=skip_channels,\n",
    "                                                    kernel_size=(1, self.seq_length-rf_size_j+1)))\n",
    "                else:\n",
    "                    self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
    "                                                    out_channels=skip_channels,\n",
    "                                                    kernel_size=(1, self.receptive_field-rf_size_j+1)))\n",
    "\n",
    "                if self.gcn_true:\n",
    "                    self.gconv1.append(mixprop(conv_channels, residual_channels, gcn_depth, dropout, propalpha))\n",
    "                    self.gconv2.append(mixprop(conv_channels, residual_channels, gcn_depth, dropout, propalpha))\n",
    "\n",
    "                if self.seq_length>self.receptive_field:\n",
    "                    self.norm.append(LayerNorm((residual_channels, num_nodes, self.seq_length - rf_size_j + 1),elementwise_affine=layer_norm_affline))\n",
    "                else:\n",
    "                    self.norm.append(LayerNorm((residual_channels, num_nodes, self.receptive_field - rf_size_j + 1),elementwise_affine=layer_norm_affline))\n",
    "\n",
    "                new_dilation *= dilation_exponential\n",
    "\n",
    "        self.layers = layers\n",
    "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
    "                                             out_channels=end_channels,\n",
    "                                             kernel_size=(1,1),\n",
    "                                             bias=True)\n",
    "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
    "                                             out_channels=out_dim,\n",
    "                                             kernel_size=(1,1),\n",
    "                                             bias=True)\n",
    "        if self.seq_length > self.receptive_field:\n",
    "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.seq_length), bias=True)\n",
    "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, self.seq_length-self.receptive_field+1), bias=True)\n",
    "\n",
    "        else:\n",
    "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.receptive_field), bias=True)\n",
    "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, 1), bias=True)\n",
    "\n",
    "\n",
    "        self.idx = torch.arange(self.num_nodes).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input, idx=None):\n",
    "        seq_len = input.size(3)\n",
    "        assert seq_len==self.seq_length, 'input sequence length not equal to preset sequence length'\n",
    "\n",
    "        if self.seq_length<self.receptive_field:\n",
    "            input = nn.functional.pad(input,(self.receptive_field-self.seq_length,0,0,0))\n",
    "\n",
    "\n",
    "\n",
    "        if self.gcn_true:\n",
    "            if self.buildA_true:\n",
    "                if idx is None:\n",
    "                    adp = self.gc(self.idx)\n",
    "                else:\n",
    "                    adp = self.gc(idx)\n",
    "            else:\n",
    "                adp = self.predefined_A\n",
    "\n",
    "        x = self.start_conv(input)\n",
    "        skip = self.skip0(F.dropout(input, self.dropout, training=self.training))\n",
    "        for i in range(self.layers):\n",
    "            residual = x\n",
    "            filter = self.filter_convs[i](x)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](x)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            s = x\n",
    "            s = self.skip_convs[i](s)\n",
    "            skip = s + skip\n",
    "            if self.gcn_true:\n",
    "                x = self.gconv1[i](x, adp)+self.gconv2[i](x, adp.transpose(1,0))\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "\n",
    "            x = x + residual[:, :, :, -x.size(3):]\n",
    "            if idx is None:\n",
    "                x = self.norm[i](x,self.idx)\n",
    "            else:\n",
    "                x = self.norm[i](x,idx)\n",
    "\n",
    "        skip = self.skipE(x) + skip\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xFATKLd2B9UW"
   },
   "outputs": [],
   "source": [
    "#from util import *\n",
    "#from trainer import Optim\n",
    "#from net import gtnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Optim(object):\n",
    "\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
    "        elif self.method == 'adam':\n",
    "            \n",
    "            #print(\"type(self.params): \",type(self.params))\n",
    "            #print(\"self.lr: \",self.lr)\n",
    "            #print(\"self.params: \",self.params)\n",
    "            #print(\"self.lr: \",self.lr)\n",
    "            #print(\"self.lr_decay: \",self.lr_decay)\n",
    "            \n",
    "            self.lr= 0.0001\n",
    "            self.lr_decay=100000\n",
    "            #self.params=\n",
    "            \n",
    "           #print(\"self.params: \",self.params)\n",
    "            #print(\"self.lr: \",self.lr)\n",
    "            #print(\"self.lr_decay: \",self.lr_decay)\n",
    "            \n",
    "            self.optimizer =optim.Adam(self.params, lr=self.lr, weight_decay=self.lr_decay)\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "            \n",
    "        print(\"done\")    \n",
    "            \n",
    "\n",
    "    def __init__(self, params, method, lr, clip, lr_decay=1, start_decay_at=None):\n",
    "        self.params = params  # careful: params may be a generator\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step(self):\n",
    "        # Compute gradients norm.\n",
    "        grad_norm = 0\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.params, self.clip)\n",
    "\n",
    "        # for param in self.params:\n",
    "        #     grad_norm += math.pow(param.grad.data.norm(), 2)\n",
    "        #\n",
    "        # grad_norm = math.sqrt(grad_norm)\n",
    "        # if grad_norm > 0:\n",
    "        #     shrinkage = self.max_grad_norm / grad_norm\n",
    "        # else:\n",
    "        #     shrinkage = 1.\n",
    "        #\n",
    "        # for param in self.params:\n",
    "        #     if shrinkage < 1:\n",
    "        #         param.grad.data.mul_(shrinkage)\n",
    "        self.optimizer.step()\n",
    "        return  grad_norm\n",
    "\n",
    "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
    "    def updateLearningRate(self, ppl, epoch):\n",
    "        if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "            self.start_decay = True\n",
    "        if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "            self.start_decay = True\n",
    "\n",
    "        if self.start_decay:\n",
    "            self.lr = self.lr * self.lr_decay\n",
    "            print(\"Decaying learning rate to %g\" % self.lr)\n",
    "        #only decay for one epoch\n",
    "        self.start_decay = False\n",
    "\n",
    "        self.last_ppl = ppl\n",
    "\n",
    "        self._makeOptimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "viUWZDKQHpst"
   },
   "outputs": [],
   "source": [
    "#train_single_step.py\n",
    "#train_single_step.py\n",
    "def main():\n",
    "\n",
    "    Data = DataLoaderS(data, 0.6, 0.2, device, horizon, seq_in_len, normalize)\n",
    "\n",
    "    model = gtnet(gcn_true, buildA_true, gcn_depth, num_nodes,\n",
    "                  device, dropout=dropout, subgraph_size=subgraph_size,\n",
    "                  node_dim=node_dim, dilation_exponential=dilation_exponential,\n",
    "                  conv_channels=conv_channels, residual_channels=residual_channels,\n",
    "                  skip_channels=skip_channels, end_channels= end_channels,\n",
    "                  seq_length=seq_in_len, in_dim=in_dim, out_dim=seq_out_len,\n",
    "                  layers=layers, propalpha=propalpha, tanhalpha=tanhalpha, layer_norm_affline=False)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #print(args)\n",
    "    print('The recpetive field size is', model.receptive_field)\n",
    "    nParams = sum([p.nelement() for p in model.parameters()])\n",
    "    print('Number of model parameters is', nParams, flush=True)\n",
    "\n",
    "    if L1Loss:\n",
    "        criterion = nn.L1Loss(size_average=False).to(device)\n",
    "    else:\n",
    "        criterion = nn.MSELoss(size_average=False).to(device)\n",
    "    evaluateL2 = nn.MSELoss(size_average=False).to(device)\n",
    "    evaluateL1 = nn.L1Loss(size_average=False).to(device)\n",
    "\n",
    "\n",
    "    best_val = 10000000\n",
    "    print('optim = Optim()')\n",
    "    optim = Optim(\n",
    "        model.parameters(), optim2, lr, clip, lr_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        print('begin training')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train_loss = train(Data, Data.train[0], Data.train[1], model, criterion, optim, batch_size)\n",
    "            val_loss, val_rae, val_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
    "                                               batch_size)\n",
    "            print(\n",
    "                '| end of epoch {:3d} | time: {:5.2f}s | train_loss {:5.4f} | valid rse {:5.4f} | valid rae {:5.4f} | valid corr  {:5.4f}'.format(\n",
    "                    epoch, (time.time() - epoch_start_time), train_loss, val_loss, val_rae, val_corr), flush=True)\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "\n",
    "            if val_loss < best_val:\n",
    "                with open(save, 'wb') as f:\n",
    "                    #torch.save(model, f)\n",
    "                    print('Skipping saving')\n",
    "                best_val = val_loss\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
    "                                                     batch_size)\n",
    "                print(\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr), flush=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "\n",
    "    # Load the best saved model.\n",
    "    #with open(save, 'rb') as f:\n",
    "        #model = torch.load(f)\n",
    "\n",
    "    vtest_acc, vtest_rae, vtest_corr = evaluate(Data, Data.valid[0], Data.valid[1], model, evaluateL2, evaluateL1,\n",
    "                                         batch_size)\n",
    "    test_acc, test_rae, test_corr = evaluate(Data, Data.test[0], Data.test[1], model, evaluateL2, evaluateL1,\n",
    "                                         batch_size)\n",
    "    print(\"final test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "    \n",
    "    return vtest_acc, vtest_rae, vtest_corr, test_acc, test_rae, test_corr\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "AqDSTmnqDlOi"
   },
   "outputs": [],
   "source": [
    "save= \"/model-solar-3.pt\"\n",
    "\n",
    "batch_size=1 \n",
    "epochs=1 \n",
    "horizon=3 \n",
    "step_size=1 \n",
    "device=\"cuda:0\"\n",
    "device=\"cpu\"\n",
    "\n",
    "#train_single_step.py\n",
    "vacc = []\n",
    "vrae = []\n",
    "vcorr = []\n",
    "acc = []\n",
    "rae = []\n",
    "corr = []\n",
    "#for i in range(2):\n",
    "#    main()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi4FewZlHXmD"
   },
   "source": [
    " vacc.append(val_acc)\n",
    "  vrae.append(val_rae)\n",
    "  vcorr.append(val_corr)\n",
    "  acc.append(test_acc)\n",
    "  rae.append(test_rae)\n",
    "  corr.append(test_corr)\n",
    "print('\\n\\n')\n",
    "print('10 runs average')\n",
    "print('\\n\\n')\n",
    "print(\"valid\\trse\\trae\\tcorr\")\n",
    "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(vacc), np.mean(vrae), np.mean(vcorr)))\n",
    "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(vacc), np.std(vrae), np.std(vcorr)))\n",
    "print('\\n\\n')\n",
    "print(\"test\\trse\\trae\\tcorr\")\n",
    "print(\"mean\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.mean(acc), np.mean(rae), np.mean(corr)))\n",
    "print(\"std\\t{:5.4f}\\t{:5.4f}\\t{:5.4f}\".format(np.std(acc), np.std(rae), np.std(corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "lfyXnYPwfhzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "COHPvU4JF2RQ"
   },
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='PyTorch Time series forecasting'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data='solar_AL.txt'\n",
    "data='C:\\0.ml\\MTGNN\\data\\solar_AL.txt'\n",
    "log_interval=2000\n",
    "metavar='N'#,help='report interval'\n",
    "save='model/model.pt'#,help='path to save the final model'\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "optim2='adam'\n",
    "\n",
    "L1Loss=True\n",
    "normalize=2\n",
    "#device='cuda:1'\n",
    "device='cuda:0'\n",
    "gcn_true=True\n",
    "buildA_true=True\n",
    "gcn_depth=2\n",
    "num_nodes=137\n",
    "dropout=0.3#,help='dropout rate'\n",
    "subgraph_size=20#,help='k'\n",
    "node_dim=40#,help='dim of nodes'\n",
    "dilation_exponential=2#,help='dilation exponential'\n",
    "conv_channels=16#,help='convolution channels'\n",
    "residual_channels=16#,help='residual channels'\n",
    "skip_channels=32#,help='skip channels'\n",
    "end_channels=64#,help='end channels'\n",
    "in_dim=1#,help='inputs dimension'\n",
    "seq_in_len=24*7#,help='input sequence length'\n",
    "seq_out_len=1#,help='output sequence length'\n",
    "horizon=3\n",
    "layers=5#,help='number of layers'\n",
    "\n",
    "batch_size=32#,help='batch size'\n",
    "lr=0.0001#,help='learning rate'\n",
    "weight_decay=0.00001#,help='weight decay rate'\n",
    "\n",
    "clip=5#,help='clip'\n",
    "\n",
    "propalpha=0.05#,help='prop alpha'\n",
    "tanhalpha=3#,help='tanh alpha'\n",
    "\n",
    "epochs=1\n",
    "num_split=1#,help='number of splits for graphs'\n",
    "step_size=100#,help='step_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter\n",
      "(52560, 137)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fin = open('C:\\\\0.ml\\\\MTGNN\\\\data\\\\solar_AL.txt')\n",
    "rawdat = np.loadtxt(fin, delimiter=',')\n",
    "#self.rawdat=load(file_name)\n",
    "   \n",
    "dat = np.zeros(rawdat.shape)\n",
    "n, m = dat.shape\n",
    "print('iter')   \n",
    "print(rawdat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt=Sampled_inputs[0]\n",
    "for i in range(1,Sampled_inputs.shape[0]):\n",
    "            ttt=np.concatenate( (ttt,Sampled_inputs[1]  ), axis=0)\n",
    "            #print(ttt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rawdat))\n",
    "print(type(dat))\n",
    "\n",
    "print(type(Sampled_inputs))\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdat2=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawdat2.shape)\n",
    "temp=rawdat2[0]\n",
    "print(temp.shape)\n",
    "df = pd.DataFrame(temp)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "YqojS9jkCEvp"
   },
   "outputs": [],
   "source": [
    "#util.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import linalg\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def load(file_name):\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        obj = pickle.load(fp)\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.)/(len(x)))\n",
    "\n",
    "class DataLoaderM(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
    "        \"\"\"\n",
    "        :param xs:\n",
    "        :param ys:\n",
    "        :param batch_size:\n",
    "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        print( 'DataLoaderM __init__')\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def shuffle(self):\n",
    "        print('shuffle')\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def get_iterator(self):\n",
    "        print('get_iterator')\n",
    "        self.current_ind = 0\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = self.xs[start_ind: end_ind, ...]\n",
    "                y_i = self.ys[start_ind: end_ind, ...]\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "class StandardScaler():\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "def sym_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32).todense()\n",
    "\n",
    "def asym_adj(adj):\n",
    "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)).flatten()\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat= sp.diags(d_inv)\n",
    "    return d_mat.dot(adj).astype(np.float32).todense()\n",
    "\n",
    "def calculate_normalized_laplacian(adj):\n",
    "    \"\"\"\n",
    "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
    "    # D = diag(A 1)\n",
    "    :param adj:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    d = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    return normalized_laplacian\n",
    "\n",
    "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
    "    if undirected:\n",
    "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
    "    L = calculate_normalized_laplacian(adj_mx)\n",
    "    if lambda_max is None:\n",
    "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
    "        lambda_max = lambda_max[0]\n",
    "    L = sp.csr_matrix(L)\n",
    "    M, _ = L.shape\n",
    "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
    "    L = (2 / lambda_max * L) - I\n",
    "    return L.astype(np.float32).todense()\n",
    "\n",
    "\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data\n",
    "\n",
    "def load_adj(pkl_filename):\n",
    "    sensor_ids, sensor_id_to_ind, adj = load_pickle(pkl_filename)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n",
    "    data = {}\n",
    "    for category in ['train', 'val', 'test']:\n",
    "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
    "        data['x_' + category] = cat_data['x']\n",
    "        data['y_' + category] = cat_data['y']\n",
    "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
    "    # Data format\n",
    "    for category in ['train', 'val', 'test']:\n",
    "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
    "\n",
    "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
    "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)\n",
    "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)\n",
    "    data['scaler'] = scaler\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def masked_mse(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = (preds-labels)**2\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def masked_rmse(preds, labels, null_val=np.nan):\n",
    "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
    "\n",
    "\n",
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def masked_mape(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels!=null_val)\n",
    "    mask = mask.float()\n",
    "    mask /=  torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds-labels)/labels\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "def metric(pred, real):\n",
    "    mae = masked_mae(pred,real,0.0).item()\n",
    "    mape = masked_mape(pred,real,0.0).item()\n",
    "    rmse = masked_rmse(pred,real,0.0).item()\n",
    "    return mae,mape,rmse\n",
    "\n",
    "\n",
    "def load_node_feature(path):\n",
    "    fi = open(path)\n",
    "    x = []\n",
    "    for li in fi:\n",
    "        li = li.strip()\n",
    "        li = li.split(\",\")\n",
    "        e = [float(t) for t in li[1:]]\n",
    "        x.append(e)\n",
    "    x = np.array(x)\n",
    "    mean = np.mean(x,axis=0)\n",
    "    std = np.std(x,axis=0)\n",
    "    z = torch.tensor((x-mean)/std,dtype=torch.float)\n",
    "    return z\n",
    "\n",
    "\n",
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.) / (len(x)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderS(object):\n",
    "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "    def __init__(self, file_name, train, valid, device, horizon, window, normalize=2):\n",
    "        print('__init__111')\n",
    "        self.P = window\n",
    "        self.h = horizon\n",
    "\n",
    "        self.rawdat=rawdat2\n",
    "        print('self.rawdat: ',self.rawdat.shape)\n",
    "\n",
    "        \n",
    "        #fin = open(file_name)\n",
    "        #self.rawdat = np.loadtxt(fin, delimiter=',')\n",
    "\n",
    "        #self.rawdat=load(file_name)\n",
    "        \n",
    "        self.dat = np.zeros(self.rawdat.shape)\n",
    "        print('self.dat.shape: ',self.dat.shape)\n",
    "\n",
    "\n",
    "        self.n, self.m = self.dat.shape\n",
    "\n",
    "\n",
    "        self.normalize = 2\n",
    "        self.scale = np.ones(self.m)\n",
    "        self._normalized(normalize)\n",
    "        self._split(int(train * self.n), int((train + valid) * self.n), self.n)\n",
    "\n",
    "        self.scale = torch.from_numpy(self.scale).float()\n",
    "        tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m)\n",
    "\n",
    "        self.scale = self.scale.to(device)\n",
    "        self.scale = Variable(self.scale)\n",
    "\n",
    "        self.rse = normal_std(tmp)\n",
    "        self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)))\n",
    "\n",
    "        self.device = device         \n",
    "\n",
    "    def _split(self, train, valid, test):\n",
    "        print('_split')\n",
    "        \n",
    "        #print(\"type(train): \",type(train))\n",
    "        \n",
    "        tarin_start=self.P + self.h - 1\n",
    "        valid_stop=self.n\n",
    "        \n",
    "        print(\"tarin_start: \",tarin_start)\n",
    "        print(\"train: \",train)\n",
    "        print(\"valid: \",valid)\n",
    "        print(\"valid_stop: \",valid_stop)\n",
    "        \n",
    "        train_set = range(tarin_start, train)\n",
    "        valid_set = range(train, valid)\n",
    "        test_set = range(valid, valid_stop)\n",
    "        \n",
    "        print(\"train_set: \",train_set)\n",
    "        print(\"valid_set: \",valid_set)\n",
    "        print(\"test_set: \",test_set)\n",
    "        \n",
    "        \n",
    "        self.train = self._batchify(train_set, self.h)\n",
    "        self.valid = self._batchify(valid_set, self.h)\n",
    "        self.test = self._batchify(test_set, self.h)\n",
    "\n",
    "    def _batchify(self, idx_set, horizon):\n",
    "        print('_batchify')\n",
    "        \n",
    "        print(\"idx_set: \", idx_set)\n",
    "        #print(\"type(idx_set): \", type(idx_set))\n",
    "   \n",
    "        n = len(idx_set)\n",
    "        X = torch.zeros((n, self.P, self.m))\n",
    "        Y = torch.zeros((n, self.m))\n",
    "        print(\"len(idx_set):\",len(idx_set))\n",
    "        print(\"X.shape:\",X.shape)\n",
    "        print(\"Y.shape:\",Y.shape)\n",
    "       \n",
    "        for i in range(n):\n",
    "            end = idx_set[i] - self.h + 1\n",
    "            start = end - self.P\n",
    "            #print(\"***********\")\n",
    "            X[i, :, :] = torch.from_numpy(self.dat[start:end, :])\n",
    "            Y[i, :] = torch.from_numpy(self.dat[idx_set[i], :])\n",
    "        return [X, Y]\n",
    "    \n",
    "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "        print('get_batches')\n",
    "        length = len(inputs)\n",
    "        if shuffle:\n",
    "            index = torch.randperm(length)\n",
    "        else:\n",
    "            index = torch.LongTensor(range(length))\n",
    "        start_idx = 0\n",
    "        while (start_idx < length):\n",
    "            end_idx = min(length, start_idx + batch_size)\n",
    "            excerpt = index[start_idx:end_idx]\n",
    "            X = inputs[excerpt]\n",
    "            Y = targets[excerpt]\n",
    "            X = X.to(self.device)\n",
    "            Y = Y.to(self.device)\n",
    "            yield Variable(X), Variable(Y)\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def _normalized(self, normalize):\n",
    "        print('_normalized')\n",
    "        # normalized by the maximum value of entire matrix.\n",
    "\n",
    "        if (normalize == 0):\n",
    "            self.dat = self.rawdat\n",
    "\n",
    "        if (normalize == 1):\n",
    "            self.dat = self.rawdat / np.max(self.rawdat)\n",
    "                \n",
    "        # normlized by the maximum value of each row(sensor).\n",
    "        #if (normalize == 2):\n",
    "            #for i in range(self.m):\n",
    "                #self.scale[i] = np.max(np.abs(self.rawdat[:, i]))\n",
    "                #self.dat[:, i] = self.rawdat[:, i] / np.max(np.abs(self.rawdat[:, i]))\n",
    "\n",
    "        # normlized by the maximum value of each row(sensor).\n",
    "        if (normalize == 2):\n",
    "            #print(\"self.m: \",self.m)\n",
    "            for i in range(self.m):\n",
    "                aaa=np.abs(self.rawdat[:, i])\n",
    "                #print(\"aaa.shape: \",aaa.shape)\n",
    "                #print(\"np.max(aaa): \",np.max(aaa))           \n",
    "                self.scale[i] = np.max(aaa)\n",
    "                \n",
    "                cccc= self.rawdat[:, i] \n",
    "                #print(\"cccc.shape: \",cccc.shape)\n",
    "                \n",
    "                bbbb=np.abs(self.rawdat[:, i])\n",
    "                           \n",
    "                dddd=np.max(bbbb)\n",
    "                #print(\"dddd: \",dddd) \n",
    "                #if dddd<= 0.0:  \n",
    "                    #dddd=1\n",
    "                    #print(\"Found zero  \" )\n",
    "                    #print(\"bbbb.shape: \",bbbb.shape)\n",
    "                    #print(\"bbbb: \",bbbb) \n",
    "                    \n",
    "                self.dat[:, i] = (cccc /dddd )\n",
    "                #print(\"cccc /dddd : \",cccc /dddd )     \n",
    "                   \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6748.177901</td>\n",
       "      <td>8.294421e+10</td>\n",
       "      <td>1.591225e+24</td>\n",
       "      <td>1.095967e+14</td>\n",
       "      <td>1510.190735</td>\n",
       "      <td>5.537032e+13</td>\n",
       "      <td>6.524454e+22</td>\n",
       "      <td>-1.945966e+25</td>\n",
       "      <td>13461.392092</td>\n",
       "      <td>-0.176659</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414326</td>\n",
       "      <td>5.392323</td>\n",
       "      <td>5.336158</td>\n",
       "      <td>5.205328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6713.245495</td>\n",
       "      <td>8.301113e+10</td>\n",
       "      <td>1.591180e+24</td>\n",
       "      <td>1.082126e+14</td>\n",
       "      <td>1552.899870</td>\n",
       "      <td>5.163912e+13</td>\n",
       "      <td>6.552311e+22</td>\n",
       "      <td>-1.967367e+25</td>\n",
       "      <td>13459.043822</td>\n",
       "      <td>-0.178458</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405846</td>\n",
       "      <td>5.403088</td>\n",
       "      <td>5.335880</td>\n",
       "      <td>5.211822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6711.746592</td>\n",
       "      <td>8.324237e+10</td>\n",
       "      <td>1.585508e+24</td>\n",
       "      <td>1.074454e+14</td>\n",
       "      <td>1569.593570</td>\n",
       "      <td>5.208053e+13</td>\n",
       "      <td>6.582194e+22</td>\n",
       "      <td>-2.022283e+25</td>\n",
       "      <td>13391.359484</td>\n",
       "      <td>-0.182930</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405772</td>\n",
       "      <td>5.398195</td>\n",
       "      <td>5.345981</td>\n",
       "      <td>5.216945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6903.974288</td>\n",
       "      <td>8.343257e+10</td>\n",
       "      <td>1.576556e+24</td>\n",
       "      <td>1.084795e+14</td>\n",
       "      <td>1608.218901</td>\n",
       "      <td>5.166028e+13</td>\n",
       "      <td>6.609184e+22</td>\n",
       "      <td>-2.072753e+25</td>\n",
       "      <td>13248.436229</td>\n",
       "      <td>-0.187068</td>\n",
       "      <td>...</td>\n",
       "      <td>5.396873</td>\n",
       "      <td>5.398732</td>\n",
       "      <td>5.327560</td>\n",
       "      <td>5.203755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6691.365852</td>\n",
       "      <td>8.370511e+10</td>\n",
       "      <td>1.600757e+24</td>\n",
       "      <td>1.086643e+14</td>\n",
       "      <td>1479.272564</td>\n",
       "      <td>5.030951e+13</td>\n",
       "      <td>6.614713e+22</td>\n",
       "      <td>-1.999304e+25</td>\n",
       "      <td>13367.513454</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>...</td>\n",
       "      <td>5.426061</td>\n",
       "      <td>5.389834</td>\n",
       "      <td>5.351214</td>\n",
       "      <td>5.217820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6834.958217</td>\n",
       "      <td>8.406158e+10</td>\n",
       "      <td>1.591036e+24</td>\n",
       "      <td>1.096684e+14</td>\n",
       "      <td>1644.488148</td>\n",
       "      <td>5.663543e+13</td>\n",
       "      <td>6.630166e+22</td>\n",
       "      <td>-2.075205e+25</td>\n",
       "      <td>13298.131406</td>\n",
       "      <td>-0.185888</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421926</td>\n",
       "      <td>5.407266</td>\n",
       "      <td>5.350388</td>\n",
       "      <td>5.219855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6830.348029</td>\n",
       "      <td>8.431275e+10</td>\n",
       "      <td>1.589851e+24</td>\n",
       "      <td>1.083214e+14</td>\n",
       "      <td>1587.280389</td>\n",
       "      <td>5.648788e+13</td>\n",
       "      <td>6.649235e+22</td>\n",
       "      <td>-2.105215e+25</td>\n",
       "      <td>13390.341243</td>\n",
       "      <td>-0.188014</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419279</td>\n",
       "      <td>5.413495</td>\n",
       "      <td>5.353073</td>\n",
       "      <td>5.232006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6894.216660</td>\n",
       "      <td>8.429531e+10</td>\n",
       "      <td>1.577024e+24</td>\n",
       "      <td>1.090471e+14</td>\n",
       "      <td>1530.971594</td>\n",
       "      <td>5.153441e+13</td>\n",
       "      <td>6.685179e+22</td>\n",
       "      <td>-2.175302e+25</td>\n",
       "      <td>13251.479423</td>\n",
       "      <td>-0.194314</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413199</td>\n",
       "      <td>5.418618</td>\n",
       "      <td>5.341884</td>\n",
       "      <td>5.221831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6906.079329</td>\n",
       "      <td>8.449927e+10</td>\n",
       "      <td>1.586829e+24</td>\n",
       "      <td>1.088327e+14</td>\n",
       "      <td>1580.642155</td>\n",
       "      <td>5.204076e+13</td>\n",
       "      <td>6.646028e+22</td>\n",
       "      <td>-2.155312e+25</td>\n",
       "      <td>13485.254521</td>\n",
       "      <td>-0.192063</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419249</td>\n",
       "      <td>5.426273</td>\n",
       "      <td>5.354147</td>\n",
       "      <td>5.243087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6899.094797</td>\n",
       "      <td>8.451908e+10</td>\n",
       "      <td>1.601676e+24</td>\n",
       "      <td>1.091217e+14</td>\n",
       "      <td>1562.139216</td>\n",
       "      <td>5.105149e+13</td>\n",
       "      <td>6.593766e+22</td>\n",
       "      <td>-2.084657e+25</td>\n",
       "      <td>13729.350076</td>\n",
       "      <td>-0.185724</td>\n",
       "      <td>...</td>\n",
       "      <td>5.427837</td>\n",
       "      <td>5.393532</td>\n",
       "      <td>5.321879</td>\n",
       "      <td>5.221579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6911.805621</td>\n",
       "      <td>8.453654e+10</td>\n",
       "      <td>1.584634e+24</td>\n",
       "      <td>1.085458e+14</td>\n",
       "      <td>1608.232234</td>\n",
       "      <td>5.032479e+13</td>\n",
       "      <td>6.626648e+22</td>\n",
       "      <td>-2.168199e+25</td>\n",
       "      <td>13676.366262</td>\n",
       "      <td>-0.193127</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421960</td>\n",
       "      <td>5.419106</td>\n",
       "      <td>5.349870</td>\n",
       "      <td>5.231510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6946.807156</td>\n",
       "      <td>8.448848e+10</td>\n",
       "      <td>1.588865e+24</td>\n",
       "      <td>1.085662e+14</td>\n",
       "      <td>1598.537641</td>\n",
       "      <td>4.720406e+13</td>\n",
       "      <td>6.604935e+22</td>\n",
       "      <td>-2.160836e+25</td>\n",
       "      <td>13798.923015</td>\n",
       "      <td>-0.192580</td>\n",
       "      <td>...</td>\n",
       "      <td>5.409650</td>\n",
       "      <td>5.423591</td>\n",
       "      <td>5.336414</td>\n",
       "      <td>5.216673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6886.818627</td>\n",
       "      <td>8.431700e+10</td>\n",
       "      <td>1.578638e+24</td>\n",
       "      <td>1.071168e+14</td>\n",
       "      <td>1581.500858</td>\n",
       "      <td>4.759987e+13</td>\n",
       "      <td>6.570224e+22</td>\n",
       "      <td>-2.183161e+25</td>\n",
       "      <td>13867.168557</td>\n",
       "      <td>-0.194966</td>\n",
       "      <td>...</td>\n",
       "      <td>5.404884</td>\n",
       "      <td>5.435943</td>\n",
       "      <td>5.344661</td>\n",
       "      <td>5.215722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6818.410837</td>\n",
       "      <td>8.425498e+10</td>\n",
       "      <td>1.577545e+24</td>\n",
       "      <td>1.059483e+14</td>\n",
       "      <td>1552.157851</td>\n",
       "      <td>4.509200e+13</td>\n",
       "      <td>6.554720e+22</td>\n",
       "      <td>-2.179559e+25</td>\n",
       "      <td>14013.525138</td>\n",
       "      <td>-0.194787</td>\n",
       "      <td>...</td>\n",
       "      <td>5.395443</td>\n",
       "      <td>5.426074</td>\n",
       "      <td>5.338376</td>\n",
       "      <td>5.218632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6837.526272</td>\n",
       "      <td>8.424089e+10</td>\n",
       "      <td>1.577035e+24</td>\n",
       "      <td>1.051378e+14</td>\n",
       "      <td>1623.889248</td>\n",
       "      <td>4.995272e+13</td>\n",
       "      <td>6.540712e+22</td>\n",
       "      <td>-2.187353e+25</td>\n",
       "      <td>14092.449955</td>\n",
       "      <td>-0.195516</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414608</td>\n",
       "      <td>5.430777</td>\n",
       "      <td>5.334956</td>\n",
       "      <td>5.211968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6813.759545</td>\n",
       "      <td>8.408494e+10</td>\n",
       "      <td>1.570983e+24</td>\n",
       "      <td>1.053031e+14</td>\n",
       "      <td>1530.518892</td>\n",
       "      <td>4.714137e+13</td>\n",
       "      <td>6.541886e+22</td>\n",
       "      <td>-2.204741e+25</td>\n",
       "      <td>14125.375441</td>\n",
       "      <td>-0.197436</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414585</td>\n",
       "      <td>5.439978</td>\n",
       "      <td>5.349852</td>\n",
       "      <td>5.199410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6810.946741</td>\n",
       "      <td>8.387123e+10</td>\n",
       "      <td>1.559647e+24</td>\n",
       "      <td>1.054864e+14</td>\n",
       "      <td>1553.297953</td>\n",
       "      <td>5.192169e+13</td>\n",
       "      <td>6.534533e+22</td>\n",
       "      <td>-2.227891e+25</td>\n",
       "      <td>14030.816852</td>\n",
       "      <td>-0.200018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.426539</td>\n",
       "      <td>5.439688</td>\n",
       "      <td>5.333861</td>\n",
       "      <td>5.220641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6750.173733</td>\n",
       "      <td>8.393793e+10</td>\n",
       "      <td>1.567801e+24</td>\n",
       "      <td>1.044780e+14</td>\n",
       "      <td>1529.499791</td>\n",
       "      <td>5.263473e+13</td>\n",
       "      <td>6.513396e+22</td>\n",
       "      <td>-2.198664e+25</td>\n",
       "      <td>14122.395674</td>\n",
       "      <td>-0.197237</td>\n",
       "      <td>...</td>\n",
       "      <td>5.427973</td>\n",
       "      <td>5.437507</td>\n",
       "      <td>5.321049</td>\n",
       "      <td>5.218753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6625.715466</td>\n",
       "      <td>8.361261e+10</td>\n",
       "      <td>1.570021e+24</td>\n",
       "      <td>1.039780e+14</td>\n",
       "      <td>1558.476742</td>\n",
       "      <td>5.476180e+13</td>\n",
       "      <td>6.486121e+22</td>\n",
       "      <td>-2.151174e+25</td>\n",
       "      <td>14176.476618</td>\n",
       "      <td>-0.193727</td>\n",
       "      <td>...</td>\n",
       "      <td>5.420675</td>\n",
       "      <td>5.402765</td>\n",
       "      <td>5.321893</td>\n",
       "      <td>5.207600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6602.456377</td>\n",
       "      <td>8.343825e+10</td>\n",
       "      <td>1.559968e+24</td>\n",
       "      <td>1.037375e+14</td>\n",
       "      <td>1475.698999</td>\n",
       "      <td>5.215698e+13</td>\n",
       "      <td>6.508146e+22</td>\n",
       "      <td>-2.175203e+25</td>\n",
       "      <td>13994.417647</td>\n",
       "      <td>-0.196301</td>\n",
       "      <td>...</td>\n",
       "      <td>5.424027</td>\n",
       "      <td>5.401567</td>\n",
       "      <td>5.328385</td>\n",
       "      <td>5.212209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6579.279348</td>\n",
       "      <td>8.332854e+10</td>\n",
       "      <td>1.556961e+24</td>\n",
       "      <td>1.027367e+14</td>\n",
       "      <td>1497.390470</td>\n",
       "      <td>5.335861e+13</td>\n",
       "      <td>6.486579e+22</td>\n",
       "      <td>-2.183151e+25</td>\n",
       "      <td>14085.404680</td>\n",
       "      <td>-0.197277</td>\n",
       "      <td>...</td>\n",
       "      <td>5.412794</td>\n",
       "      <td>5.392322</td>\n",
       "      <td>5.331879</td>\n",
       "      <td>5.229455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6659.340976</td>\n",
       "      <td>8.366482e+10</td>\n",
       "      <td>1.561306e+24</td>\n",
       "      <td>1.038385e+14</td>\n",
       "      <td>1595.699476</td>\n",
       "      <td>5.790126e+13</td>\n",
       "      <td>6.530352e+22</td>\n",
       "      <td>-2.204153e+25</td>\n",
       "      <td>13978.934717</td>\n",
       "      <td>-0.198375</td>\n",
       "      <td>...</td>\n",
       "      <td>5.407542</td>\n",
       "      <td>5.395212</td>\n",
       "      <td>5.338315</td>\n",
       "      <td>5.199087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2201.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6674.435798</td>\n",
       "      <td>8.363221e+10</td>\n",
       "      <td>1.558285e+24</td>\n",
       "      <td>1.049437e+14</td>\n",
       "      <td>1561.801278</td>\n",
       "      <td>5.628489e+13</td>\n",
       "      <td>6.543992e+22</td>\n",
       "      <td>-2.211603e+25</td>\n",
       "      <td>13883.545665</td>\n",
       "      <td>-0.199123</td>\n",
       "      <td>...</td>\n",
       "      <td>5.418874</td>\n",
       "      <td>5.404427</td>\n",
       "      <td>5.351370</td>\n",
       "      <td>5.195523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2147.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6628.940407</td>\n",
       "      <td>8.364418e+10</td>\n",
       "      <td>1.560219e+24</td>\n",
       "      <td>1.037925e+14</td>\n",
       "      <td>1558.677333</td>\n",
       "      <td>5.651785e+13</td>\n",
       "      <td>6.543197e+22</td>\n",
       "      <td>-2.211868e+25</td>\n",
       "      <td>13923.843184</td>\n",
       "      <td>-0.199118</td>\n",
       "      <td>...</td>\n",
       "      <td>5.429408</td>\n",
       "      <td>5.417444</td>\n",
       "      <td>5.352889</td>\n",
       "      <td>5.225211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6534.272066</td>\n",
       "      <td>8.347730e+10</td>\n",
       "      <td>1.564493e+24</td>\n",
       "      <td>1.040900e+14</td>\n",
       "      <td>1475.048207</td>\n",
       "      <td>5.209285e+13</td>\n",
       "      <td>6.540494e+22</td>\n",
       "      <td>-2.160368e+25</td>\n",
       "      <td>13833.285734</td>\n",
       "      <td>-0.194871</td>\n",
       "      <td>...</td>\n",
       "      <td>5.429178</td>\n",
       "      <td>5.405270</td>\n",
       "      <td>5.353649</td>\n",
       "      <td>5.195410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6618.672577</td>\n",
       "      <td>8.346451e+10</td>\n",
       "      <td>1.557323e+24</td>\n",
       "      <td>1.033482e+14</td>\n",
       "      <td>1600.107458</td>\n",
       "      <td>5.740748e+13</td>\n",
       "      <td>6.560208e+22</td>\n",
       "      <td>-2.194959e+25</td>\n",
       "      <td>13785.588862</td>\n",
       "      <td>-0.198021</td>\n",
       "      <td>...</td>\n",
       "      <td>5.446002</td>\n",
       "      <td>5.412405</td>\n",
       "      <td>5.345017</td>\n",
       "      <td>5.246542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2193.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6556.497525</td>\n",
       "      <td>8.331999e+10</td>\n",
       "      <td>1.555459e+24</td>\n",
       "      <td>1.036262e+14</td>\n",
       "      <td>1582.988346</td>\n",
       "      <td>5.571156e+13</td>\n",
       "      <td>6.565363e+22</td>\n",
       "      <td>-2.201092e+25</td>\n",
       "      <td>13704.966138</td>\n",
       "      <td>-0.198919</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436352</td>\n",
       "      <td>5.416115</td>\n",
       "      <td>5.330692</td>\n",
       "      <td>5.228028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6664.787875</td>\n",
       "      <td>8.381583e+10</td>\n",
       "      <td>1.554123e+24</td>\n",
       "      <td>1.049621e+14</td>\n",
       "      <td>1607.631910</td>\n",
       "      <td>5.965254e+13</td>\n",
       "      <td>6.624869e+22</td>\n",
       "      <td>-2.268300e+25</td>\n",
       "      <td>13567.615361</td>\n",
       "      <td>-0.203780</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449425</td>\n",
       "      <td>5.438875</td>\n",
       "      <td>5.323645</td>\n",
       "      <td>5.244872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6610.832931</td>\n",
       "      <td>8.372099e+10</td>\n",
       "      <td>1.551750e+24</td>\n",
       "      <td>1.045356e+14</td>\n",
       "      <td>1628.840026</td>\n",
       "      <td>5.812513e+13</td>\n",
       "      <td>6.629692e+22</td>\n",
       "      <td>-2.266189e+25</td>\n",
       "      <td>13518.841224</td>\n",
       "      <td>-0.203821</td>\n",
       "      <td>...</td>\n",
       "      <td>5.449262</td>\n",
       "      <td>5.424079</td>\n",
       "      <td>5.304066</td>\n",
       "      <td>5.238537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6623.903993</td>\n",
       "      <td>8.393815e+10</td>\n",
       "      <td>1.547466e+24</td>\n",
       "      <td>1.046448e+14</td>\n",
       "      <td>1681.735488</td>\n",
       "      <td>6.043938e+13</td>\n",
       "      <td>6.663268e+22</td>\n",
       "      <td>-2.302672e+25</td>\n",
       "      <td>13449.626827</td>\n",
       "      <td>-0.206567</td>\n",
       "      <td>...</td>\n",
       "      <td>5.438638</td>\n",
       "      <td>5.447932</td>\n",
       "      <td>5.311403</td>\n",
       "      <td>5.249751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6604.636688</td>\n",
       "      <td>8.384039e+10</td>\n",
       "      <td>1.543670e+24</td>\n",
       "      <td>1.042238e+14</td>\n",
       "      <td>1658.377172</td>\n",
       "      <td>5.433214e+13</td>\n",
       "      <td>6.649111e+22</td>\n",
       "      <td>-2.306179e+25</td>\n",
       "      <td>13492.628976</td>\n",
       "      <td>-0.207122</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415847</td>\n",
       "      <td>5.465051</td>\n",
       "      <td>5.325372</td>\n",
       "      <td>5.225893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6590.454932</td>\n",
       "      <td>8.376975e+10</td>\n",
       "      <td>1.544275e+24</td>\n",
       "      <td>1.037572e+14</td>\n",
       "      <td>1656.194789</td>\n",
       "      <td>5.797011e+13</td>\n",
       "      <td>6.652554e+22</td>\n",
       "      <td>-2.285079e+25</td>\n",
       "      <td>13423.901150</td>\n",
       "      <td>-0.205400</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436240</td>\n",
       "      <td>5.471888</td>\n",
       "      <td>5.326894</td>\n",
       "      <td>5.228644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6554.577321</td>\n",
       "      <td>8.382140e+10</td>\n",
       "      <td>1.551716e+24</td>\n",
       "      <td>1.037426e+14</td>\n",
       "      <td>1666.964946</td>\n",
       "      <td>5.595178e+13</td>\n",
       "      <td>6.659935e+22</td>\n",
       "      <td>-2.270023e+25</td>\n",
       "      <td>13490.144006</td>\n",
       "      <td>-0.203921</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421760</td>\n",
       "      <td>5.455594</td>\n",
       "      <td>5.327001</td>\n",
       "      <td>5.247861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6589.028665</td>\n",
       "      <td>8.362348e+10</td>\n",
       "      <td>1.548201e+24</td>\n",
       "      <td>1.047694e+14</td>\n",
       "      <td>1718.904126</td>\n",
       "      <td>5.865956e+13</td>\n",
       "      <td>6.661440e+22</td>\n",
       "      <td>-2.252770e+25</td>\n",
       "      <td>13382.792443</td>\n",
       "      <td>-0.202850</td>\n",
       "      <td>...</td>\n",
       "      <td>5.416107</td>\n",
       "      <td>5.438976</td>\n",
       "      <td>5.316460</td>\n",
       "      <td>5.241509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6696.230175</td>\n",
       "      <td>8.374229e+10</td>\n",
       "      <td>1.550484e+24</td>\n",
       "      <td>1.050534e+14</td>\n",
       "      <td>1780.950881</td>\n",
       "      <td>6.372367e+13</td>\n",
       "      <td>6.670024e+22</td>\n",
       "      <td>-2.260599e+25</td>\n",
       "      <td>13434.294831</td>\n",
       "      <td>-0.203267</td>\n",
       "      <td>...</td>\n",
       "      <td>5.403140</td>\n",
       "      <td>5.454633</td>\n",
       "      <td>5.323734</td>\n",
       "      <td>5.228264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6680.609878</td>\n",
       "      <td>8.378515e+10</td>\n",
       "      <td>1.540382e+24</td>\n",
       "      <td>1.042639e+14</td>\n",
       "      <td>1790.150343</td>\n",
       "      <td>6.220128e+13</td>\n",
       "      <td>6.695897e+22</td>\n",
       "      <td>-2.314251e+25</td>\n",
       "      <td>13321.778770</td>\n",
       "      <td>-0.207984</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405974</td>\n",
       "      <td>5.434141</td>\n",
       "      <td>5.301531</td>\n",
       "      <td>5.222481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6655.976645</td>\n",
       "      <td>8.391120e+10</td>\n",
       "      <td>1.542652e+24</td>\n",
       "      <td>1.037527e+14</td>\n",
       "      <td>1774.532620</td>\n",
       "      <td>6.236907e+13</td>\n",
       "      <td>6.709688e+22</td>\n",
       "      <td>-2.312158e+25</td>\n",
       "      <td>13351.836402</td>\n",
       "      <td>-0.207484</td>\n",
       "      <td>...</td>\n",
       "      <td>5.412006</td>\n",
       "      <td>5.447493</td>\n",
       "      <td>5.318548</td>\n",
       "      <td>5.231530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6621.583380</td>\n",
       "      <td>8.412847e+10</td>\n",
       "      <td>1.550473e+24</td>\n",
       "      <td>1.048586e+14</td>\n",
       "      <td>1828.370218</td>\n",
       "      <td>6.313825e+13</td>\n",
       "      <td>6.710731e+22</td>\n",
       "      <td>-2.288411e+25</td>\n",
       "      <td>13328.826569</td>\n",
       "      <td>-0.204823</td>\n",
       "      <td>...</td>\n",
       "      <td>5.399834</td>\n",
       "      <td>5.472706</td>\n",
       "      <td>5.313747</td>\n",
       "      <td>5.233165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2326.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6714.414943</td>\n",
       "      <td>8.417939e+10</td>\n",
       "      <td>1.533120e+24</td>\n",
       "      <td>1.041958e+14</td>\n",
       "      <td>1799.467489</td>\n",
       "      <td>6.292113e+13</td>\n",
       "      <td>6.729763e+22</td>\n",
       "      <td>-2.367600e+25</td>\n",
       "      <td>13291.640048</td>\n",
       "      <td>-0.211782</td>\n",
       "      <td>...</td>\n",
       "      <td>5.409866</td>\n",
       "      <td>5.463669</td>\n",
       "      <td>5.298373</td>\n",
       "      <td>5.224789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2385.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6713.854682</td>\n",
       "      <td>8.428436e+10</td>\n",
       "      <td>1.542590e+24</td>\n",
       "      <td>1.039199e+14</td>\n",
       "      <td>1784.416135</td>\n",
       "      <td>6.210627e+13</td>\n",
       "      <td>6.705084e+22</td>\n",
       "      <td>-2.349227e+25</td>\n",
       "      <td>13439.508935</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>...</td>\n",
       "      <td>5.431626</td>\n",
       "      <td>5.451033</td>\n",
       "      <td>5.297726</td>\n",
       "      <td>5.220776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2336.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6675.049946</td>\n",
       "      <td>8.433016e+10</td>\n",
       "      <td>1.538034e+24</td>\n",
       "      <td>1.040402e+14</td>\n",
       "      <td>1819.143184</td>\n",
       "      <td>6.488646e+13</td>\n",
       "      <td>6.720600e+22</td>\n",
       "      <td>-2.352803e+25</td>\n",
       "      <td>13388.201074</td>\n",
       "      <td>-0.210083</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421072</td>\n",
       "      <td>5.471903</td>\n",
       "      <td>5.294902</td>\n",
       "      <td>5.225364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2445.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6651.842005</td>\n",
       "      <td>8.457251e+10</td>\n",
       "      <td>1.546075e+24</td>\n",
       "      <td>1.035750e+14</td>\n",
       "      <td>1821.234321</td>\n",
       "      <td>6.246896e+13</td>\n",
       "      <td>6.683234e+22</td>\n",
       "      <td>-2.344465e+25</td>\n",
       "      <td>13559.769347</td>\n",
       "      <td>-0.208738</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413351</td>\n",
       "      <td>5.463487</td>\n",
       "      <td>5.295877</td>\n",
       "      <td>5.215056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6618.690252</td>\n",
       "      <td>8.479324e+10</td>\n",
       "      <td>1.556837e+24</td>\n",
       "      <td>1.026949e+14</td>\n",
       "      <td>1879.972741</td>\n",
       "      <td>6.718395e+13</td>\n",
       "      <td>6.662508e+22</td>\n",
       "      <td>-2.320355e+25</td>\n",
       "      <td>13744.924963</td>\n",
       "      <td>-0.206054</td>\n",
       "      <td>...</td>\n",
       "      <td>5.418516</td>\n",
       "      <td>5.460616</td>\n",
       "      <td>5.305683</td>\n",
       "      <td>5.225273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2574.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6689.582061</td>\n",
       "      <td>8.511911e+10</td>\n",
       "      <td>1.554712e+24</td>\n",
       "      <td>1.029952e+14</td>\n",
       "      <td>1920.438043</td>\n",
       "      <td>6.659065e+13</td>\n",
       "      <td>6.703677e+22</td>\n",
       "      <td>-2.359518e+25</td>\n",
       "      <td>13709.773165</td>\n",
       "      <td>-0.208729</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413922</td>\n",
       "      <td>5.463642</td>\n",
       "      <td>5.314288</td>\n",
       "      <td>5.241342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6651.957058</td>\n",
       "      <td>8.528108e+10</td>\n",
       "      <td>1.556176e+24</td>\n",
       "      <td>1.023277e+14</td>\n",
       "      <td>1799.937489</td>\n",
       "      <td>6.106326e+13</td>\n",
       "      <td>6.719677e+22</td>\n",
       "      <td>-2.369129e+25</td>\n",
       "      <td>13673.206273</td>\n",
       "      <td>-0.209182</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415397</td>\n",
       "      <td>5.484200</td>\n",
       "      <td>5.318812</td>\n",
       "      <td>5.233028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6729.800739</td>\n",
       "      <td>8.549333e+10</td>\n",
       "      <td>1.566051e+24</td>\n",
       "      <td>1.030126e+14</td>\n",
       "      <td>1810.811383</td>\n",
       "      <td>6.165691e+13</td>\n",
       "      <td>6.719766e+22</td>\n",
       "      <td>-2.351989e+25</td>\n",
       "      <td>13800.878660</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>...</td>\n",
       "      <td>5.423642</td>\n",
       "      <td>5.449615</td>\n",
       "      <td>5.301622</td>\n",
       "      <td>5.208788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2513.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6749.666713</td>\n",
       "      <td>8.547887e+10</td>\n",
       "      <td>1.565630e+24</td>\n",
       "      <td>1.027505e+14</td>\n",
       "      <td>1816.423463</td>\n",
       "      <td>6.445213e+13</td>\n",
       "      <td>6.702058e+22</td>\n",
       "      <td>-2.357341e+25</td>\n",
       "      <td>13929.375838</td>\n",
       "      <td>-0.207659</td>\n",
       "      <td>...</td>\n",
       "      <td>5.419790</td>\n",
       "      <td>5.491169</td>\n",
       "      <td>5.298181</td>\n",
       "      <td>5.240031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2672.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6712.981030</td>\n",
       "      <td>8.579096e+10</td>\n",
       "      <td>1.570802e+24</td>\n",
       "      <td>1.032009e+14</td>\n",
       "      <td>1875.490188</td>\n",
       "      <td>6.395814e+13</td>\n",
       "      <td>6.737582e+22</td>\n",
       "      <td>-2.368335e+25</td>\n",
       "      <td>13878.799260</td>\n",
       "      <td>-0.207869</td>\n",
       "      <td>...</td>\n",
       "      <td>5.414254</td>\n",
       "      <td>5.486133</td>\n",
       "      <td>5.296874</td>\n",
       "      <td>5.243491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6694.946004</td>\n",
       "      <td>8.595762e+10</td>\n",
       "      <td>1.565608e+24</td>\n",
       "      <td>1.032871e+14</td>\n",
       "      <td>1714.993210</td>\n",
       "      <td>5.665472e+13</td>\n",
       "      <td>6.773257e+22</td>\n",
       "      <td>-2.408134e+25</td>\n",
       "      <td>13738.599523</td>\n",
       "      <td>-0.210952</td>\n",
       "      <td>...</td>\n",
       "      <td>5.408871</td>\n",
       "      <td>5.489321</td>\n",
       "      <td>5.307261</td>\n",
       "      <td>5.219881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6720.925432</td>\n",
       "      <td>8.642296e+10</td>\n",
       "      <td>1.568783e+24</td>\n",
       "      <td>1.033549e+14</td>\n",
       "      <td>1788.925676</td>\n",
       "      <td>6.013856e+13</td>\n",
       "      <td>6.814229e+22</td>\n",
       "      <td>-2.453392e+25</td>\n",
       "      <td>13722.647950</td>\n",
       "      <td>-0.213759</td>\n",
       "      <td>...</td>\n",
       "      <td>5.410845</td>\n",
       "      <td>5.464945</td>\n",
       "      <td>5.290419</td>\n",
       "      <td>5.235641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2616.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6643.833325</td>\n",
       "      <td>8.686979e+10</td>\n",
       "      <td>1.585399e+24</td>\n",
       "      <td>1.027543e+14</td>\n",
       "      <td>1811.564073</td>\n",
       "      <td>5.910807e+13</td>\n",
       "      <td>6.827774e+22</td>\n",
       "      <td>-2.426738e+25</td>\n",
       "      <td>13834.722831</td>\n",
       "      <td>-0.210350</td>\n",
       "      <td>...</td>\n",
       "      <td>5.415777</td>\n",
       "      <td>5.462901</td>\n",
       "      <td>5.289332</td>\n",
       "      <td>5.233490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2473.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6678.435053</td>\n",
       "      <td>8.726270e+10</td>\n",
       "      <td>1.588082e+24</td>\n",
       "      <td>1.029321e+14</td>\n",
       "      <td>1888.605961</td>\n",
       "      <td>6.151702e+13</td>\n",
       "      <td>6.895435e+22</td>\n",
       "      <td>-2.466136e+25</td>\n",
       "      <td>13750.892245</td>\n",
       "      <td>-0.212802</td>\n",
       "      <td>...</td>\n",
       "      <td>5.407051</td>\n",
       "      <td>5.498585</td>\n",
       "      <td>5.300528</td>\n",
       "      <td>5.232761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6719.481170</td>\n",
       "      <td>8.771953e+10</td>\n",
       "      <td>1.595610e+24</td>\n",
       "      <td>1.032735e+14</td>\n",
       "      <td>1839.960188</td>\n",
       "      <td>6.065092e+13</td>\n",
       "      <td>6.955306e+22</td>\n",
       "      <td>-2.491348e+25</td>\n",
       "      <td>13604.712481</td>\n",
       "      <td>-0.213858</td>\n",
       "      <td>...</td>\n",
       "      <td>5.430693</td>\n",
       "      <td>5.476389</td>\n",
       "      <td>5.304190</td>\n",
       "      <td>5.233783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2460.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6731.164211</td>\n",
       "      <td>8.808078e+10</td>\n",
       "      <td>1.601331e+24</td>\n",
       "      <td>1.040559e+14</td>\n",
       "      <td>1870.737825</td>\n",
       "      <td>6.834156e+13</td>\n",
       "      <td>6.991464e+22</td>\n",
       "      <td>-2.486756e+25</td>\n",
       "      <td>13517.112435</td>\n",
       "      <td>-0.212588</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434391</td>\n",
       "      <td>5.482465</td>\n",
       "      <td>5.301332</td>\n",
       "      <td>5.229522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2138.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6743.632587</td>\n",
       "      <td>8.818426e+10</td>\n",
       "      <td>1.600107e+24</td>\n",
       "      <td>1.048489e+14</td>\n",
       "      <td>1870.199589</td>\n",
       "      <td>6.936537e+13</td>\n",
       "      <td>7.029250e+22</td>\n",
       "      <td>-2.508392e+25</td>\n",
       "      <td>13356.904229</td>\n",
       "      <td>-0.214186</td>\n",
       "      <td>...</td>\n",
       "      <td>5.425442</td>\n",
       "      <td>5.483249</td>\n",
       "      <td>5.316916</td>\n",
       "      <td>5.207895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6873.947508</td>\n",
       "      <td>8.867265e+10</td>\n",
       "      <td>1.605313e+24</td>\n",
       "      <td>1.060069e+14</td>\n",
       "      <td>1904.759793</td>\n",
       "      <td>6.729846e+13</td>\n",
       "      <td>7.111755e+22</td>\n",
       "      <td>-2.558268e+25</td>\n",
       "      <td>13233.412164</td>\n",
       "      <td>-0.217242</td>\n",
       "      <td>...</td>\n",
       "      <td>5.428043</td>\n",
       "      <td>5.480052</td>\n",
       "      <td>5.321115</td>\n",
       "      <td>5.230335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6901.910797</td>\n",
       "      <td>8.892919e+10</td>\n",
       "      <td>1.600205e+24</td>\n",
       "      <td>1.069829e+14</td>\n",
       "      <td>1909.576325</td>\n",
       "      <td>6.973227e+13</td>\n",
       "      <td>7.179346e+22</td>\n",
       "      <td>-2.599559e+25</td>\n",
       "      <td>13050.715628</td>\n",
       "      <td>-0.220111</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434801</td>\n",
       "      <td>5.483810</td>\n",
       "      <td>5.317496</td>\n",
       "      <td>5.226047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6951.264001</td>\n",
       "      <td>8.907855e+10</td>\n",
       "      <td>1.597570e+24</td>\n",
       "      <td>1.084181e+14</td>\n",
       "      <td>1931.615401</td>\n",
       "      <td>7.088334e+13</td>\n",
       "      <td>7.229525e+22</td>\n",
       "      <td>-2.639231e+25</td>\n",
       "      <td>12883.123491</td>\n",
       "      <td>-0.223096</td>\n",
       "      <td>...</td>\n",
       "      <td>5.433837</td>\n",
       "      <td>5.479156</td>\n",
       "      <td>5.305367</td>\n",
       "      <td>5.208082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6916.164823</td>\n",
       "      <td>8.879042e+10</td>\n",
       "      <td>1.595188e+24</td>\n",
       "      <td>1.082073e+14</td>\n",
       "      <td>1893.966861</td>\n",
       "      <td>6.963535e+13</td>\n",
       "      <td>7.229235e+22</td>\n",
       "      <td>-2.627192e+25</td>\n",
       "      <td>12791.450513</td>\n",
       "      <td>-0.222799</td>\n",
       "      <td>...</td>\n",
       "      <td>5.435923</td>\n",
       "      <td>5.495387</td>\n",
       "      <td>5.309339</td>\n",
       "      <td>5.212751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6954.711005</td>\n",
       "      <td>8.892432e+10</td>\n",
       "      <td>1.585478e+24</td>\n",
       "      <td>1.077902e+14</td>\n",
       "      <td>1915.377016</td>\n",
       "      <td>7.154282e+13</td>\n",
       "      <td>7.282973e+22</td>\n",
       "      <td>-2.687569e+25</td>\n",
       "      <td>12666.103787</td>\n",
       "      <td>-0.227576</td>\n",
       "      <td>...</td>\n",
       "      <td>5.405683</td>\n",
       "      <td>5.495611</td>\n",
       "      <td>5.322693</td>\n",
       "      <td>5.194471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3            4   \\\n",
       "0   6748.177901  8.294421e+10  1.591225e+24  1.095967e+14  1510.190735   \n",
       "1   6713.245495  8.301113e+10  1.591180e+24  1.082126e+14  1552.899870   \n",
       "2   6711.746592  8.324237e+10  1.585508e+24  1.074454e+14  1569.593570   \n",
       "3   6903.974288  8.343257e+10  1.576556e+24  1.084795e+14  1608.218901   \n",
       "4   6691.365852  8.370511e+10  1.600757e+24  1.086643e+14  1479.272564   \n",
       "5   6834.958217  8.406158e+10  1.591036e+24  1.096684e+14  1644.488148   \n",
       "6   6830.348029  8.431275e+10  1.589851e+24  1.083214e+14  1587.280389   \n",
       "7   6894.216660  8.429531e+10  1.577024e+24  1.090471e+14  1530.971594   \n",
       "8   6906.079329  8.449927e+10  1.586829e+24  1.088327e+14  1580.642155   \n",
       "9   6899.094797  8.451908e+10  1.601676e+24  1.091217e+14  1562.139216   \n",
       "10  6911.805621  8.453654e+10  1.584634e+24  1.085458e+14  1608.232234   \n",
       "11  6946.807156  8.448848e+10  1.588865e+24  1.085662e+14  1598.537641   \n",
       "12  6886.818627  8.431700e+10  1.578638e+24  1.071168e+14  1581.500858   \n",
       "13  6818.410837  8.425498e+10  1.577545e+24  1.059483e+14  1552.157851   \n",
       "14  6837.526272  8.424089e+10  1.577035e+24  1.051378e+14  1623.889248   \n",
       "15  6813.759545  8.408494e+10  1.570983e+24  1.053031e+14  1530.518892   \n",
       "16  6810.946741  8.387123e+10  1.559647e+24  1.054864e+14  1553.297953   \n",
       "17  6750.173733  8.393793e+10  1.567801e+24  1.044780e+14  1529.499791   \n",
       "18  6625.715466  8.361261e+10  1.570021e+24  1.039780e+14  1558.476742   \n",
       "19  6602.456377  8.343825e+10  1.559968e+24  1.037375e+14  1475.698999   \n",
       "20  6579.279348  8.332854e+10  1.556961e+24  1.027367e+14  1497.390470   \n",
       "21  6659.340976  8.366482e+10  1.561306e+24  1.038385e+14  1595.699476   \n",
       "22  6674.435798  8.363221e+10  1.558285e+24  1.049437e+14  1561.801278   \n",
       "23  6628.940407  8.364418e+10  1.560219e+24  1.037925e+14  1558.677333   \n",
       "24  6534.272066  8.347730e+10  1.564493e+24  1.040900e+14  1475.048207   \n",
       "25  6618.672577  8.346451e+10  1.557323e+24  1.033482e+14  1600.107458   \n",
       "26  6556.497525  8.331999e+10  1.555459e+24  1.036262e+14  1582.988346   \n",
       "27  6664.787875  8.381583e+10  1.554123e+24  1.049621e+14  1607.631910   \n",
       "28  6610.832931  8.372099e+10  1.551750e+24  1.045356e+14  1628.840026   \n",
       "29  6623.903993  8.393815e+10  1.547466e+24  1.046448e+14  1681.735488   \n",
       "30  6604.636688  8.384039e+10  1.543670e+24  1.042238e+14  1658.377172   \n",
       "31  6590.454932  8.376975e+10  1.544275e+24  1.037572e+14  1656.194789   \n",
       "32  6554.577321  8.382140e+10  1.551716e+24  1.037426e+14  1666.964946   \n",
       "33  6589.028665  8.362348e+10  1.548201e+24  1.047694e+14  1718.904126   \n",
       "34  6696.230175  8.374229e+10  1.550484e+24  1.050534e+14  1780.950881   \n",
       "35  6680.609878  8.378515e+10  1.540382e+24  1.042639e+14  1790.150343   \n",
       "36  6655.976645  8.391120e+10  1.542652e+24  1.037527e+14  1774.532620   \n",
       "37  6621.583380  8.412847e+10  1.550473e+24  1.048586e+14  1828.370218   \n",
       "38  6714.414943  8.417939e+10  1.533120e+24  1.041958e+14  1799.467489   \n",
       "39  6713.854682  8.428436e+10  1.542590e+24  1.039199e+14  1784.416135   \n",
       "40  6675.049946  8.433016e+10  1.538034e+24  1.040402e+14  1819.143184   \n",
       "41  6651.842005  8.457251e+10  1.546075e+24  1.035750e+14  1821.234321   \n",
       "42  6618.690252  8.479324e+10  1.556837e+24  1.026949e+14  1879.972741   \n",
       "43  6689.582061  8.511911e+10  1.554712e+24  1.029952e+14  1920.438043   \n",
       "44  6651.957058  8.528108e+10  1.556176e+24  1.023277e+14  1799.937489   \n",
       "45  6729.800739  8.549333e+10  1.566051e+24  1.030126e+14  1810.811383   \n",
       "46  6749.666713  8.547887e+10  1.565630e+24  1.027505e+14  1816.423463   \n",
       "47  6712.981030  8.579096e+10  1.570802e+24  1.032009e+14  1875.490188   \n",
       "48  6694.946004  8.595762e+10  1.565608e+24  1.032871e+14  1714.993210   \n",
       "49  6720.925432  8.642296e+10  1.568783e+24  1.033549e+14  1788.925676   \n",
       "50  6643.833325  8.686979e+10  1.585399e+24  1.027543e+14  1811.564073   \n",
       "51  6678.435053  8.726270e+10  1.588082e+24  1.029321e+14  1888.605961   \n",
       "52  6719.481170  8.771953e+10  1.595610e+24  1.032735e+14  1839.960188   \n",
       "53  6731.164211  8.808078e+10  1.601331e+24  1.040559e+14  1870.737825   \n",
       "54  6743.632587  8.818426e+10  1.600107e+24  1.048489e+14  1870.199589   \n",
       "55  6873.947508  8.867265e+10  1.605313e+24  1.060069e+14  1904.759793   \n",
       "56  6901.910797  8.892919e+10  1.600205e+24  1.069829e+14  1909.576325   \n",
       "57  6951.264001  8.907855e+10  1.597570e+24  1.084181e+14  1931.615401   \n",
       "58  6916.164823  8.879042e+10  1.595188e+24  1.082073e+14  1893.966861   \n",
       "59  6954.711005  8.892432e+10  1.585478e+24  1.077902e+14  1915.377016   \n",
       "\n",
       "              5             6             7             8         9   ...  \\\n",
       "0   5.537032e+13  6.524454e+22 -1.945966e+25  13461.392092 -0.176659  ...   \n",
       "1   5.163912e+13  6.552311e+22 -1.967367e+25  13459.043822 -0.178458  ...   \n",
       "2   5.208053e+13  6.582194e+22 -2.022283e+25  13391.359484 -0.182930  ...   \n",
       "3   5.166028e+13  6.609184e+22 -2.072753e+25  13248.436229 -0.187068  ...   \n",
       "4   5.030951e+13  6.614713e+22 -1.999304e+25  13367.513454 -0.179852  ...   \n",
       "5   5.663543e+13  6.630166e+22 -2.075205e+25  13298.131406 -0.185888  ...   \n",
       "6   5.648788e+13  6.649235e+22 -2.105215e+25  13390.341243 -0.188014  ...   \n",
       "7   5.153441e+13  6.685179e+22 -2.175302e+25  13251.479423 -0.194314  ...   \n",
       "8   5.204076e+13  6.646028e+22 -2.155312e+25  13485.254521 -0.192063  ...   \n",
       "9   5.105149e+13  6.593766e+22 -2.084657e+25  13729.350076 -0.185724  ...   \n",
       "10  5.032479e+13  6.626648e+22 -2.168199e+25  13676.366262 -0.193127  ...   \n",
       "11  4.720406e+13  6.604935e+22 -2.160836e+25  13798.923015 -0.192580  ...   \n",
       "12  4.759987e+13  6.570224e+22 -2.183161e+25  13867.168557 -0.194966  ...   \n",
       "13  4.509200e+13  6.554720e+22 -2.179559e+25  14013.525138 -0.194787  ...   \n",
       "14  4.995272e+13  6.540712e+22 -2.187353e+25  14092.449955 -0.195516  ...   \n",
       "15  4.714137e+13  6.541886e+22 -2.204741e+25  14125.375441 -0.197436  ...   \n",
       "16  5.192169e+13  6.534533e+22 -2.227891e+25  14030.816852 -0.200018  ...   \n",
       "17  5.263473e+13  6.513396e+22 -2.198664e+25  14122.395674 -0.197237  ...   \n",
       "18  5.476180e+13  6.486121e+22 -2.151174e+25  14176.476618 -0.193727  ...   \n",
       "19  5.215698e+13  6.508146e+22 -2.175203e+25  13994.417647 -0.196301  ...   \n",
       "20  5.335861e+13  6.486579e+22 -2.183151e+25  14085.404680 -0.197277  ...   \n",
       "21  5.790126e+13  6.530352e+22 -2.204153e+25  13978.934717 -0.198375  ...   \n",
       "22  5.628489e+13  6.543992e+22 -2.211603e+25  13883.545665 -0.199123  ...   \n",
       "23  5.651785e+13  6.543197e+22 -2.211868e+25  13923.843184 -0.199118  ...   \n",
       "24  5.209285e+13  6.540494e+22 -2.160368e+25  13833.285734 -0.194871  ...   \n",
       "25  5.740748e+13  6.560208e+22 -2.194959e+25  13785.588862 -0.198021  ...   \n",
       "26  5.571156e+13  6.565363e+22 -2.201092e+25  13704.966138 -0.198919  ...   \n",
       "27  5.965254e+13  6.624869e+22 -2.268300e+25  13567.615361 -0.203780  ...   \n",
       "28  5.812513e+13  6.629692e+22 -2.266189e+25  13518.841224 -0.203821  ...   \n",
       "29  6.043938e+13  6.663268e+22 -2.302672e+25  13449.626827 -0.206567  ...   \n",
       "30  5.433214e+13  6.649111e+22 -2.306179e+25  13492.628976 -0.207122  ...   \n",
       "31  5.797011e+13  6.652554e+22 -2.285079e+25  13423.901150 -0.205400  ...   \n",
       "32  5.595178e+13  6.659935e+22 -2.270023e+25  13490.144006 -0.203921  ...   \n",
       "33  5.865956e+13  6.661440e+22 -2.252770e+25  13382.792443 -0.202850  ...   \n",
       "34  6.372367e+13  6.670024e+22 -2.260599e+25  13434.294831 -0.203267  ...   \n",
       "35  6.220128e+13  6.695897e+22 -2.314251e+25  13321.778770 -0.207984  ...   \n",
       "36  6.236907e+13  6.709688e+22 -2.312158e+25  13351.836402 -0.207484  ...   \n",
       "37  6.313825e+13  6.710731e+22 -2.288411e+25  13328.826569 -0.204823  ...   \n",
       "38  6.292113e+13  6.729763e+22 -2.367600e+25  13291.640048 -0.211782  ...   \n",
       "39  6.210627e+13  6.705084e+22 -2.349227e+25  13439.508935 -0.209877  ...   \n",
       "40  6.488646e+13  6.720600e+22 -2.352803e+25  13388.201074 -0.210083  ...   \n",
       "41  6.246896e+13  6.683234e+22 -2.344465e+25  13559.769347 -0.208738  ...   \n",
       "42  6.718395e+13  6.662508e+22 -2.320355e+25  13744.924963 -0.206054  ...   \n",
       "43  6.659065e+13  6.703677e+22 -2.359518e+25  13709.773165 -0.208729  ...   \n",
       "44  6.106326e+13  6.719677e+22 -2.369129e+25  13673.206273 -0.209182  ...   \n",
       "45  6.165691e+13  6.719766e+22 -2.351989e+25  13800.878660 -0.207153  ...   \n",
       "46  6.445213e+13  6.702058e+22 -2.357341e+25  13929.375838 -0.207659  ...   \n",
       "47  6.395814e+13  6.737582e+22 -2.368335e+25  13878.799260 -0.207869  ...   \n",
       "48  5.665472e+13  6.773257e+22 -2.408134e+25  13738.599523 -0.210952  ...   \n",
       "49  6.013856e+13  6.814229e+22 -2.453392e+25  13722.647950 -0.213759  ...   \n",
       "50  5.910807e+13  6.827774e+22 -2.426738e+25  13834.722831 -0.210350  ...   \n",
       "51  6.151702e+13  6.895435e+22 -2.466136e+25  13750.892245 -0.212802  ...   \n",
       "52  6.065092e+13  6.955306e+22 -2.491348e+25  13604.712481 -0.213858  ...   \n",
       "53  6.834156e+13  6.991464e+22 -2.486756e+25  13517.112435 -0.212588  ...   \n",
       "54  6.936537e+13  7.029250e+22 -2.508392e+25  13356.904229 -0.214186  ...   \n",
       "55  6.729846e+13  7.111755e+22 -2.558268e+25  13233.412164 -0.217242  ...   \n",
       "56  6.973227e+13  7.179346e+22 -2.599559e+25  13050.715628 -0.220111  ...   \n",
       "57  7.088334e+13  7.229525e+22 -2.639231e+25  12883.123491 -0.223096  ...   \n",
       "58  6.963535e+13  7.229235e+22 -2.627192e+25  12791.450513 -0.222799  ...   \n",
       "59  7.154282e+13  7.282973e+22 -2.687569e+25  12666.103787 -0.227576  ...   \n",
       "\n",
       "          23        24        25        26   27   28        29        30  \\\n",
       "0   5.414326  5.392323  5.336158  5.205328  0.0  0.0  0.000000  0.000000   \n",
       "1   5.405846  5.403088  5.335880  5.211822  0.0  0.0  0.000000  0.000000   \n",
       "2   5.405772  5.398195  5.345981  5.216945  0.0  0.0  0.000000  0.000039   \n",
       "3   5.396873  5.398732  5.327560  5.203755  0.0  0.0  0.000000  0.000000   \n",
       "4   5.426061  5.389834  5.351214  5.217820  0.0  0.0  0.000000  0.000000   \n",
       "5   5.421926  5.407266  5.350388  5.219855  0.0  0.0  0.000000  0.000000   \n",
       "6   5.419279  5.413495  5.353073  5.232006  0.0  0.0  0.000039  0.000000   \n",
       "7   5.413199  5.418618  5.341884  5.221831  0.0  0.0  0.000000  0.000000   \n",
       "8   5.419249  5.426273  5.354147  5.243087  0.0  0.0  0.000000  0.000000   \n",
       "9   5.427837  5.393532  5.321879  5.221579  0.0  0.0  0.000000  0.000000   \n",
       "10  5.421960  5.419106  5.349870  5.231510  0.0  0.0  0.000000  0.000000   \n",
       "11  5.409650  5.423591  5.336414  5.216673  0.0  0.0  0.000000  0.000000   \n",
       "12  5.404884  5.435943  5.344661  5.215722  0.0  0.0  0.000000  0.000000   \n",
       "13  5.395443  5.426074  5.338376  5.218632  0.0  0.0  0.000000  0.000000   \n",
       "14  5.414608  5.430777  5.334956  5.211968  0.0  0.0  0.000000  0.000000   \n",
       "15  5.414585  5.439978  5.349852  5.199410  0.0  0.0  0.000000  0.000000   \n",
       "16  5.426539  5.439688  5.333861  5.220641  0.0  0.0  0.000000  0.000000   \n",
       "17  5.427973  5.437507  5.321049  5.218753  0.0  0.0  0.000000  0.000000   \n",
       "18  5.420675  5.402765  5.321893  5.207600  0.0  0.0  0.000000  0.000000   \n",
       "19  5.424027  5.401567  5.328385  5.212209  0.0  0.0  0.000000  0.000000   \n",
       "20  5.412794  5.392322  5.331879  5.229455  0.0  0.0  0.000000  0.000000   \n",
       "21  5.407542  5.395212  5.338315  5.199087  0.0  0.0  0.000000  0.000000   \n",
       "22  5.418874  5.404427  5.351370  5.195523  0.0  0.0  0.000000  0.000000   \n",
       "23  5.429408  5.417444  5.352889  5.225211  0.0  0.0  0.000000  0.000000   \n",
       "24  5.429178  5.405270  5.353649  5.195410  0.0  0.0  0.000000  0.000000   \n",
       "25  5.446002  5.412405  5.345017  5.246542  0.0  0.0  0.000000  0.000000   \n",
       "26  5.436352  5.416115  5.330692  5.228028  0.0  0.0  0.000000  0.000000   \n",
       "27  5.449425  5.438875  5.323645  5.244872  0.0  0.0  0.000000  0.000000   \n",
       "28  5.449262  5.424079  5.304066  5.238537  0.0  0.0  0.000000  0.000000   \n",
       "29  5.438638  5.447932  5.311403  5.249751  0.0  0.0  0.000000  0.000000   \n",
       "30  5.415847  5.465051  5.325372  5.225893  0.0  0.0  0.000000  0.000000   \n",
       "31  5.436240  5.471888  5.326894  5.228644  0.0  0.0  0.000000  0.000000   \n",
       "32  5.421760  5.455594  5.327001  5.247861  0.0  0.0  0.000000  0.000000   \n",
       "33  5.416107  5.438976  5.316460  5.241509  0.0  0.0  0.000000  0.000000   \n",
       "34  5.403140  5.454633  5.323734  5.228264  0.0  0.0  0.000000  0.000000   \n",
       "35  5.405974  5.434141  5.301531  5.222481  0.0  0.0  0.000000  0.000000   \n",
       "36  5.412006  5.447493  5.318548  5.231530  0.0  0.0  0.000000  0.000000   \n",
       "37  5.399834  5.472706  5.313747  5.233165  0.0  0.0  0.000000  0.000000   \n",
       "38  5.409866  5.463669  5.298373  5.224789  0.0  0.0  0.000000  0.000000   \n",
       "39  5.431626  5.451033  5.297726  5.220776  0.0  0.0  0.000000  0.000000   \n",
       "40  5.421072  5.471903  5.294902  5.225364  0.0  0.0  0.000000  0.000000   \n",
       "41  5.413351  5.463487  5.295877  5.215056  0.0  0.0  0.000000  0.000000   \n",
       "42  5.418516  5.460616  5.305683  5.225273  0.0  0.0  0.000000  0.000000   \n",
       "43  5.413922  5.463642  5.314288  5.241342  0.0  0.0  0.000000  0.000000   \n",
       "44  5.415397  5.484200  5.318812  5.233028  0.0  0.0  0.000000  0.000000   \n",
       "45  5.423642  5.449615  5.301622  5.208788  0.0  0.0  0.000000  0.000000   \n",
       "46  5.419790  5.491169  5.298181  5.240031  0.0  0.0  0.000000  0.000000   \n",
       "47  5.414254  5.486133  5.296874  5.243491  0.0  0.0  0.000000  0.000000   \n",
       "48  5.408871  5.489321  5.307261  5.219881  0.0  0.0  0.000000  0.000000   \n",
       "49  5.410845  5.464945  5.290419  5.235641  0.0  0.0  0.000000  0.000000   \n",
       "50  5.415777  5.462901  5.289332  5.233490  0.0  0.0  0.000000  0.000000   \n",
       "51  5.407051  5.498585  5.300528  5.232761  0.0  0.0  0.000000  0.000000   \n",
       "52  5.430693  5.476389  5.304190  5.233783  0.0  0.0  0.000000  0.000000   \n",
       "53  5.434391  5.482465  5.301332  5.229522  0.0  0.0  0.000000  0.000000   \n",
       "54  5.425442  5.483249  5.316916  5.207895  0.0  0.0  0.000000  0.000000   \n",
       "55  5.428043  5.480052  5.321115  5.230335  0.0  0.0  0.000000  0.000039   \n",
       "56  5.434801  5.483810  5.317496  5.226047  0.0  0.0  0.000000  0.000000   \n",
       "57  5.433837  5.479156  5.305367  5.208082  0.0  0.0  0.000000  0.000000   \n",
       "58  5.435923  5.495387  5.309339  5.212751  0.0  0.0  0.000000  0.000000   \n",
       "59  5.405683  5.495611  5.322693  5.194471  0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "        31        32  \n",
       "0   1424.0  0.000004  \n",
       "1   1400.0  0.000007  \n",
       "2   1405.0  0.000003  \n",
       "3   1429.0  0.000003  \n",
       "4   1430.0  0.000003  \n",
       "5   1517.0  0.000004  \n",
       "6   1477.0  0.000004  \n",
       "7   1599.0  0.000015  \n",
       "8   1585.0  0.000008  \n",
       "9   1726.0  0.000005  \n",
       "10  1865.0  0.000004  \n",
       "11  1854.0  0.000004  \n",
       "12  1874.0  0.000004  \n",
       "13  1839.0  0.000003  \n",
       "14  1876.0  0.000002  \n",
       "15  1840.0  0.000002  \n",
       "16  1923.0  0.000005  \n",
       "17  1930.0  0.000019  \n",
       "18  1917.0  0.000016  \n",
       "19  1972.0  0.000004  \n",
       "20  2032.0  0.000003  \n",
       "21  2201.0  0.000003  \n",
       "22  2147.0  0.000003  \n",
       "23  2299.0  0.000003  \n",
       "24  2214.0  0.000003  \n",
       "25  2193.0  0.000003  \n",
       "26  2316.0  0.000003  \n",
       "27  2226.0  0.000004  \n",
       "28  2203.0  0.000004  \n",
       "29  2164.0  0.000003  \n",
       "30  2276.0  0.000003  \n",
       "31  2253.0  0.000003  \n",
       "32  2178.0  0.000005  \n",
       "33  2132.0  0.000013  \n",
       "34  2123.0  0.000004  \n",
       "35  2300.0  0.000003  \n",
       "36  2250.0  0.000003  \n",
       "37  2326.0  0.000003  \n",
       "38  2385.0  0.000003  \n",
       "39  2336.0  0.000004  \n",
       "40  2445.0  0.000008  \n",
       "41  2376.0  0.000007  \n",
       "42  2574.0  0.000004  \n",
       "43  2595.0  0.000004  \n",
       "44  2660.0  0.000003  \n",
       "45  2513.0  0.000005  \n",
       "46  2672.0  0.000005  \n",
       "47  2725.0  0.000003  \n",
       "48  2610.0  0.000003  \n",
       "49  2616.0  0.000003  \n",
       "50  2473.0  0.000003  \n",
       "51  2302.0  0.000003  \n",
       "52  2460.0  0.000003  \n",
       "53  2138.0  0.000005  \n",
       "54  2230.0  0.000007  \n",
       "55  2280.0  0.000003  \n",
       "56  2132.0  0.000002  \n",
       "57  2230.0  0.000002  \n",
       "58  2297.0  0.000003  \n",
       "59  2188.0  0.000007  \n",
       "\n",
       "[60 rows x 33 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawdat2.shape:  (52560, 137)\n",
      "num_nodes:  137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data='C:\\\\0.ml\\\\MTGNN\\\\data\\\\solar_AL.txt'\n",
    "#data='C:\\\\0.ml\\\\MTGNN\\\\data\\\\electricity.txt'\n",
    "\n",
    "fin = open(data)\n",
    "rawdat2 = np.loadtxt(fin, delimiter=',')\n",
    "print(\"rawdat2.shape: \",rawdat2.shape)\n",
    "\n",
    "\n",
    "\n",
    "num_nodes= 137 \n",
    "print(\"num_nodes: \",num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled_inputs.shape:  (1540, 33, 60)\n",
      "trainData.shape:  (1540, 60, 33)\n",
      "rawdat2: (60, 27)\n",
      "num_nodes:  33\n",
      "rawdat2: (60, 27)\n",
      "df.shape: (60, 27)\n"
     ]
    }
   ],
   "source": [
    "data=\"C:\\\\0.ml\\\\Sampled_inputs.pck\"\n",
    "\n",
    "Sampled_inputs=load(data)\n",
    "\n",
    "trainData=Sampled_inputs\n",
    "\n",
    "temptrainData=np.empty([1540,60, 33])\n",
    "n=len(trainData)\n",
    "for l in range(0, n):\n",
    "  temp=trainData[l]\n",
    "  #print(temp)\n",
    "  #temp=np.transpose(temp)\n",
    "  temp=temp.T\n",
    "  #print(temp.shape)\n",
    "  #print(temp)\n",
    "  temptrainData[l,:,:]=temp\n",
    "  n=n+1\n",
    "  #np.append(temptrainData, temp)\n",
    "  #print(temptrainData)\n",
    "\n",
    "#print(temptrainData.shape)\n",
    "#print(trainData.shape) \n",
    "trainData=temptrainData\n",
    "print(\"Sampled_inputs.shape: \",Sampled_inputs.shape)\n",
    "print(\"trainData.shape: \",trainData.shape)\n",
    "#print(trainData[0])\n",
    "\n",
    "temp=trainData[0]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "rawdat2=trainData[0]\n",
    "rawdat2 = np.delete(rawdat2, [27,28,29,30,31,32], 1)\n",
    "\n",
    "print(\"rawdat2:\",rawdat2.shape)\n",
    "\n",
    "\n",
    "num_nodes= 33 \n",
    "print(\"num_nodes: \",num_nodes)\n",
    "\n",
    "ttt=trainData[0]\n",
    "\n",
    "#Sampled_inputs.shape[0]\n",
    "for i in range(1,3): \n",
    "    temp=np.concatenate( (temp,trainData[1]  ), axis=0)\n",
    "#print(temp.shape)\n",
    "#rawdat2=temp\n",
    "\n",
    "#rawdat2[rawdat2 == 0] = 1\n",
    "\n",
    "print(\"rawdat2:\",rawdat2.shape)\n",
    "#print(rawdat2)\n",
    "df = pd.DataFrame(rawdat2)\n",
    "#df.replace(0,1,inplace=True)\n",
    "print(\"df.shape:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6748.177901</td>\n",
       "      <td>8.294421e+10</td>\n",
       "      <td>1.591225e+24</td>\n",
       "      <td>1.095967e+14</td>\n",
       "      <td>1510.190735</td>\n",
       "      <td>5.537032e+13</td>\n",
       "      <td>6.524454e+22</td>\n",
       "      <td>-1.945966e+25</td>\n",
       "      <td>13461.392092</td>\n",
       "      <td>-0.176659</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.381126e+24</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>-0.030947</td>\n",
       "      <td>-1.004464e+25</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>5.414326</td>\n",
       "      <td>5.392323</td>\n",
       "      <td>5.336158</td>\n",
       "      <td>5.205328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6713.245495</td>\n",
       "      <td>8.301113e+10</td>\n",
       "      <td>1.591180e+24</td>\n",
       "      <td>1.082126e+14</td>\n",
       "      <td>1552.899870</td>\n",
       "      <td>5.163912e+13</td>\n",
       "      <td>6.552311e+22</td>\n",
       "      <td>-1.967367e+25</td>\n",
       "      <td>13459.043822</td>\n",
       "      <td>-0.178458</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.383287e+24</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>-1.012427e+25</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.091836</td>\n",
       "      <td>5.405846</td>\n",
       "      <td>5.403088</td>\n",
       "      <td>5.335880</td>\n",
       "      <td>5.211822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6711.746592</td>\n",
       "      <td>8.324237e+10</td>\n",
       "      <td>1.585508e+24</td>\n",
       "      <td>1.074454e+14</td>\n",
       "      <td>1569.593570</td>\n",
       "      <td>5.208053e+13</td>\n",
       "      <td>6.582194e+22</td>\n",
       "      <td>-2.022283e+25</td>\n",
       "      <td>13391.359484</td>\n",
       "      <td>-0.182930</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.420609e+24</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>-0.031880</td>\n",
       "      <td>-1.017996e+25</td>\n",
       "      <td>0.039988</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>5.405772</td>\n",
       "      <td>5.398195</td>\n",
       "      <td>5.345981</td>\n",
       "      <td>5.216945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6903.974288</td>\n",
       "      <td>8.343257e+10</td>\n",
       "      <td>1.576556e+24</td>\n",
       "      <td>1.084795e+14</td>\n",
       "      <td>1608.218901</td>\n",
       "      <td>5.166028e+13</td>\n",
       "      <td>6.609184e+22</td>\n",
       "      <td>-2.072753e+25</td>\n",
       "      <td>13248.436229</td>\n",
       "      <td>-0.187068</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.415880e+24</td>\n",
       "      <td>-0.025166</td>\n",
       "      <td>-0.032477</td>\n",
       "      <td>-1.008520e+25</td>\n",
       "      <td>0.039854</td>\n",
       "      <td>0.091020</td>\n",
       "      <td>5.396873</td>\n",
       "      <td>5.398732</td>\n",
       "      <td>5.327560</td>\n",
       "      <td>5.203755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6691.365852</td>\n",
       "      <td>8.370511e+10</td>\n",
       "      <td>1.600757e+24</td>\n",
       "      <td>1.086643e+14</td>\n",
       "      <td>1479.272564</td>\n",
       "      <td>5.030951e+13</td>\n",
       "      <td>6.614713e+22</td>\n",
       "      <td>-1.999304e+25</td>\n",
       "      <td>13367.513454</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.419732e+24</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>-0.029958</td>\n",
       "      <td>-1.013794e+25</td>\n",
       "      <td>0.039759</td>\n",
       "      <td>0.091198</td>\n",
       "      <td>5.426061</td>\n",
       "      <td>5.389834</td>\n",
       "      <td>5.351214</td>\n",
       "      <td>5.217820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6834.958217</td>\n",
       "      <td>8.406158e+10</td>\n",
       "      <td>1.591036e+24</td>\n",
       "      <td>1.096684e+14</td>\n",
       "      <td>1644.488148</td>\n",
       "      <td>5.663543e+13</td>\n",
       "      <td>6.630166e+22</td>\n",
       "      <td>-2.075205e+25</td>\n",
       "      <td>13298.131406</td>\n",
       "      <td>-0.185888</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.460377e+24</td>\n",
       "      <td>-0.045736</td>\n",
       "      <td>-0.032993</td>\n",
       "      <td>-1.014526e+25</td>\n",
       "      <td>0.039954</td>\n",
       "      <td>0.090877</td>\n",
       "      <td>5.421926</td>\n",
       "      <td>5.407266</td>\n",
       "      <td>5.350388</td>\n",
       "      <td>5.219855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6830.348029</td>\n",
       "      <td>8.431275e+10</td>\n",
       "      <td>1.589851e+24</td>\n",
       "      <td>1.083214e+14</td>\n",
       "      <td>1587.280389</td>\n",
       "      <td>5.648788e+13</td>\n",
       "      <td>6.649235e+22</td>\n",
       "      <td>-2.105215e+25</td>\n",
       "      <td>13390.341243</td>\n",
       "      <td>-0.188014</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.441479e+24</td>\n",
       "      <td>-0.083556</td>\n",
       "      <td>-0.031693</td>\n",
       "      <td>-1.019122e+25</td>\n",
       "      <td>0.039666</td>\n",
       "      <td>0.091017</td>\n",
       "      <td>5.419279</td>\n",
       "      <td>5.413495</td>\n",
       "      <td>5.353073</td>\n",
       "      <td>5.232006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6894.216660</td>\n",
       "      <td>8.429531e+10</td>\n",
       "      <td>1.577024e+24</td>\n",
       "      <td>1.090471e+14</td>\n",
       "      <td>1530.971594</td>\n",
       "      <td>5.153441e+13</td>\n",
       "      <td>6.685179e+22</td>\n",
       "      <td>-2.175302e+25</td>\n",
       "      <td>13251.479423</td>\n",
       "      <td>-0.194314</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.376187e+24</td>\n",
       "      <td>-0.028262</td>\n",
       "      <td>-0.030414</td>\n",
       "      <td>-1.001832e+25</td>\n",
       "      <td>0.039091</td>\n",
       "      <td>0.089491</td>\n",
       "      <td>5.413199</td>\n",
       "      <td>5.418618</td>\n",
       "      <td>5.341884</td>\n",
       "      <td>5.221831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6906.079329</td>\n",
       "      <td>8.449927e+10</td>\n",
       "      <td>1.586829e+24</td>\n",
       "      <td>1.088327e+14</td>\n",
       "      <td>1580.642155</td>\n",
       "      <td>5.204076e+13</td>\n",
       "      <td>6.646028e+22</td>\n",
       "      <td>-2.155312e+25</td>\n",
       "      <td>13485.254521</td>\n",
       "      <td>-0.192063</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.374637e+24</td>\n",
       "      <td>-0.070905</td>\n",
       "      <td>-0.031384</td>\n",
       "      <td>-1.007602e+25</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>0.089789</td>\n",
       "      <td>5.419249</td>\n",
       "      <td>5.426273</td>\n",
       "      <td>5.354147</td>\n",
       "      <td>5.243087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6899.094797</td>\n",
       "      <td>8.451908e+10</td>\n",
       "      <td>1.601676e+24</td>\n",
       "      <td>1.091217e+14</td>\n",
       "      <td>1562.139216</td>\n",
       "      <td>5.105149e+13</td>\n",
       "      <td>6.593766e+22</td>\n",
       "      <td>-2.084657e+25</td>\n",
       "      <td>13729.350076</td>\n",
       "      <td>-0.185724</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.178514e+24</td>\n",
       "      <td>-0.088317</td>\n",
       "      <td>-0.031175</td>\n",
       "      <td>-1.025307e+25</td>\n",
       "      <td>0.037227</td>\n",
       "      <td>0.091345</td>\n",
       "      <td>5.427837</td>\n",
       "      <td>5.393532</td>\n",
       "      <td>5.321879</td>\n",
       "      <td>5.221579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6911.805621</td>\n",
       "      <td>8.453654e+10</td>\n",
       "      <td>1.584634e+24</td>\n",
       "      <td>1.085458e+14</td>\n",
       "      <td>1608.232234</td>\n",
       "      <td>5.032479e+13</td>\n",
       "      <td>6.626648e+22</td>\n",
       "      <td>-2.168199e+25</td>\n",
       "      <td>13676.366262</td>\n",
       "      <td>-0.193127</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.271062e+24</td>\n",
       "      <td>-0.098197</td>\n",
       "      <td>-0.031890</td>\n",
       "      <td>-1.019758e+25</td>\n",
       "      <td>0.038043</td>\n",
       "      <td>0.090832</td>\n",
       "      <td>5.421960</td>\n",
       "      <td>5.419106</td>\n",
       "      <td>5.349870</td>\n",
       "      <td>5.231510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6946.807156</td>\n",
       "      <td>8.448848e+10</td>\n",
       "      <td>1.588865e+24</td>\n",
       "      <td>1.085662e+14</td>\n",
       "      <td>1598.537641</td>\n",
       "      <td>4.720406e+13</td>\n",
       "      <td>6.604935e+22</td>\n",
       "      <td>-2.160836e+25</td>\n",
       "      <td>13798.923015</td>\n",
       "      <td>-0.192580</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.247994e+24</td>\n",
       "      <td>-0.075577</td>\n",
       "      <td>-0.031730</td>\n",
       "      <td>-1.013852e+25</td>\n",
       "      <td>0.037859</td>\n",
       "      <td>0.090358</td>\n",
       "      <td>5.409650</td>\n",
       "      <td>5.423591</td>\n",
       "      <td>5.336414</td>\n",
       "      <td>5.216673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6886.818627</td>\n",
       "      <td>8.431700e+10</td>\n",
       "      <td>1.578638e+24</td>\n",
       "      <td>1.071168e+14</td>\n",
       "      <td>1581.500858</td>\n",
       "      <td>4.759987e+13</td>\n",
       "      <td>6.570224e+22</td>\n",
       "      <td>-2.183161e+25</td>\n",
       "      <td>13867.168557</td>\n",
       "      <td>-0.194966</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.178460e+24</td>\n",
       "      <td>-0.092255</td>\n",
       "      <td>-0.031394</td>\n",
       "      <td>-1.018455e+25</td>\n",
       "      <td>0.037315</td>\n",
       "      <td>0.090952</td>\n",
       "      <td>5.404884</td>\n",
       "      <td>5.435943</td>\n",
       "      <td>5.344661</td>\n",
       "      <td>5.215722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6818.410837</td>\n",
       "      <td>8.425498e+10</td>\n",
       "      <td>1.577545e+24</td>\n",
       "      <td>1.059483e+14</td>\n",
       "      <td>1552.157851</td>\n",
       "      <td>4.509200e+13</td>\n",
       "      <td>6.554720e+22</td>\n",
       "      <td>-2.179559e+25</td>\n",
       "      <td>14013.525138</td>\n",
       "      <td>-0.194787</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.113676e+24</td>\n",
       "      <td>-0.112595</td>\n",
       "      <td>-0.030838</td>\n",
       "      <td>-1.014934e+25</td>\n",
       "      <td>0.036764</td>\n",
       "      <td>0.090705</td>\n",
       "      <td>5.395443</td>\n",
       "      <td>5.426074</td>\n",
       "      <td>5.338376</td>\n",
       "      <td>5.218632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6837.526272</td>\n",
       "      <td>8.424089e+10</td>\n",
       "      <td>1.577035e+24</td>\n",
       "      <td>1.051378e+14</td>\n",
       "      <td>1623.889248</td>\n",
       "      <td>4.995272e+13</td>\n",
       "      <td>6.540712e+22</td>\n",
       "      <td>-2.187353e+25</td>\n",
       "      <td>14092.449955</td>\n",
       "      <td>-0.195516</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.063629e+24</td>\n",
       "      <td>-0.132570</td>\n",
       "      <td>-0.032248</td>\n",
       "      <td>-1.020192e+25</td>\n",
       "      <td>0.036323</td>\n",
       "      <td>0.091190</td>\n",
       "      <td>5.414608</td>\n",
       "      <td>5.430777</td>\n",
       "      <td>5.334956</td>\n",
       "      <td>5.211968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6813.759545</td>\n",
       "      <td>8.408494e+10</td>\n",
       "      <td>1.570983e+24</td>\n",
       "      <td>1.053031e+14</td>\n",
       "      <td>1530.518892</td>\n",
       "      <td>4.714137e+13</td>\n",
       "      <td>6.541886e+22</td>\n",
       "      <td>-2.204741e+25</td>\n",
       "      <td>14125.375441</td>\n",
       "      <td>-0.197436</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.051511e+24</td>\n",
       "      <td>-0.110612</td>\n",
       "      <td>-0.030402</td>\n",
       "      <td>-1.014000e+25</td>\n",
       "      <td>0.036282</td>\n",
       "      <td>0.090804</td>\n",
       "      <td>5.414585</td>\n",
       "      <td>5.439978</td>\n",
       "      <td>5.349852</td>\n",
       "      <td>5.199410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6810.946741</td>\n",
       "      <td>8.387123e+10</td>\n",
       "      <td>1.559647e+24</td>\n",
       "      <td>1.054864e+14</td>\n",
       "      <td>1553.297953</td>\n",
       "      <td>5.192169e+13</td>\n",
       "      <td>6.534533e+22</td>\n",
       "      <td>-2.227891e+25</td>\n",
       "      <td>14030.816852</td>\n",
       "      <td>-0.200018</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.953202e+24</td>\n",
       "      <td>-0.149886</td>\n",
       "      <td>-0.030867</td>\n",
       "      <td>-1.008534e+25</td>\n",
       "      <td>0.035491</td>\n",
       "      <td>0.090545</td>\n",
       "      <td>5.426539</td>\n",
       "      <td>5.439688</td>\n",
       "      <td>5.333861</td>\n",
       "      <td>5.220641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6750.173733</td>\n",
       "      <td>8.393793e+10</td>\n",
       "      <td>1.567801e+24</td>\n",
       "      <td>1.044780e+14</td>\n",
       "      <td>1529.499791</td>\n",
       "      <td>5.263473e+13</td>\n",
       "      <td>6.513396e+22</td>\n",
       "      <td>-2.198664e+25</td>\n",
       "      <td>14122.395674</td>\n",
       "      <td>-0.197237</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.900017e+24</td>\n",
       "      <td>-0.157156</td>\n",
       "      <td>-0.030440</td>\n",
       "      <td>-1.010515e+25</td>\n",
       "      <td>0.034986</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>5.427973</td>\n",
       "      <td>5.437507</td>\n",
       "      <td>5.321049</td>\n",
       "      <td>5.218753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6625.715466</td>\n",
       "      <td>8.361261e+10</td>\n",
       "      <td>1.570021e+24</td>\n",
       "      <td>1.039780e+14</td>\n",
       "      <td>1558.476742</td>\n",
       "      <td>5.476180e+13</td>\n",
       "      <td>6.486121e+22</td>\n",
       "      <td>-2.151174e+25</td>\n",
       "      <td>14176.476618</td>\n",
       "      <td>-0.193727</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.858987e+24</td>\n",
       "      <td>-0.153253</td>\n",
       "      <td>-0.031229</td>\n",
       "      <td>-1.020425e+25</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>0.091896</td>\n",
       "      <td>5.420675</td>\n",
       "      <td>5.402765</td>\n",
       "      <td>5.321893</td>\n",
       "      <td>5.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6602.456377</td>\n",
       "      <td>8.343825e+10</td>\n",
       "      <td>1.559968e+24</td>\n",
       "      <td>1.037375e+14</td>\n",
       "      <td>1475.698999</td>\n",
       "      <td>5.215698e+13</td>\n",
       "      <td>6.508146e+22</td>\n",
       "      <td>-2.175203e+25</td>\n",
       "      <td>13994.417647</td>\n",
       "      <td>-0.196301</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.685400e+24</td>\n",
       "      <td>-0.160772</td>\n",
       "      <td>-0.029572</td>\n",
       "      <td>-1.015592e+25</td>\n",
       "      <td>0.033259</td>\n",
       "      <td>0.091652</td>\n",
       "      <td>5.424027</td>\n",
       "      <td>5.401567</td>\n",
       "      <td>5.328385</td>\n",
       "      <td>5.212209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6579.279348</td>\n",
       "      <td>8.332854e+10</td>\n",
       "      <td>1.556961e+24</td>\n",
       "      <td>1.027367e+14</td>\n",
       "      <td>1497.390470</td>\n",
       "      <td>5.335861e+13</td>\n",
       "      <td>6.486579e+22</td>\n",
       "      <td>-2.183151e+25</td>\n",
       "      <td>14085.404680</td>\n",
       "      <td>-0.197277</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.693101e+24</td>\n",
       "      <td>-0.148305</td>\n",
       "      <td>-0.030018</td>\n",
       "      <td>-1.013598e+25</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.091592</td>\n",
       "      <td>5.412794</td>\n",
       "      <td>5.392322</td>\n",
       "      <td>5.331879</td>\n",
       "      <td>5.229455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6659.340976</td>\n",
       "      <td>8.366482e+10</td>\n",
       "      <td>1.561306e+24</td>\n",
       "      <td>1.038385e+14</td>\n",
       "      <td>1595.699476</td>\n",
       "      <td>5.790126e+13</td>\n",
       "      <td>6.530352e+22</td>\n",
       "      <td>-2.204153e+25</td>\n",
       "      <td>13978.934717</td>\n",
       "      <td>-0.198375</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.754855e+24</td>\n",
       "      <td>-0.146121</td>\n",
       "      <td>-0.031831</td>\n",
       "      <td>-1.018197e+25</td>\n",
       "      <td>0.033794</td>\n",
       "      <td>0.091638</td>\n",
       "      <td>5.407542</td>\n",
       "      <td>5.395212</td>\n",
       "      <td>5.338315</td>\n",
       "      <td>5.199087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6674.435798</td>\n",
       "      <td>8.363221e+10</td>\n",
       "      <td>1.558285e+24</td>\n",
       "      <td>1.049437e+14</td>\n",
       "      <td>1561.801278</td>\n",
       "      <td>5.628489e+13</td>\n",
       "      <td>6.543992e+22</td>\n",
       "      <td>-2.211603e+25</td>\n",
       "      <td>13883.545665</td>\n",
       "      <td>-0.199123</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.787351e+24</td>\n",
       "      <td>-0.184686</td>\n",
       "      <td>-0.031147</td>\n",
       "      <td>-1.019079e+25</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.091753</td>\n",
       "      <td>5.418874</td>\n",
       "      <td>5.404427</td>\n",
       "      <td>5.351370</td>\n",
       "      <td>5.195523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6628.940407</td>\n",
       "      <td>8.364418e+10</td>\n",
       "      <td>1.560219e+24</td>\n",
       "      <td>1.037925e+14</td>\n",
       "      <td>1558.677333</td>\n",
       "      <td>5.651785e+13</td>\n",
       "      <td>6.543197e+22</td>\n",
       "      <td>-2.211868e+25</td>\n",
       "      <td>13923.843184</td>\n",
       "      <td>-0.199118</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.808673e+24</td>\n",
       "      <td>-0.203930</td>\n",
       "      <td>-0.031081</td>\n",
       "      <td>-1.023279e+25</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>0.092118</td>\n",
       "      <td>5.429408</td>\n",
       "      <td>5.417444</td>\n",
       "      <td>5.352889</td>\n",
       "      <td>5.225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6534.272066</td>\n",
       "      <td>8.347730e+10</td>\n",
       "      <td>1.564493e+24</td>\n",
       "      <td>1.040900e+14</td>\n",
       "      <td>1475.048207</td>\n",
       "      <td>5.209285e+13</td>\n",
       "      <td>6.540494e+22</td>\n",
       "      <td>-2.160368e+25</td>\n",
       "      <td>13833.285734</td>\n",
       "      <td>-0.194871</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.857863e+24</td>\n",
       "      <td>-0.124318</td>\n",
       "      <td>-0.029577</td>\n",
       "      <td>-1.013414e+25</td>\n",
       "      <td>0.034799</td>\n",
       "      <td>0.091413</td>\n",
       "      <td>5.429178</td>\n",
       "      <td>5.405270</td>\n",
       "      <td>5.353649</td>\n",
       "      <td>5.195410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6618.672577</td>\n",
       "      <td>8.346451e+10</td>\n",
       "      <td>1.557323e+24</td>\n",
       "      <td>1.033482e+14</td>\n",
       "      <td>1600.107458</td>\n",
       "      <td>5.740748e+13</td>\n",
       "      <td>6.560208e+22</td>\n",
       "      <td>-2.194959e+25</td>\n",
       "      <td>13785.588862</td>\n",
       "      <td>-0.198021</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.822330e+24</td>\n",
       "      <td>-0.177619</td>\n",
       "      <td>-0.032005</td>\n",
       "      <td>-1.033253e+25</td>\n",
       "      <td>0.034484</td>\n",
       "      <td>0.093216</td>\n",
       "      <td>5.446002</td>\n",
       "      <td>5.412405</td>\n",
       "      <td>5.345017</td>\n",
       "      <td>5.246542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6556.497525</td>\n",
       "      <td>8.331999e+10</td>\n",
       "      <td>1.555459e+24</td>\n",
       "      <td>1.036262e+14</td>\n",
       "      <td>1582.988346</td>\n",
       "      <td>5.571156e+13</td>\n",
       "      <td>6.565363e+22</td>\n",
       "      <td>-2.201092e+25</td>\n",
       "      <td>13704.966138</td>\n",
       "      <td>-0.198919</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.714714e+24</td>\n",
       "      <td>-0.135121</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-1.019728e+25</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.092156</td>\n",
       "      <td>5.436352</td>\n",
       "      <td>5.416115</td>\n",
       "      <td>5.330692</td>\n",
       "      <td>5.228028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6664.787875</td>\n",
       "      <td>8.381583e+10</td>\n",
       "      <td>1.554123e+24</td>\n",
       "      <td>1.049621e+14</td>\n",
       "      <td>1607.631910</td>\n",
       "      <td>5.965254e+13</td>\n",
       "      <td>6.624869e+22</td>\n",
       "      <td>-2.268300e+25</td>\n",
       "      <td>13567.615361</td>\n",
       "      <td>-0.203780</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.679195e+24</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>-0.031868</td>\n",
       "      <td>-1.023184e+25</td>\n",
       "      <td>0.033053</td>\n",
       "      <td>0.091921</td>\n",
       "      <td>5.449425</td>\n",
       "      <td>5.438875</td>\n",
       "      <td>5.323645</td>\n",
       "      <td>5.244872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6610.832931</td>\n",
       "      <td>8.372099e+10</td>\n",
       "      <td>1.551750e+24</td>\n",
       "      <td>1.045356e+14</td>\n",
       "      <td>1628.840026</td>\n",
       "      <td>5.812513e+13</td>\n",
       "      <td>6.629692e+22</td>\n",
       "      <td>-2.266189e+25</td>\n",
       "      <td>13518.841224</td>\n",
       "      <td>-0.203821</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.683323e+24</td>\n",
       "      <td>-0.133163</td>\n",
       "      <td>-0.032324</td>\n",
       "      <td>-1.028509e+25</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.092504</td>\n",
       "      <td>5.449262</td>\n",
       "      <td>5.424079</td>\n",
       "      <td>5.304066</td>\n",
       "      <td>5.238537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6623.903993</td>\n",
       "      <td>8.393815e+10</td>\n",
       "      <td>1.547466e+24</td>\n",
       "      <td>1.046448e+14</td>\n",
       "      <td>1681.735488</td>\n",
       "      <td>6.043938e+13</td>\n",
       "      <td>6.663268e+22</td>\n",
       "      <td>-2.302672e+25</td>\n",
       "      <td>13449.626827</td>\n",
       "      <td>-0.206567</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.588994e+24</td>\n",
       "      <td>-0.113793</td>\n",
       "      <td>-0.033211</td>\n",
       "      <td>-1.039490e+25</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>0.093250</td>\n",
       "      <td>5.438638</td>\n",
       "      <td>5.447932</td>\n",
       "      <td>5.311403</td>\n",
       "      <td>5.249751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6604.636688</td>\n",
       "      <td>8.384039e+10</td>\n",
       "      <td>1.543670e+24</td>\n",
       "      <td>1.042238e+14</td>\n",
       "      <td>1658.377172</td>\n",
       "      <td>5.433214e+13</td>\n",
       "      <td>6.649111e+22</td>\n",
       "      <td>-2.306179e+25</td>\n",
       "      <td>13492.628976</td>\n",
       "      <td>-0.207122</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.531302e+24</td>\n",
       "      <td>-0.126929</td>\n",
       "      <td>-0.032772</td>\n",
       "      <td>-1.040647e+25</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.093462</td>\n",
       "      <td>5.415847</td>\n",
       "      <td>5.465051</td>\n",
       "      <td>5.325372</td>\n",
       "      <td>5.225893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6590.454932</td>\n",
       "      <td>8.376975e+10</td>\n",
       "      <td>1.544275e+24</td>\n",
       "      <td>1.037572e+14</td>\n",
       "      <td>1656.194789</td>\n",
       "      <td>5.797011e+13</td>\n",
       "      <td>6.652554e+22</td>\n",
       "      <td>-2.285079e+25</td>\n",
       "      <td>13423.901150</td>\n",
       "      <td>-0.205400</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.656476e+24</td>\n",
       "      <td>-0.159487</td>\n",
       "      <td>-0.032804</td>\n",
       "      <td>-1.042174e+25</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>5.436240</td>\n",
       "      <td>5.471888</td>\n",
       "      <td>5.326894</td>\n",
       "      <td>5.228644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6554.577321</td>\n",
       "      <td>8.382140e+10</td>\n",
       "      <td>1.551716e+24</td>\n",
       "      <td>1.037426e+14</td>\n",
       "      <td>1666.964946</td>\n",
       "      <td>5.595178e+13</td>\n",
       "      <td>6.659935e+22</td>\n",
       "      <td>-2.270023e+25</td>\n",
       "      <td>13490.144006</td>\n",
       "      <td>-0.203921</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.703933e+24</td>\n",
       "      <td>-0.106630</td>\n",
       "      <td>-0.033040</td>\n",
       "      <td>-1.053608e+25</td>\n",
       "      <td>0.033273</td>\n",
       "      <td>0.094648</td>\n",
       "      <td>5.421760</td>\n",
       "      <td>5.455594</td>\n",
       "      <td>5.327001</td>\n",
       "      <td>5.247861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6589.028665</td>\n",
       "      <td>8.362348e+10</td>\n",
       "      <td>1.548201e+24</td>\n",
       "      <td>1.047694e+14</td>\n",
       "      <td>1718.904126</td>\n",
       "      <td>5.865956e+13</td>\n",
       "      <td>6.661440e+22</td>\n",
       "      <td>-2.252770e+25</td>\n",
       "      <td>13382.792443</td>\n",
       "      <td>-0.202850</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.786542e+24</td>\n",
       "      <td>-0.137219</td>\n",
       "      <td>-0.034181</td>\n",
       "      <td>-1.054248e+25</td>\n",
       "      <td>0.034096</td>\n",
       "      <td>0.094930</td>\n",
       "      <td>5.416107</td>\n",
       "      <td>5.438976</td>\n",
       "      <td>5.316460</td>\n",
       "      <td>5.241509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6696.230175</td>\n",
       "      <td>8.374229e+10</td>\n",
       "      <td>1.550484e+24</td>\n",
       "      <td>1.050534e+14</td>\n",
       "      <td>1780.950881</td>\n",
       "      <td>6.372367e+13</td>\n",
       "      <td>6.670024e+22</td>\n",
       "      <td>-2.260599e+25</td>\n",
       "      <td>13434.294831</td>\n",
       "      <td>-0.203267</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.711333e+24</td>\n",
       "      <td>-0.116468</td>\n",
       "      <td>-0.035352</td>\n",
       "      <td>-1.044080e+25</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>5.403140</td>\n",
       "      <td>5.454633</td>\n",
       "      <td>5.323734</td>\n",
       "      <td>5.228264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6680.609878</td>\n",
       "      <td>8.378515e+10</td>\n",
       "      <td>1.540382e+24</td>\n",
       "      <td>1.042639e+14</td>\n",
       "      <td>1790.150343</td>\n",
       "      <td>6.220128e+13</td>\n",
       "      <td>6.695897e+22</td>\n",
       "      <td>-2.314251e+25</td>\n",
       "      <td>13321.778770</td>\n",
       "      <td>-0.207984</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.540336e+24</td>\n",
       "      <td>-0.118908</td>\n",
       "      <td>-0.035375</td>\n",
       "      <td>-1.050380e+25</td>\n",
       "      <td>0.031817</td>\n",
       "      <td>0.094399</td>\n",
       "      <td>5.405974</td>\n",
       "      <td>5.434141</td>\n",
       "      <td>5.301531</td>\n",
       "      <td>5.222481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6655.976645</td>\n",
       "      <td>8.391120e+10</td>\n",
       "      <td>1.542652e+24</td>\n",
       "      <td>1.037527e+14</td>\n",
       "      <td>1774.532620</td>\n",
       "      <td>6.236907e+13</td>\n",
       "      <td>6.709688e+22</td>\n",
       "      <td>-2.312158e+25</td>\n",
       "      <td>13351.836402</td>\n",
       "      <td>-0.207484</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.532473e+24</td>\n",
       "      <td>-0.131462</td>\n",
       "      <td>-0.035028</td>\n",
       "      <td>-1.053937e+25</td>\n",
       "      <td>0.031699</td>\n",
       "      <td>0.094576</td>\n",
       "      <td>5.412006</td>\n",
       "      <td>5.447493</td>\n",
       "      <td>5.318548</td>\n",
       "      <td>5.231530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6621.583380</td>\n",
       "      <td>8.412847e+10</td>\n",
       "      <td>1.550473e+24</td>\n",
       "      <td>1.048586e+14</td>\n",
       "      <td>1828.370218</td>\n",
       "      <td>6.313825e+13</td>\n",
       "      <td>6.710731e+22</td>\n",
       "      <td>-2.288411e+25</td>\n",
       "      <td>13328.826569</td>\n",
       "      <td>-0.204823</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.591288e+24</td>\n",
       "      <td>-0.119938</td>\n",
       "      <td>-0.036077</td>\n",
       "      <td>-1.056423e+25</td>\n",
       "      <td>0.032144</td>\n",
       "      <td>0.094554</td>\n",
       "      <td>5.399834</td>\n",
       "      <td>5.472706</td>\n",
       "      <td>5.313747</td>\n",
       "      <td>5.233165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6714.414943</td>\n",
       "      <td>8.417939e+10</td>\n",
       "      <td>1.533120e+24</td>\n",
       "      <td>1.041958e+14</td>\n",
       "      <td>1799.467489</td>\n",
       "      <td>6.292113e+13</td>\n",
       "      <td>6.729763e+22</td>\n",
       "      <td>-2.367600e+25</td>\n",
       "      <td>13291.640048</td>\n",
       "      <td>-0.211782</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.548269e+24</td>\n",
       "      <td>-0.118411</td>\n",
       "      <td>-0.035281</td>\n",
       "      <td>-1.049419e+25</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.093871</td>\n",
       "      <td>5.409866</td>\n",
       "      <td>5.463669</td>\n",
       "      <td>5.298373</td>\n",
       "      <td>5.224789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6713.854682</td>\n",
       "      <td>8.428436e+10</td>\n",
       "      <td>1.542590e+24</td>\n",
       "      <td>1.039199e+14</td>\n",
       "      <td>1784.416135</td>\n",
       "      <td>6.210627e+13</td>\n",
       "      <td>6.705084e+22</td>\n",
       "      <td>-2.349227e+25</td>\n",
       "      <td>13439.508935</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.586477e+24</td>\n",
       "      <td>-0.161360</td>\n",
       "      <td>-0.034997</td>\n",
       "      <td>-1.050022e+25</td>\n",
       "      <td>0.032041</td>\n",
       "      <td>0.093808</td>\n",
       "      <td>5.431626</td>\n",
       "      <td>5.451033</td>\n",
       "      <td>5.297726</td>\n",
       "      <td>5.220776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6675.049946</td>\n",
       "      <td>8.433016e+10</td>\n",
       "      <td>1.538034e+24</td>\n",
       "      <td>1.040402e+14</td>\n",
       "      <td>1819.143184</td>\n",
       "      <td>6.488646e+13</td>\n",
       "      <td>6.720600e+22</td>\n",
       "      <td>-2.352803e+25</td>\n",
       "      <td>13388.201074</td>\n",
       "      <td>-0.210083</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.505879e+24</td>\n",
       "      <td>-0.195021</td>\n",
       "      <td>-0.035655</td>\n",
       "      <td>-1.028755e+25</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>5.421072</td>\n",
       "      <td>5.471903</td>\n",
       "      <td>5.294902</td>\n",
       "      <td>5.225364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6651.842005</td>\n",
       "      <td>8.457251e+10</td>\n",
       "      <td>1.546075e+24</td>\n",
       "      <td>1.035750e+14</td>\n",
       "      <td>1821.234321</td>\n",
       "      <td>6.246896e+13</td>\n",
       "      <td>6.683234e+22</td>\n",
       "      <td>-2.344465e+25</td>\n",
       "      <td>13559.769347</td>\n",
       "      <td>-0.208738</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.616186e+24</td>\n",
       "      <td>-0.196562</td>\n",
       "      <td>-0.035635</td>\n",
       "      <td>-1.028820e+25</td>\n",
       "      <td>0.032197</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>5.413351</td>\n",
       "      <td>5.463487</td>\n",
       "      <td>5.295877</td>\n",
       "      <td>5.215056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6618.690252</td>\n",
       "      <td>8.479324e+10</td>\n",
       "      <td>1.556837e+24</td>\n",
       "      <td>1.026949e+14</td>\n",
       "      <td>1879.972741</td>\n",
       "      <td>6.718395e+13</td>\n",
       "      <td>6.662508e+22</td>\n",
       "      <td>-2.320355e+25</td>\n",
       "      <td>13744.924963</td>\n",
       "      <td>-0.206054</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.491040e+24</td>\n",
       "      <td>-0.234025</td>\n",
       "      <td>-0.036759</td>\n",
       "      <td>-1.037269e+25</td>\n",
       "      <td>0.031001</td>\n",
       "      <td>0.092112</td>\n",
       "      <td>5.418516</td>\n",
       "      <td>5.460616</td>\n",
       "      <td>5.305683</td>\n",
       "      <td>5.225273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6689.582061</td>\n",
       "      <td>8.511911e+10</td>\n",
       "      <td>1.554712e+24</td>\n",
       "      <td>1.029952e+14</td>\n",
       "      <td>1920.438043</td>\n",
       "      <td>6.659065e+13</td>\n",
       "      <td>6.703677e+22</td>\n",
       "      <td>-2.359518e+25</td>\n",
       "      <td>13709.773165</td>\n",
       "      <td>-0.208729</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.443014e+24</td>\n",
       "      <td>-0.210064</td>\n",
       "      <td>-0.037330</td>\n",
       "      <td>-1.024041e+25</td>\n",
       "      <td>0.030458</td>\n",
       "      <td>0.090589</td>\n",
       "      <td>5.413922</td>\n",
       "      <td>5.463642</td>\n",
       "      <td>5.314288</td>\n",
       "      <td>5.241342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6651.957058</td>\n",
       "      <td>8.528108e+10</td>\n",
       "      <td>1.556176e+24</td>\n",
       "      <td>1.023277e+14</td>\n",
       "      <td>1799.937489</td>\n",
       "      <td>6.106326e+13</td>\n",
       "      <td>6.719677e+22</td>\n",
       "      <td>-2.369129e+25</td>\n",
       "      <td>13673.206273</td>\n",
       "      <td>-0.209182</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.498174e+24</td>\n",
       "      <td>-0.213750</td>\n",
       "      <td>-0.034908</td>\n",
       "      <td>-1.026132e+25</td>\n",
       "      <td>0.030887</td>\n",
       "      <td>0.090602</td>\n",
       "      <td>5.415397</td>\n",
       "      <td>5.484200</td>\n",
       "      <td>5.318812</td>\n",
       "      <td>5.233028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6729.800739</td>\n",
       "      <td>8.549333e+10</td>\n",
       "      <td>1.566051e+24</td>\n",
       "      <td>1.030126e+14</td>\n",
       "      <td>1810.811383</td>\n",
       "      <td>6.165691e+13</td>\n",
       "      <td>6.719766e+22</td>\n",
       "      <td>-2.351989e+25</td>\n",
       "      <td>13800.878660</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.504991e+24</td>\n",
       "      <td>-0.210466</td>\n",
       "      <td>-0.035094</td>\n",
       "      <td>-1.041971e+25</td>\n",
       "      <td>0.030870</td>\n",
       "      <td>0.091772</td>\n",
       "      <td>5.423642</td>\n",
       "      <td>5.449615</td>\n",
       "      <td>5.301622</td>\n",
       "      <td>5.208788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6749.666713</td>\n",
       "      <td>8.547887e+10</td>\n",
       "      <td>1.565630e+24</td>\n",
       "      <td>1.027505e+14</td>\n",
       "      <td>1816.423463</td>\n",
       "      <td>6.445213e+13</td>\n",
       "      <td>6.702058e+22</td>\n",
       "      <td>-2.357341e+25</td>\n",
       "      <td>13929.375838</td>\n",
       "      <td>-0.207659</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.633299e+24</td>\n",
       "      <td>-0.188200</td>\n",
       "      <td>-0.035193</td>\n",
       "      <td>-1.041887e+25</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.091780</td>\n",
       "      <td>5.419790</td>\n",
       "      <td>5.491169</td>\n",
       "      <td>5.298181</td>\n",
       "      <td>5.240031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6712.981030</td>\n",
       "      <td>8.579096e+10</td>\n",
       "      <td>1.570802e+24</td>\n",
       "      <td>1.032009e+14</td>\n",
       "      <td>1875.490188</td>\n",
       "      <td>6.395814e+13</td>\n",
       "      <td>6.737582e+22</td>\n",
       "      <td>-2.368335e+25</td>\n",
       "      <td>13878.799260</td>\n",
       "      <td>-0.207869</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.617056e+24</td>\n",
       "      <td>-0.173928</td>\n",
       "      <td>-0.036203</td>\n",
       "      <td>-1.056949e+25</td>\n",
       "      <td>0.031747</td>\n",
       "      <td>0.092768</td>\n",
       "      <td>5.414254</td>\n",
       "      <td>5.486133</td>\n",
       "      <td>5.296874</td>\n",
       "      <td>5.243491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6694.946004</td>\n",
       "      <td>8.595762e+10</td>\n",
       "      <td>1.565608e+24</td>\n",
       "      <td>1.032871e+14</td>\n",
       "      <td>1714.993210</td>\n",
       "      <td>5.665472e+13</td>\n",
       "      <td>6.773257e+22</td>\n",
       "      <td>-2.408134e+25</td>\n",
       "      <td>13738.599523</td>\n",
       "      <td>-0.210952</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.706007e+24</td>\n",
       "      <td>-0.163303</td>\n",
       "      <td>-0.032949</td>\n",
       "      <td>-1.046147e+25</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>0.091642</td>\n",
       "      <td>5.408871</td>\n",
       "      <td>5.489321</td>\n",
       "      <td>5.307261</td>\n",
       "      <td>5.219881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6720.925432</td>\n",
       "      <td>8.642296e+10</td>\n",
       "      <td>1.568783e+24</td>\n",
       "      <td>1.033549e+14</td>\n",
       "      <td>1788.925676</td>\n",
       "      <td>6.013856e+13</td>\n",
       "      <td>6.814229e+22</td>\n",
       "      <td>-2.453392e+25</td>\n",
       "      <td>13722.647950</td>\n",
       "      <td>-0.213759</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.853578e+24</td>\n",
       "      <td>-0.175096</td>\n",
       "      <td>-0.034101</td>\n",
       "      <td>-1.054021e+25</td>\n",
       "      <td>0.033575</td>\n",
       "      <td>0.091835</td>\n",
       "      <td>5.410845</td>\n",
       "      <td>5.464945</td>\n",
       "      <td>5.290419</td>\n",
       "      <td>5.235641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6643.833325</td>\n",
       "      <td>8.686979e+10</td>\n",
       "      <td>1.585399e+24</td>\n",
       "      <td>1.027543e+14</td>\n",
       "      <td>1811.564073</td>\n",
       "      <td>5.910807e+13</td>\n",
       "      <td>6.827774e+22</td>\n",
       "      <td>-2.426738e+25</td>\n",
       "      <td>13834.722831</td>\n",
       "      <td>-0.210350</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.779251e+24</td>\n",
       "      <td>-0.160521</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>-1.067594e+25</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.092539</td>\n",
       "      <td>5.415777</td>\n",
       "      <td>5.462901</td>\n",
       "      <td>5.289332</td>\n",
       "      <td>5.233490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6678.435053</td>\n",
       "      <td>8.726270e+10</td>\n",
       "      <td>1.588082e+24</td>\n",
       "      <td>1.029321e+14</td>\n",
       "      <td>1888.605961</td>\n",
       "      <td>6.151702e+13</td>\n",
       "      <td>6.895435e+22</td>\n",
       "      <td>-2.466136e+25</td>\n",
       "      <td>13750.892245</td>\n",
       "      <td>-0.212802</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.799170e+24</td>\n",
       "      <td>-0.167646</td>\n",
       "      <td>-0.035689</td>\n",
       "      <td>-1.070670e+25</td>\n",
       "      <td>0.032783</td>\n",
       "      <td>0.092388</td>\n",
       "      <td>5.407051</td>\n",
       "      <td>5.498585</td>\n",
       "      <td>5.300528</td>\n",
       "      <td>5.232761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6719.481170</td>\n",
       "      <td>8.771953e+10</td>\n",
       "      <td>1.595610e+24</td>\n",
       "      <td>1.032735e+14</td>\n",
       "      <td>1839.960188</td>\n",
       "      <td>6.065092e+13</td>\n",
       "      <td>6.955306e+22</td>\n",
       "      <td>-2.491348e+25</td>\n",
       "      <td>13604.712481</td>\n",
       "      <td>-0.213858</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.781941e+24</td>\n",
       "      <td>-0.140456</td>\n",
       "      <td>-0.034560</td>\n",
       "      <td>-1.074795e+25</td>\n",
       "      <td>0.032464</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>5.430693</td>\n",
       "      <td>5.476389</td>\n",
       "      <td>5.304190</td>\n",
       "      <td>5.233783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6731.164211</td>\n",
       "      <td>8.808078e+10</td>\n",
       "      <td>1.601331e+24</td>\n",
       "      <td>1.040559e+14</td>\n",
       "      <td>1870.737825</td>\n",
       "      <td>6.834156e+13</td>\n",
       "      <td>6.991464e+22</td>\n",
       "      <td>-2.486756e+25</td>\n",
       "      <td>13517.112435</td>\n",
       "      <td>-0.212588</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.795374e+24</td>\n",
       "      <td>-0.160903</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-1.081751e+25</td>\n",
       "      <td>0.032446</td>\n",
       "      <td>0.092477</td>\n",
       "      <td>5.434391</td>\n",
       "      <td>5.482465</td>\n",
       "      <td>5.301332</td>\n",
       "      <td>5.229522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6743.632587</td>\n",
       "      <td>8.818426e+10</td>\n",
       "      <td>1.600107e+24</td>\n",
       "      <td>1.048489e+14</td>\n",
       "      <td>1870.199589</td>\n",
       "      <td>6.936537e+13</td>\n",
       "      <td>7.029250e+22</td>\n",
       "      <td>-2.508392e+25</td>\n",
       "      <td>13356.904229</td>\n",
       "      <td>-0.214186</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.856373e+24</td>\n",
       "      <td>-0.121693</td>\n",
       "      <td>-0.034926</td>\n",
       "      <td>-1.072823e+25</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>0.091606</td>\n",
       "      <td>5.425442</td>\n",
       "      <td>5.483249</td>\n",
       "      <td>5.316916</td>\n",
       "      <td>5.207895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6873.947508</td>\n",
       "      <td>8.867265e+10</td>\n",
       "      <td>1.605313e+24</td>\n",
       "      <td>1.060069e+14</td>\n",
       "      <td>1904.759793</td>\n",
       "      <td>6.729846e+13</td>\n",
       "      <td>7.111755e+22</td>\n",
       "      <td>-2.558268e+25</td>\n",
       "      <td>13233.412164</td>\n",
       "      <td>-0.217242</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.891803e+24</td>\n",
       "      <td>-0.133893</td>\n",
       "      <td>-0.035291</td>\n",
       "      <td>-1.084880e+25</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.092125</td>\n",
       "      <td>5.428043</td>\n",
       "      <td>5.480052</td>\n",
       "      <td>5.321115</td>\n",
       "      <td>5.230335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6901.910797</td>\n",
       "      <td>8.892919e+10</td>\n",
       "      <td>1.600205e+24</td>\n",
       "      <td>1.069829e+14</td>\n",
       "      <td>1909.576325</td>\n",
       "      <td>6.973227e+13</td>\n",
       "      <td>7.179346e+22</td>\n",
       "      <td>-2.599559e+25</td>\n",
       "      <td>13050.715628</td>\n",
       "      <td>-0.220111</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.922378e+24</td>\n",
       "      <td>-0.150484</td>\n",
       "      <td>-0.035198</td>\n",
       "      <td>-1.100249e+25</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.093161</td>\n",
       "      <td>5.434801</td>\n",
       "      <td>5.483810</td>\n",
       "      <td>5.317496</td>\n",
       "      <td>5.226047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6951.264001</td>\n",
       "      <td>8.907855e+10</td>\n",
       "      <td>1.597570e+24</td>\n",
       "      <td>1.084181e+14</td>\n",
       "      <td>1931.615401</td>\n",
       "      <td>7.088334e+13</td>\n",
       "      <td>7.229525e+22</td>\n",
       "      <td>-2.639231e+25</td>\n",
       "      <td>12883.123491</td>\n",
       "      <td>-0.223096</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.918786e+24</td>\n",
       "      <td>-0.141911</td>\n",
       "      <td>-0.035456</td>\n",
       "      <td>-1.096993e+25</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.092730</td>\n",
       "      <td>5.433837</td>\n",
       "      <td>5.479156</td>\n",
       "      <td>5.305367</td>\n",
       "      <td>5.208082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6916.164823</td>\n",
       "      <td>8.879042e+10</td>\n",
       "      <td>1.595188e+24</td>\n",
       "      <td>1.082073e+14</td>\n",
       "      <td>1893.966861</td>\n",
       "      <td>6.963535e+13</td>\n",
       "      <td>7.229235e+22</td>\n",
       "      <td>-2.627192e+25</td>\n",
       "      <td>12791.450513</td>\n",
       "      <td>-0.222799</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.953323e+24</td>\n",
       "      <td>-0.136823</td>\n",
       "      <td>-0.034894</td>\n",
       "      <td>-1.102503e+25</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.093498</td>\n",
       "      <td>5.435923</td>\n",
       "      <td>5.495387</td>\n",
       "      <td>5.309339</td>\n",
       "      <td>5.212751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6954.711005</td>\n",
       "      <td>8.892432e+10</td>\n",
       "      <td>1.585478e+24</td>\n",
       "      <td>1.077902e+14</td>\n",
       "      <td>1915.377016</td>\n",
       "      <td>7.154282e+13</td>\n",
       "      <td>7.282973e+22</td>\n",
       "      <td>-2.687569e+25</td>\n",
       "      <td>12666.103787</td>\n",
       "      <td>-0.227576</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.991121e+24</td>\n",
       "      <td>-0.139750</td>\n",
       "      <td>-0.035094</td>\n",
       "      <td>-1.098930e+25</td>\n",
       "      <td>0.033796</td>\n",
       "      <td>0.093054</td>\n",
       "      <td>5.405683</td>\n",
       "      <td>5.495611</td>\n",
       "      <td>5.322693</td>\n",
       "      <td>5.194471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3            4   \\\n",
       "0   6748.177901  8.294421e+10  1.591225e+24  1.095967e+14  1510.190735   \n",
       "1   6713.245495  8.301113e+10  1.591180e+24  1.082126e+14  1552.899870   \n",
       "2   6711.746592  8.324237e+10  1.585508e+24  1.074454e+14  1569.593570   \n",
       "3   6903.974288  8.343257e+10  1.576556e+24  1.084795e+14  1608.218901   \n",
       "4   6691.365852  8.370511e+10  1.600757e+24  1.086643e+14  1479.272564   \n",
       "5   6834.958217  8.406158e+10  1.591036e+24  1.096684e+14  1644.488148   \n",
       "6   6830.348029  8.431275e+10  1.589851e+24  1.083214e+14  1587.280389   \n",
       "7   6894.216660  8.429531e+10  1.577024e+24  1.090471e+14  1530.971594   \n",
       "8   6906.079329  8.449927e+10  1.586829e+24  1.088327e+14  1580.642155   \n",
       "9   6899.094797  8.451908e+10  1.601676e+24  1.091217e+14  1562.139216   \n",
       "10  6911.805621  8.453654e+10  1.584634e+24  1.085458e+14  1608.232234   \n",
       "11  6946.807156  8.448848e+10  1.588865e+24  1.085662e+14  1598.537641   \n",
       "12  6886.818627  8.431700e+10  1.578638e+24  1.071168e+14  1581.500858   \n",
       "13  6818.410837  8.425498e+10  1.577545e+24  1.059483e+14  1552.157851   \n",
       "14  6837.526272  8.424089e+10  1.577035e+24  1.051378e+14  1623.889248   \n",
       "15  6813.759545  8.408494e+10  1.570983e+24  1.053031e+14  1530.518892   \n",
       "16  6810.946741  8.387123e+10  1.559647e+24  1.054864e+14  1553.297953   \n",
       "17  6750.173733  8.393793e+10  1.567801e+24  1.044780e+14  1529.499791   \n",
       "18  6625.715466  8.361261e+10  1.570021e+24  1.039780e+14  1558.476742   \n",
       "19  6602.456377  8.343825e+10  1.559968e+24  1.037375e+14  1475.698999   \n",
       "20  6579.279348  8.332854e+10  1.556961e+24  1.027367e+14  1497.390470   \n",
       "21  6659.340976  8.366482e+10  1.561306e+24  1.038385e+14  1595.699476   \n",
       "22  6674.435798  8.363221e+10  1.558285e+24  1.049437e+14  1561.801278   \n",
       "23  6628.940407  8.364418e+10  1.560219e+24  1.037925e+14  1558.677333   \n",
       "24  6534.272066  8.347730e+10  1.564493e+24  1.040900e+14  1475.048207   \n",
       "25  6618.672577  8.346451e+10  1.557323e+24  1.033482e+14  1600.107458   \n",
       "26  6556.497525  8.331999e+10  1.555459e+24  1.036262e+14  1582.988346   \n",
       "27  6664.787875  8.381583e+10  1.554123e+24  1.049621e+14  1607.631910   \n",
       "28  6610.832931  8.372099e+10  1.551750e+24  1.045356e+14  1628.840026   \n",
       "29  6623.903993  8.393815e+10  1.547466e+24  1.046448e+14  1681.735488   \n",
       "30  6604.636688  8.384039e+10  1.543670e+24  1.042238e+14  1658.377172   \n",
       "31  6590.454932  8.376975e+10  1.544275e+24  1.037572e+14  1656.194789   \n",
       "32  6554.577321  8.382140e+10  1.551716e+24  1.037426e+14  1666.964946   \n",
       "33  6589.028665  8.362348e+10  1.548201e+24  1.047694e+14  1718.904126   \n",
       "34  6696.230175  8.374229e+10  1.550484e+24  1.050534e+14  1780.950881   \n",
       "35  6680.609878  8.378515e+10  1.540382e+24  1.042639e+14  1790.150343   \n",
       "36  6655.976645  8.391120e+10  1.542652e+24  1.037527e+14  1774.532620   \n",
       "37  6621.583380  8.412847e+10  1.550473e+24  1.048586e+14  1828.370218   \n",
       "38  6714.414943  8.417939e+10  1.533120e+24  1.041958e+14  1799.467489   \n",
       "39  6713.854682  8.428436e+10  1.542590e+24  1.039199e+14  1784.416135   \n",
       "40  6675.049946  8.433016e+10  1.538034e+24  1.040402e+14  1819.143184   \n",
       "41  6651.842005  8.457251e+10  1.546075e+24  1.035750e+14  1821.234321   \n",
       "42  6618.690252  8.479324e+10  1.556837e+24  1.026949e+14  1879.972741   \n",
       "43  6689.582061  8.511911e+10  1.554712e+24  1.029952e+14  1920.438043   \n",
       "44  6651.957058  8.528108e+10  1.556176e+24  1.023277e+14  1799.937489   \n",
       "45  6729.800739  8.549333e+10  1.566051e+24  1.030126e+14  1810.811383   \n",
       "46  6749.666713  8.547887e+10  1.565630e+24  1.027505e+14  1816.423463   \n",
       "47  6712.981030  8.579096e+10  1.570802e+24  1.032009e+14  1875.490188   \n",
       "48  6694.946004  8.595762e+10  1.565608e+24  1.032871e+14  1714.993210   \n",
       "49  6720.925432  8.642296e+10  1.568783e+24  1.033549e+14  1788.925676   \n",
       "50  6643.833325  8.686979e+10  1.585399e+24  1.027543e+14  1811.564073   \n",
       "51  6678.435053  8.726270e+10  1.588082e+24  1.029321e+14  1888.605961   \n",
       "52  6719.481170  8.771953e+10  1.595610e+24  1.032735e+14  1839.960188   \n",
       "53  6731.164211  8.808078e+10  1.601331e+24  1.040559e+14  1870.737825   \n",
       "54  6743.632587  8.818426e+10  1.600107e+24  1.048489e+14  1870.199589   \n",
       "55  6873.947508  8.867265e+10  1.605313e+24  1.060069e+14  1904.759793   \n",
       "56  6901.910797  8.892919e+10  1.600205e+24  1.069829e+14  1909.576325   \n",
       "57  6951.264001  8.907855e+10  1.597570e+24  1.084181e+14  1931.615401   \n",
       "58  6916.164823  8.879042e+10  1.595188e+24  1.082073e+14  1893.966861   \n",
       "59  6954.711005  8.892432e+10  1.585478e+24  1.077902e+14  1915.377016   \n",
       "\n",
       "              5             6             7             8         9   ...  \\\n",
       "0   5.537032e+13  6.524454e+22 -1.945966e+25  13461.392092 -0.176659  ...   \n",
       "1   5.163912e+13  6.552311e+22 -1.967367e+25  13459.043822 -0.178458  ...   \n",
       "2   5.208053e+13  6.582194e+22 -2.022283e+25  13391.359484 -0.182930  ...   \n",
       "3   5.166028e+13  6.609184e+22 -2.072753e+25  13248.436229 -0.187068  ...   \n",
       "4   5.030951e+13  6.614713e+22 -1.999304e+25  13367.513454 -0.179852  ...   \n",
       "5   5.663543e+13  6.630166e+22 -2.075205e+25  13298.131406 -0.185888  ...   \n",
       "6   5.648788e+13  6.649235e+22 -2.105215e+25  13390.341243 -0.188014  ...   \n",
       "7   5.153441e+13  6.685179e+22 -2.175302e+25  13251.479423 -0.194314  ...   \n",
       "8   5.204076e+13  6.646028e+22 -2.155312e+25  13485.254521 -0.192063  ...   \n",
       "9   5.105149e+13  6.593766e+22 -2.084657e+25  13729.350076 -0.185724  ...   \n",
       "10  5.032479e+13  6.626648e+22 -2.168199e+25  13676.366262 -0.193127  ...   \n",
       "11  4.720406e+13  6.604935e+22 -2.160836e+25  13798.923015 -0.192580  ...   \n",
       "12  4.759987e+13  6.570224e+22 -2.183161e+25  13867.168557 -0.194966  ...   \n",
       "13  4.509200e+13  6.554720e+22 -2.179559e+25  14013.525138 -0.194787  ...   \n",
       "14  4.995272e+13  6.540712e+22 -2.187353e+25  14092.449955 -0.195516  ...   \n",
       "15  4.714137e+13  6.541886e+22 -2.204741e+25  14125.375441 -0.197436  ...   \n",
       "16  5.192169e+13  6.534533e+22 -2.227891e+25  14030.816852 -0.200018  ...   \n",
       "17  5.263473e+13  6.513396e+22 -2.198664e+25  14122.395674 -0.197237  ...   \n",
       "18  5.476180e+13  6.486121e+22 -2.151174e+25  14176.476618 -0.193727  ...   \n",
       "19  5.215698e+13  6.508146e+22 -2.175203e+25  13994.417647 -0.196301  ...   \n",
       "20  5.335861e+13  6.486579e+22 -2.183151e+25  14085.404680 -0.197277  ...   \n",
       "21  5.790126e+13  6.530352e+22 -2.204153e+25  13978.934717 -0.198375  ...   \n",
       "22  5.628489e+13  6.543992e+22 -2.211603e+25  13883.545665 -0.199123  ...   \n",
       "23  5.651785e+13  6.543197e+22 -2.211868e+25  13923.843184 -0.199118  ...   \n",
       "24  5.209285e+13  6.540494e+22 -2.160368e+25  13833.285734 -0.194871  ...   \n",
       "25  5.740748e+13  6.560208e+22 -2.194959e+25  13785.588862 -0.198021  ...   \n",
       "26  5.571156e+13  6.565363e+22 -2.201092e+25  13704.966138 -0.198919  ...   \n",
       "27  5.965254e+13  6.624869e+22 -2.268300e+25  13567.615361 -0.203780  ...   \n",
       "28  5.812513e+13  6.629692e+22 -2.266189e+25  13518.841224 -0.203821  ...   \n",
       "29  6.043938e+13  6.663268e+22 -2.302672e+25  13449.626827 -0.206567  ...   \n",
       "30  5.433214e+13  6.649111e+22 -2.306179e+25  13492.628976 -0.207122  ...   \n",
       "31  5.797011e+13  6.652554e+22 -2.285079e+25  13423.901150 -0.205400  ...   \n",
       "32  5.595178e+13  6.659935e+22 -2.270023e+25  13490.144006 -0.203921  ...   \n",
       "33  5.865956e+13  6.661440e+22 -2.252770e+25  13382.792443 -0.202850  ...   \n",
       "34  6.372367e+13  6.670024e+22 -2.260599e+25  13434.294831 -0.203267  ...   \n",
       "35  6.220128e+13  6.695897e+22 -2.314251e+25  13321.778770 -0.207984  ...   \n",
       "36  6.236907e+13  6.709688e+22 -2.312158e+25  13351.836402 -0.207484  ...   \n",
       "37  6.313825e+13  6.710731e+22 -2.288411e+25  13328.826569 -0.204823  ...   \n",
       "38  6.292113e+13  6.729763e+22 -2.367600e+25  13291.640048 -0.211782  ...   \n",
       "39  6.210627e+13  6.705084e+22 -2.349227e+25  13439.508935 -0.209877  ...   \n",
       "40  6.488646e+13  6.720600e+22 -2.352803e+25  13388.201074 -0.210083  ...   \n",
       "41  6.246896e+13  6.683234e+22 -2.344465e+25  13559.769347 -0.208738  ...   \n",
       "42  6.718395e+13  6.662508e+22 -2.320355e+25  13744.924963 -0.206054  ...   \n",
       "43  6.659065e+13  6.703677e+22 -2.359518e+25  13709.773165 -0.208729  ...   \n",
       "44  6.106326e+13  6.719677e+22 -2.369129e+25  13673.206273 -0.209182  ...   \n",
       "45  6.165691e+13  6.719766e+22 -2.351989e+25  13800.878660 -0.207153  ...   \n",
       "46  6.445213e+13  6.702058e+22 -2.357341e+25  13929.375838 -0.207659  ...   \n",
       "47  6.395814e+13  6.737582e+22 -2.368335e+25  13878.799260 -0.207869  ...   \n",
       "48  5.665472e+13  6.773257e+22 -2.408134e+25  13738.599523 -0.210952  ...   \n",
       "49  6.013856e+13  6.814229e+22 -2.453392e+25  13722.647950 -0.213759  ...   \n",
       "50  5.910807e+13  6.827774e+22 -2.426738e+25  13834.722831 -0.210350  ...   \n",
       "51  6.151702e+13  6.895435e+22 -2.466136e+25  13750.892245 -0.212802  ...   \n",
       "52  6.065092e+13  6.955306e+22 -2.491348e+25  13604.712481 -0.213858  ...   \n",
       "53  6.834156e+13  6.991464e+22 -2.486756e+25  13517.112435 -0.212588  ...   \n",
       "54  6.936537e+13  7.029250e+22 -2.508392e+25  13356.904229 -0.214186  ...   \n",
       "55  6.729846e+13  7.111755e+22 -2.558268e+25  13233.412164 -0.217242  ...   \n",
       "56  6.973227e+13  7.179346e+22 -2.599559e+25  13050.715628 -0.220111  ...   \n",
       "57  7.088334e+13  7.229525e+22 -2.639231e+25  12883.123491 -0.223096  ...   \n",
       "58  6.963535e+13  7.229235e+22 -2.627192e+25  12791.450513 -0.222799  ...   \n",
       "59  7.154282e+13  7.282973e+22 -2.687569e+25  12666.103787 -0.227576  ...   \n",
       "\n",
       "              17        18        19            20        21        22  \\\n",
       "0  -4.381126e+24  0.003869 -0.030947 -1.004464e+25  0.039773  0.091188   \n",
       "1  -4.383287e+24  0.011711 -0.031749 -1.012427e+25  0.039760  0.091836   \n",
       "2  -4.420609e+24  0.007235 -0.031880 -1.017996e+25  0.039988  0.092085   \n",
       "3  -4.415880e+24 -0.025166 -0.032477 -1.008520e+25  0.039854  0.091020   \n",
       "4  -4.419732e+24 -0.003252 -0.029958 -1.013794e+25  0.039759  0.091198   \n",
       "5  -4.460377e+24 -0.045736 -0.032993 -1.014526e+25  0.039954  0.090877   \n",
       "6  -4.441479e+24 -0.083556 -0.031693 -1.019122e+25  0.039666  0.091017   \n",
       "7  -4.376187e+24 -0.028262 -0.030414 -1.001832e+25  0.039091  0.089491   \n",
       "8  -4.374637e+24 -0.070905 -0.031384 -1.007602e+25  0.038983  0.089789   \n",
       "9  -4.178514e+24 -0.088317 -0.031175 -1.025307e+25  0.037227  0.091345   \n",
       "10 -4.271062e+24 -0.098197 -0.031890 -1.019758e+25  0.038043  0.090832   \n",
       "11 -4.247994e+24 -0.075577 -0.031730 -1.013852e+25  0.037859  0.090358   \n",
       "12 -4.178460e+24 -0.092255 -0.031394 -1.018455e+25  0.037315  0.090952   \n",
       "13 -4.113676e+24 -0.112595 -0.030838 -1.014934e+25  0.036764  0.090705   \n",
       "14 -4.063629e+24 -0.132570 -0.032248 -1.020192e+25  0.036323  0.091190   \n",
       "15 -4.051511e+24 -0.110612 -0.030402 -1.014000e+25  0.036282  0.090804   \n",
       "16 -3.953202e+24 -0.149886 -0.030867 -1.008534e+25  0.035491  0.090545   \n",
       "17 -3.900017e+24 -0.157156 -0.030440 -1.010515e+25  0.034986  0.090651   \n",
       "18 -3.858987e+24 -0.153253 -0.031229 -1.020425e+25  0.034753  0.091896   \n",
       "19 -3.685400e+24 -0.160772 -0.029572 -1.015592e+25  0.033259  0.091652   \n",
       "20 -3.693101e+24 -0.148305 -0.030018 -1.013598e+25  0.033372  0.091592   \n",
       "21 -3.754855e+24 -0.146121 -0.031831 -1.018197e+25  0.033794  0.091638   \n",
       "22 -3.787351e+24 -0.184686 -0.031147 -1.019079e+25  0.034100  0.091753   \n",
       "23 -3.808673e+24 -0.203930 -0.031081 -1.023279e+25  0.034287  0.092118   \n",
       "24 -3.857863e+24 -0.124318 -0.029577 -1.013414e+25  0.034799  0.091413   \n",
       "25 -3.822330e+24 -0.177619 -0.032005 -1.033253e+25  0.034484  0.093216   \n",
       "26 -3.714714e+24 -0.135121 -0.031694 -1.019728e+25  0.033571  0.092156   \n",
       "27 -3.679195e+24 -0.153029 -0.031868 -1.023184e+25  0.033053  0.091921   \n",
       "28 -3.683323e+24 -0.133163 -0.032324 -1.028509e+25  0.033128  0.092504   \n",
       "29 -3.588994e+24 -0.113793 -0.033211 -1.039490e+25  0.032196  0.093250   \n",
       "30 -3.531302e+24 -0.126929 -0.032772 -1.040647e+25  0.031715  0.093462   \n",
       "31 -3.656476e+24 -0.159487 -0.032804 -1.042174e+25  0.032867  0.093679   \n",
       "32 -3.703933e+24 -0.106630 -0.033040 -1.053608e+25  0.033273  0.094648   \n",
       "33 -3.786542e+24 -0.137219 -0.034181 -1.054248e+25  0.034096  0.094930   \n",
       "34 -3.711333e+24 -0.116468 -0.035352 -1.044080e+25  0.033371  0.093881   \n",
       "35 -3.540336e+24 -0.118908 -0.035375 -1.050380e+25  0.031817  0.094399   \n",
       "36 -3.532473e+24 -0.131462 -0.035028 -1.053937e+25  0.031699  0.094576   \n",
       "37 -3.591288e+24 -0.119938 -0.036077 -1.056423e+25  0.032144  0.094554   \n",
       "38 -3.548269e+24 -0.118411 -0.035281 -1.049419e+25  0.031739  0.093871   \n",
       "39 -3.586477e+24 -0.161360 -0.034997 -1.050022e+25  0.032041  0.093808   \n",
       "40 -3.505879e+24 -0.195021 -0.035655 -1.028755e+25  0.031304  0.091858   \n",
       "41 -3.616186e+24 -0.196562 -0.035635 -1.028820e+25  0.032197  0.091600   \n",
       "42 -3.491040e+24 -0.234025 -0.036759 -1.037269e+25  0.031001  0.092112   \n",
       "43 -3.443014e+24 -0.210064 -0.037330 -1.024041e+25  0.030458  0.090589   \n",
       "44 -3.498174e+24 -0.213750 -0.034908 -1.026132e+25  0.030887  0.090602   \n",
       "45 -3.504991e+24 -0.210466 -0.035094 -1.041971e+25  0.030870  0.091772   \n",
       "46 -3.633299e+24 -0.188200 -0.035193 -1.041887e+25  0.032006  0.091780   \n",
       "47 -3.617056e+24 -0.173928 -0.036203 -1.056949e+25  0.031747  0.092768   \n",
       "48 -3.706007e+24 -0.163303 -0.032949 -1.046147e+25  0.032465  0.091642   \n",
       "49 -3.853578e+24 -0.175096 -0.034101 -1.054021e+25  0.033575  0.091835   \n",
       "50 -3.779251e+24 -0.160521 -0.034450 -1.067594e+25  0.032759  0.092539   \n",
       "51 -3.799170e+24 -0.167646 -0.035689 -1.070670e+25  0.032783  0.092388   \n",
       "52 -3.781941e+24 -0.140456 -0.034560 -1.074795e+25  0.032464  0.092261   \n",
       "53 -3.795374e+24 -0.160903 -0.035032 -1.081751e+25  0.032446  0.092477   \n",
       "54 -3.856373e+24 -0.121693 -0.034926 -1.072823e+25  0.032929  0.091606   \n",
       "55 -3.891803e+24 -0.133893 -0.035291 -1.084880e+25  0.033048  0.092125   \n",
       "56 -3.922378e+24 -0.150484 -0.035198 -1.100249e+25  0.033212  0.093161   \n",
       "57 -3.918786e+24 -0.141911 -0.035456 -1.096993e+25  0.033126  0.092730   \n",
       "58 -3.953323e+24 -0.136823 -0.034894 -1.102503e+25  0.033526  0.093498   \n",
       "59 -3.991121e+24 -0.139750 -0.035094 -1.098930e+25  0.033796  0.093054   \n",
       "\n",
       "          23        24        25        26  \n",
       "0   5.414326  5.392323  5.336158  5.205328  \n",
       "1   5.405846  5.403088  5.335880  5.211822  \n",
       "2   5.405772  5.398195  5.345981  5.216945  \n",
       "3   5.396873  5.398732  5.327560  5.203755  \n",
       "4   5.426061  5.389834  5.351214  5.217820  \n",
       "5   5.421926  5.407266  5.350388  5.219855  \n",
       "6   5.419279  5.413495  5.353073  5.232006  \n",
       "7   5.413199  5.418618  5.341884  5.221831  \n",
       "8   5.419249  5.426273  5.354147  5.243087  \n",
       "9   5.427837  5.393532  5.321879  5.221579  \n",
       "10  5.421960  5.419106  5.349870  5.231510  \n",
       "11  5.409650  5.423591  5.336414  5.216673  \n",
       "12  5.404884  5.435943  5.344661  5.215722  \n",
       "13  5.395443  5.426074  5.338376  5.218632  \n",
       "14  5.414608  5.430777  5.334956  5.211968  \n",
       "15  5.414585  5.439978  5.349852  5.199410  \n",
       "16  5.426539  5.439688  5.333861  5.220641  \n",
       "17  5.427973  5.437507  5.321049  5.218753  \n",
       "18  5.420675  5.402765  5.321893  5.207600  \n",
       "19  5.424027  5.401567  5.328385  5.212209  \n",
       "20  5.412794  5.392322  5.331879  5.229455  \n",
       "21  5.407542  5.395212  5.338315  5.199087  \n",
       "22  5.418874  5.404427  5.351370  5.195523  \n",
       "23  5.429408  5.417444  5.352889  5.225211  \n",
       "24  5.429178  5.405270  5.353649  5.195410  \n",
       "25  5.446002  5.412405  5.345017  5.246542  \n",
       "26  5.436352  5.416115  5.330692  5.228028  \n",
       "27  5.449425  5.438875  5.323645  5.244872  \n",
       "28  5.449262  5.424079  5.304066  5.238537  \n",
       "29  5.438638  5.447932  5.311403  5.249751  \n",
       "30  5.415847  5.465051  5.325372  5.225893  \n",
       "31  5.436240  5.471888  5.326894  5.228644  \n",
       "32  5.421760  5.455594  5.327001  5.247861  \n",
       "33  5.416107  5.438976  5.316460  5.241509  \n",
       "34  5.403140  5.454633  5.323734  5.228264  \n",
       "35  5.405974  5.434141  5.301531  5.222481  \n",
       "36  5.412006  5.447493  5.318548  5.231530  \n",
       "37  5.399834  5.472706  5.313747  5.233165  \n",
       "38  5.409866  5.463669  5.298373  5.224789  \n",
       "39  5.431626  5.451033  5.297726  5.220776  \n",
       "40  5.421072  5.471903  5.294902  5.225364  \n",
       "41  5.413351  5.463487  5.295877  5.215056  \n",
       "42  5.418516  5.460616  5.305683  5.225273  \n",
       "43  5.413922  5.463642  5.314288  5.241342  \n",
       "44  5.415397  5.484200  5.318812  5.233028  \n",
       "45  5.423642  5.449615  5.301622  5.208788  \n",
       "46  5.419790  5.491169  5.298181  5.240031  \n",
       "47  5.414254  5.486133  5.296874  5.243491  \n",
       "48  5.408871  5.489321  5.307261  5.219881  \n",
       "49  5.410845  5.464945  5.290419  5.235641  \n",
       "50  5.415777  5.462901  5.289332  5.233490  \n",
       "51  5.407051  5.498585  5.300528  5.232761  \n",
       "52  5.430693  5.476389  5.304190  5.233783  \n",
       "53  5.434391  5.482465  5.301332  5.229522  \n",
       "54  5.425442  5.483249  5.316916  5.207895  \n",
       "55  5.428043  5.480052  5.321115  5.230335  \n",
       "56  5.434801  5.483810  5.317496  5.226047  \n",
       "57  5.433837  5.479156  5.305367  5.208082  \n",
       "58  5.435923  5.495387  5.309339  5.212751  \n",
       "59  5.405683  5.495611  5.322693  5.194471  \n",
       "\n",
       "[60 rows x 27 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "_3wGjyeQHgJA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__111\n",
      "self.rawdat:  (60, 27)\n",
      "self.dat.shape:  (60, 27)\n",
      "_normalized\n",
      "_split\n",
      "tarin_start:  0\n",
      "train:  36\n",
      "valid:  48\n",
      "valid_stop:  60\n",
      "train_set:  range(0, 36)\n",
      "valid_set:  range(36, 48)\n",
      "test_set:  range(48, 60)\n",
      "_batchify\n",
      "idx_set:  range(0, 36)\n",
      "len(idx_set): 36\n",
      "X.shape: torch.Size([36, 0, 27])\n",
      "Y.shape: torch.Size([36, 27])\n",
      "_batchify\n",
      "idx_set:  range(36, 48)\n",
      "len(idx_set): 12\n",
      "X.shape: torch.Size([12, 0, 27])\n",
      "Y.shape: torch.Size([12, 27])\n",
      "_batchify\n",
      "idx_set:  range(48, 60)\n",
      "len(idx_set): 12\n",
      "X.shape: torch.Size([12, 0, 27])\n",
      "Y.shape: torch.Size([12, 27])\n",
      "The recpetive field size is 187\n",
      "Number of model parameters is 338865\n",
      "optim = Optim()\n",
      "done\n",
      "begin training\n",
      "train()\n",
      "get_batches\n",
      "iter:  0 | loss: 1878786536439704887230464.000\n",
      "get_batches\n",
      "sigma_p:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "sigma_g:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "correlation:  [           nan  2.9172645e-06  7.7181639e-06 -1.4642128e-05\n",
      " -1.8610361e-06  9.5199458e-02  2.2337321e-05            nan\n",
      "  4.1032925e-02  5.6680706e-06            nan -1.1970454e-01\n",
      "  1.4352466e-05  9.4477400e-02  6.9108996e-06 -8.4207913e-06\n",
      "  1.7199606e-01 -1.5172879e-01  1.8755446e-07  1.0040427e-06\n",
      "            nan  3.5527731e-07  1.0479629e-06 -1.0798854e-05\n",
      "  6.1542787e-02  0.0000000e+00  2.3114766e-05]\n",
      "| end of epoch   1 | time:  3.05s | train_loss 1923161185149996813516800.0000 | valid rse   inf | valid rae 0.7480 | valid corr    nan\n",
      "get_batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muzah\\AppData\\Local\\Temp\\ipykernel_17236\\787462309.py:46: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation =  aaa/ bbb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_p:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "sigma_g:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "correlation:  [           nan  2.9172645e-06  7.7181639e-06 -1.4642128e-05\n",
      " -1.8610361e-06  9.5199458e-02  2.2337321e-05            nan\n",
      "  4.1032925e-02  5.6680706e-06            nan -1.1970454e-01\n",
      "  1.4352466e-05  9.4477400e-02  6.9108996e-06 -8.4207913e-06\n",
      "  1.7199606e-01 -1.5172879e-01  1.8755446e-07  1.0040427e-06\n",
      "            nan  3.5527731e-07  1.0479629e-06 -1.0798854e-05\n",
      "  6.1542787e-02  0.0000000e+00  2.3114766e-05]\n",
      "get_batches\n",
      "sigma_p:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "sigma_g:  [0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 5.9604645e-08\n",
      " 4.2146848e-08 5.9604645e-08 0.0000000e+00 4.7121610e-08 2.9802322e-08\n",
      " 0.0000000e+00 4.7121610e-08 2.9802322e-08 4.7121610e-08 5.9604645e-08\n",
      " 2.9802322e-08 2.1073424e-08 2.1073424e-08 2.9802322e-08 2.9802322e-08\n",
      " 0.0000000e+00 5.9604645e-08 2.9802322e-08 2.9802322e-08 4.7121610e-08\n",
      " 2.9802322e-08 5.9604645e-08]\n",
      "correlation:  [           nan  3.0401588e-06  7.2239313e-06 -5.1717379e-06\n",
      "  2.3864902e-06 -1.6793032e-01  8.4619444e-07            nan\n",
      " -4.0212363e-02 -2.7740411e-06            nan  9.6026249e-02\n",
      "  5.6897366e-06  4.1636396e-02 -3.6158073e-07  8.3153503e-07\n",
      "  2.6783791e-01 -1.8301384e-01  1.5114207e-07  2.1190590e-06\n",
      "            nan -1.0448287e-05  4.9355585e-06  3.0511803e-05\n",
      " -2.3878787e-02  2.5252980e-06  8.1703110e-06]\n",
      "final test rse   inf | test rae 0.7840 | test corr   nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(inf), tensor(0.7480), nan, tensor(inf), tensor(0.7840), nan)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save= \"C:\\0.ml\\MTGNN\\\\model-save-3.pt\"\n",
    "seq_in_len=0#24*7#window size\n",
    "batch_size=2#,help='batch size'\n",
    "num_nodes= 27 \n",
    "horizon=1\n",
    "device='cpu' \n",
    "#device='cuda:0'\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MTGNN+model5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
